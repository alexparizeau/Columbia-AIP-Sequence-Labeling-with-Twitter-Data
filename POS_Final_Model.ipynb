{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS_Final_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cpTFb0fFs1PQ",
        "i4naj31ntBTi",
        "pqgbOvnftamT",
        "fe-6qih7tkNj",
        "lAPGttEo1DgG",
        "RMtIUKyszhBB",
        "XsOx1IcI34e3",
        "POrctmH35yfs",
        "t_DiLpjm53L1",
        "mMsgNBlk6Vsz",
        "rJYdk_Lcx-i7",
        "hIVKwr0i1ay-",
        "yzkNLl2W4JXd",
        "-Ng_Pez64Oa6",
        "Q2f02iQO2xM1",
        "ZBjPp5J322w9",
        "z6w5QpY14U_i",
        "nxEu9NbD3K-w",
        "vOe3OjcX3P_i",
        "_BlQDdJnnXZr",
        "nUVnYQz93cR1",
        "_H757C_k3fNs",
        "GJmtINY-HYZW",
        "1I672dnP4MM6",
        "2lxp21YxJdhJ",
        "EvCzwa9xJhKd",
        "LqOgpUMc9__h",
        "I_P0NcYZ-a-P",
        "DdiI0bHz-oYx",
        "37y7zf0j-1eX",
        "4QvxMpbh_DQo",
        "9ujxIkyC_ena",
        "gYDiSCHE5SFm",
        "Kmib3OPV53wS",
        "uyicOt-hAtME",
        "LcJr1T286HxG",
        "_mWINBqI6UxF"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexparizeau/IEORE4524-P033B/blob/main/POS_Final_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Environment Set Up"
      ],
      "metadata": {
        "id": "cpTFb0fFs1PQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMyoJFur7aol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d7b96c-8ba6-4bdf-c166-4cc5ded5a90d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "d8PGgxVW7zO1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cb09798-df31-483e-867c-3c71904ceff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.50.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 50.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 27.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 41.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Collecting click==8.0\n",
            "  Downloading click-8.0.0-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.50-py3-none-any.whl size=895166 sha256=4bcb75f9e620a04528f40ad4a6fbcfa1777f9fa4f15f72de3ce3b782cb0e5376\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/72/54/519f0d5143cc6c73fa3297509123c86fc8586a7fdea8d25311\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: pyyaml, click, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.0.0 which is incompatible.\u001b[0m\n",
            "Successfully installed click-8.0.0 huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.50 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers seqeval[gpu]"
      ],
      "metadata": {
        "id": "RiAUxGm_R-5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e21461b-07b6-4db6-e66f-f91d8d932572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Collecting seqeval[gpu]\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 591 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.50)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click==8.0 in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval[gpu]) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=2da1e2ff485a4205e039127e70a4ca8458a145e96d3e8ab7b6e203553dac7240\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import cuda\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "#from seqeval.metrics import classification_report"
      ],
      "metadata": {
        "id": "htIsnYC88Yjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdmZ-AYnSulH",
        "outputId": "dff9cac3-fdba-4b98-c656-b9c06fa02a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "74uRhUjCIgsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Supervised POS Models"
      ],
      "metadata": {
        "id": "i4naj31ntBTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a) Conll Dataset (Standard English)"
      ],
      "metadata": {
        "id": "pqgbOvnftamT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Loading"
      ],
      "metadata": {
        "id": "fe-6qih7tkNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conll_train = drive.CreateFile({'id':'14dvjq020icKk21X0H2m2VLYIhHfe3ga0'}) \n",
        "conll_train.GetContentFile('Conll_train.txt')\n",
        "conll_valid = drive.CreateFile({'id':'1MSJiPNgWKPuK2wOTeNBOnGqZR-Qvq6VV'}) \n",
        "conll_valid.GetContentFile('Conll_valid.txt')\n",
        "conll_test = drive.CreateFile({'id':'1RYuoceeUyExeZdGJvPsSVzCBqDAbUUdp'}) \n",
        "conll_test.GetContentFile('Conll_test.txt')"
      ],
      "metadata": {
        "id": "jFUlB9eCtpiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing"
      ],
      "metadata": {
        "id": "lAPGttEo1DgG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dscOcCaFyOwx"
      },
      "outputs": [],
      "source": [
        "def get_raw_df(txt_list):\n",
        "  i = 0\n",
        "  all_list = [sentence.strip().split() for sentence in txt_list]\n",
        "  for l in all_list:\n",
        "    if l == []:\n",
        "      i += 1\n",
        "    else:\n",
        "      l.append(f'sentence {i}')\n",
        "  all_list = [x for x in all_list if x and x[4] != \"sentence 0\"]\n",
        "  df = pd.DataFrame(all_list, columns=[\"id\", \"pos_tags\",\"chunk_tags\",\"ner_tags\", \"sentence number\"])\n",
        "  return df\n",
        "\n",
        "def get_preprocessed_df(txt_list, tag_name):\n",
        "  raw_df = get_raw_df(txt_list)\n",
        "  \"\"\"tag_name is a string representing tag name from \"pos_tags\",\"chunk_tags\",\"ner_tags\" \"\"\"\n",
        "  raw_df[\"sentence\"] = raw_df[[\"sentence number\", \"id\", \"pos_tags\"]].groupby([\"sentence number\"])[\"id\"].transform(lambda x: \" \".join(x))\n",
        "  raw_df[\"labels\"] = raw_df[[\"sentence number\", \"id\", \"pos_tags\"]].groupby([\"sentence number\"])[tag_name].transform(lambda x: \" \".join(x))\n",
        "  return raw_df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Conll_train.txt\") as f:\n",
        "  train = f.readlines()\n",
        "df_train_raw = get_preprocessed_df(train, \"pos_tags\")\n",
        "\n",
        "with open(\"Conll_valid.txt\") as f:\n",
        "  valid = f.readlines()\n",
        "df_valid_raw = get_preprocessed_df(valid, \"pos_tags\")\n",
        "\n",
        "with open(\"Conll_test.txt\") as f:\n",
        "  test = f.readlines()\n",
        "df_test_raw = get_preprocessed_df(test, \"pos_tags\")"
      ],
      "metadata": {
        "id": "TaSXtMptuaLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo8vgWQzu_20",
        "outputId": "ad3a74c0-5d0e-4c0c-bb4d-7e4b5b437d09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train dataset: 204567, Length of validation dataset: 51578, Length of test dataset: 46666 \n"
          ]
        }
      ],
      "source": [
        "print(f\"Length of train dataset: {len(df_train_raw)}, Length of validation dataset: {len(df_valid_raw)}, Length of test dataset: {len(df_test_raw)} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hxg9G6uv1_Y"
      },
      "outputs": [],
      "source": [
        "labels_to_ids = {'\"': 0, \"''\": 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9, 'CC': 10, 'CD': 11, 'DT': 12,\n",
        " 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23,\n",
        " 'NNS': 24, 'NN|SYM': 25, 'PDT': 26, 'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33,\n",
        " 'SYM': 34, 'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42, 'WDT': 43,\n",
        " 'WP': 44, 'WP$': 45, 'WRB': 46}\n",
        "ids_to_labels = dict((v,k) for k,v in labels_to_ids.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9ziLRchv_y-"
      },
      "outputs": [],
      "source": [
        "def get_sentence_labels(raw_df):\n",
        "  return raw_df[[\"sentence\", \"labels\"]].drop_duplicates().reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zppjV2B9v_y_"
      },
      "outputs": [],
      "source": [
        "df_train = get_sentence_labels(df_train_raw)\n",
        "df_valid = get_sentence_labels(df_valid_raw)\n",
        "df_test = get_sentence_labels(df_test_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb88Z8AnL3O7"
      },
      "outputs": [],
      "source": [
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz4H_HKcv_y_"
      },
      "outputs": [],
      "source": [
        "print(f\"Length of train dataset: {len(df_train)}, Length of validation dataset: {len(df_valid)}, Length of test dataset: {len(df_test)} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change dataframe to PyTorch tensors "
      ],
      "metadata": {
        "id": "RMtIUKyszhBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 3\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased') #Bert using wordpiece *** may improve further"
      ],
      "metadata": {
        "id": "-_k5NbXezhBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(Dataset):\n",
        "  def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        # step 1: get the sentence and word labels \n",
        "        sentence = self.data.sentence[index].strip().split()  \n",
        "        word_labels = self.data.labels[index].split() \n",
        "\n",
        "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
        "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
        "        encoding = self.tokenizer(sentence,\n",
        "                             is_split_into_words=True, #no is_pretokenlized(Modification), we already have a splitted sentence\n",
        "                             return_offsets_mapping=True, \n",
        "                             padding='max_length', \n",
        "                             truncation=True, \n",
        "                             max_length=self.max_len)\n",
        "        \n",
        "        # step 3: create token labels only for first word pieces of each tokenized word\n",
        "        labels = [labels_to_ids[label] for label in word_labels] \n",
        "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
        "        # create an empty array of -100 of length max_length\n",
        "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
        "        \n",
        "        # set only labels whose first offset position is 0 and the second is not 0\n",
        "        i = 0\n",
        "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
        "          if mapping[0] == 0 and mapping[1] != 0:\n",
        "            # overwrite label\n",
        "            encoded_labels[idx] = labels[i]\n",
        "            i += 1       \n",
        "\n",
        "        # step 4: turn everything into PyTorch tensors\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.as_tensor(encoded_labels)\n",
        "        \n",
        "        return item\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "9hCXglZzzhBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo8OgKxHzhBC"
      },
      "outputs": [],
      "source": [
        "training_set = dataset(df_train, tokenizer, MAX_LEN)\n",
        "validation_set = dataset(df_valid, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(df_test, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization Analysis"
      ],
      "metadata": {
        "id": "XsOx1IcI34e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(tensor_of_tokens):\n",
        "  list_of_tokens = tensor_of_tokens.tolist()\n",
        "  return tokenizer.convert_ids_to_tokens((list_of_tokens))\n",
        "\n",
        "def token_analysis(line_num):\n",
        "  \"\"\"input a row of the training data you wish to look at\"\"\"\n",
        "  input = df_train.iloc[line_num]['sentence']\n",
        "  pos_labels = df_train.iloc[line_num]['labels']\n",
        "  numerical_labels = training_set[line_num]['input_ids']\n",
        "  label = decoder(training_set[line_num]['input_ids'])\n",
        "  attn = training_set[line_num]['attention_mask']\n",
        "  print(f'original sentence: {input}\\n POS Tags: {pos_labels}\\n Token IDs: {numerical_labels}\\n Decoded Tokens: {label} \\n Attention Masks: {attn}')"
      ],
      "metadata": {
        "id": "3GOXUXJlJ7rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_analysis(22) #222 shows word piece splitting "
      ],
      "metadata": {
        "id": "rf2Mf9ddxRpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea0bf24-500f-4c04-8464-110dcd46e640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original sentence: RT @iceaxe5 : #cholera #haiti ... And how many lives ( and family providers ) would have been saved with faster warning & response .\n",
            " POS Tags: ~ @ ~ # # , & R A N , & N N , V V V V P A N & N ,\n",
            " Token IDs: tensor([  101, 19387,  1030,  3256,  8528,  2063,  2629,  1024,  1001, 25916,\n",
            "         1001, 12867,  1012,  1012,  1012,  1998,  2129,  2116,  3268,  1006,\n",
            "         1998,  2155, 11670,  1007,  2052,  2031,  2042,  5552,  2007,  5514,\n",
            "         5432,  1004,  3433,  1012,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            " Decoded Tokens: ['[CLS]', 'rt', '@', 'ice', '##ax', '##e', '##5', ':', '#', 'cholera', '#', 'haiti', '.', '.', '.', 'and', 'how', 'many', 'lives', '(', 'and', 'family', 'providers', ')', 'would', 'have', 'been', 'saved', 'with', 'faster', 'warning', '&', 'response', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] \n",
            " Attention Masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2hBX04D3Pao"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POrctmH35yfs"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H4vL5ObQRr7o"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "    \n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        \n",
        "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "        labels = batch['labels'].to(device, dtype = torch.long)\n",
        "\n",
        "        #loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "        output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "        tr_loss += output[0]\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += labels.size(0)\n",
        "        \n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "           \n",
        "        # compute training accuracy\n",
        "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = output[1].view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        \n",
        "        # only compute accuracy at active labels\n",
        "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
        "        \n",
        "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "        \n",
        "        tr_labels.extend(labels)\n",
        "        tr_preds.extend(predictions)\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "    \n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "        \n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        output[0].backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bezmloKrS8GG",
        "outputId": "a1305088-b774-49d2-8919-c2ac0a90afb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 2.244277000427246\n",
            "Training loss per 100 training steps: 0.9602926969528198\n",
            "Training loss per 100 training steps: 0.8355238437652588\n",
            "Training loss per 100 training steps: 0.6532976627349854\n",
            "Training loss per 100 training steps: 0.5435946583747864\n",
            "Training loss per 100 training steps: 0.4813309609889984\n",
            "Training loss per 100 training steps: 0.41956397891044617\n",
            "Training loss per 100 training steps: 0.3989030122756958\n",
            "Training loss per 100 training steps: 0.3652709126472473\n",
            "Training loss per 100 training steps: 0.3389182388782501\n",
            "Training loss per 100 training steps: 0.31545451283454895\n",
            "Training loss per 100 training steps: 0.29849863052368164\n",
            "Training loss per 100 training steps: 0.2832047641277313\n",
            "Training loss per 100 training steps: 0.2676505148410797\n",
            "Training loss per 100 training steps: 0.25448182225227356\n",
            "Training loss per 100 training steps: 0.24340149760246277\n",
            "Training loss per 100 training steps: 0.23153622448444366\n",
            "Training loss per 100 training steps: 0.2231494039297104\n",
            "Training loss per 100 training steps: 0.21831263601779938\n",
            "Training loss per 100 training steps: 0.21099388599395752\n",
            "Training loss per 100 training steps: 0.20470863580703735\n",
            "Training loss per 100 training steps: 0.20011967420578003\n",
            "Training loss per 100 training steps: 0.19895519316196442\n",
            "Training loss per 100 training steps: 0.19360004365444183\n",
            "Training loss per 100 training steps: 0.18811637163162231\n",
            "Training loss per 100 training steps: 0.18540804088115692\n",
            "Training loss per 100 training steps: 0.18196821212768555\n",
            "Training loss per 100 training steps: 0.1797037124633789\n",
            "Training loss per 100 training steps: 0.17639732360839844\n",
            "Training loss per 100 training steps: 0.17262253165245056\n",
            "Training loss per 100 training steps: 0.16969110071659088\n",
            "Training loss per 100 training steps: 0.16678990423679352\n",
            "Training loss epoch: 0.1653120368719101\n",
            "Training accuracy epoch: 0.9560129599000865\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  print(f\"Training epoch: {epoch + 1}\")\n",
        "  train(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLeaK1PYRC6Z"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRGEqF6fRJ-D",
        "outputId": "dfa98a38-8070-4e83-dc1a-6f512d98b209"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_to_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_3daoOTRb81"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if cuda.is_available() else 'cpu' #save the processing time\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te2SqWJiRkP1"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_DiLpjm53L1"
      },
      "source": [
        "### Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q_vGlAUkuKba"
      },
      "outputs": [],
      "source": [
        "def valid(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            \n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['labels'].to(device, dtype = torch.long)\n",
        "            \n",
        "            #loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "            output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "\n",
        "            eval_loss += output[0].item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += labels.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = output[1].view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            \n",
        "            # only compute accuracy at active labels\n",
        "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        \n",
        "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            \n",
        "            eval_labels.extend(labels)\n",
        "            eval_preds.extend(predictions)\n",
        "            \n",
        "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
        "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xo3ntHnAxA3m",
        "outputId": "42d59492-b0a0-43de-f954-731236e88940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss per 100 evaluation steps: 0.5765858292579651\n",
            "Validation loss per 100 evaluation steps: 0.08514627988166683\n",
            "Validation loss per 100 evaluation steps: 0.09021025108068079\n",
            "Validation loss per 100 evaluation steps: 0.09818781472868642\n",
            "Validation loss per 100 evaluation steps: 0.0920739399212322\n",
            "Validation loss per 100 evaluation steps: 0.09368502276023839\n",
            "Validation loss per 100 evaluation steps: 0.1006063915864284\n",
            "Validation loss per 100 evaluation steps: 0.0973066015040486\n",
            "Validation loss per 100 evaluation steps: 0.09554475259917153\n",
            "Validation loss per 100 evaluation steps: 0.10182160273517071\n",
            "Validation loss per 100 evaluation steps: 0.09928708535496501\n",
            "Validation loss per 100 evaluation steps: 0.1066447233639638\n",
            "Validation loss per 100 evaluation steps: 0.10558185114991114\n",
            "Validation loss per 100 evaluation steps: 0.10778211929813367\n",
            "Validation loss per 100 evaluation steps: 0.10395088882258144\n",
            "Validation loss per 100 evaluation steps: 0.09960441261274444\n",
            "Validation Loss: 0.1003675622444666\n",
            "Validation Accuracy: 0.9728829117259169\n"
          ]
        }
      ],
      "source": [
        "labels_conll, predictions_conll = valid(model, testing_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(list(labels_conll),list(predictions_conll)))"
      ],
      "metadata": {
        "id": "Cy4XgsWh04N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMsgNBlk6Vsz"
      },
      "source": [
        "### Save the model for further use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UHKor2PU6ZWo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "directory = \"./supervised_pos_Conll\"\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# save vocabulary of the tokenizer\n",
        "tokenizer.save_vocabulary(directory)\n",
        "# save the model weights and its configuration file\n",
        "model.save_pretrained(directory)\n",
        "print('Supervised POS Conll model saved')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b) Gimpel et Al (Twitter)"
      ],
      "metadata": {
        "id": "1SOfgJ01xgdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Loading"
      ],
      "metadata": {
        "id": "wvCZUPqZxtbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gimpel_pos = drive.CreateFile({'id':\"14UNwfhWzYYILrc3IG6frc-XyxU1x6MnL\"})\n",
        "gimpel_pos.GetContentFile(\"oct27.supertsv\")"
      ],
      "metadata": {
        "id": "G2fkXPZ87la0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "rJYdk_Lcx-i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_raw_df(txt_list):\n",
        "  i = 0\n",
        "  all_list = [sentence.strip().split() for sentence in txt_list]\n",
        "  for l in all_list:\n",
        "    if l == []:\n",
        "      i += 1\n",
        "    else:\n",
        "      l.append(f'sentence {i}')\n",
        "  all_list = [x for x in all_list if x and x[0] != '-TWEETSTART-']\n",
        "  df = pd.DataFrame(all_list, columns=[\"id\", \"pos_tags\", \"sentence number\",\"delete\",])\n",
        "  df = df.drop(['delete'],axis=1)\n",
        "  return df\n",
        "\n",
        "def get_preprocessed_df(txt_list, tag_name):\n",
        "  raw_df = get_raw_df(txt_list)\n",
        "  \"\"\"tag_name is a string representing tag name from \"pos_tags\",\"chunk_tags\",\"ner_tags\" \"\"\"\n",
        "  raw_df[\"sentence\"] = raw_df[[\"sentence number\", \"id\", \"pos_tags\",]].groupby([\"sentence number\"])[\"id\"].transform(lambda x: \" \".join(x))\n",
        "  raw_df[\"labels\"] = raw_df[[\"sentence number\", \"id\", \"pos_tags\"]].groupby([\"sentence number\"])[tag_name].transform(lambda x: \" \".join(x))\n",
        "  return raw_df\n"
      ],
      "metadata": {
        "id": "keeKKy3l9HCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_to_ids = {',':0, '&':1, '$':2, 'D':3,'X':4, 'G':5, 'P':6, 'A':7, 'V':8, 'N':9, '^':10, 'O':11, 'R':12, '!':13, 'S':14, 'Z':15, 'L':16, 'M':17, '#':18, '@':19, '~':20, 'U':21, 'E':22, 'T':23,'Y':24}\n",
        "ids_to_labels = dict((v,k) for k,v in labels_to_ids.items())"
      ],
      "metadata": {
        "id": "-3nkfP7VOCoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = \"oct27.supertsv\"\n",
        "data = open(tweets, \"r\",encoding='utf-8')\n",
        "text = data.read()\n",
        "texts = text.split('\\n')\n",
        "text_edit = list()\n",
        "for item in texts:\n",
        "  if item.startswith('TWEET\\t'):\n",
        "    text_edit.append('-TWEETSTART- -X-\\n')\n",
        "  elif item == 'TOKENS':\n",
        "    text_edit.append('\\n')\n",
        "  elif item != '':\n",
        "    token = item[2:]\n",
        "    tag = item[0]\n",
        "    text_edit.append(f'{token} {tag}\\n')"
      ],
      "metadata": {
        "id": "B_sUiafj_Irz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.model_selection as sk_ms\n",
        "full_tweets = get_preprocessed_df(text_edit,'pos_tags')\n",
        "train_tweets, test_tweets = sk_ms.train_test_split(full_tweets, train_size=0.7,random_state=123,shuffle=True,stratify=full_tweets['labels'])"
      ],
      "metadata": {
        "id": "xDkalSEDmlBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tweets = test_tweets.reset_index()"
      ],
      "metadata": {
        "id": "IRj8uOc12jVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tweets = train_tweets[train_tweets['pos_tags'].isin(labels_to_ids.keys())]\n",
        "test_tweets = test_tweets[test_tweets['pos_tags'].isin(labels_to_ids.keys())]"
      ],
      "metadata": {
        "id": "xy66jN6j3bEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tweets[\"pos_tags\"].value_counts()"
      ],
      "metadata": {
        "id": "_oqZGDQJImk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_labels(raw_df):\n",
        "  return raw_df[[\"sentence\", \"labels\"]].drop_duplicates().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "TK4zo38mLdEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = get_sentence_labels(train_tweets)\n",
        "#df_valid = get_sentence_labels(df_valid)\n",
        "df_test = get_sentence_labels(test_tweets)"
      ],
      "metadata": {
        "id": "seTkKUM_Hn6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "id": "VKohBAN9E4gK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf0a542-d3b0-48e7-96bc-bfba24d7ba31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               sentence  \\\n",
              "0     APPCRAFT software : T-Mobile Will Deliver The ...   \n",
              "1     RT @AMGoHam : #Shoutouts to @theBEEzneez_ she ...   \n",
              "2     now what i do hate is for a man to think its o...   \n",
              "3     Google : Different Algorithms Can Apply To Dif...   \n",
              "4                                 I LOVE MY CELTICS !!!   \n",
              "...                                                 ...   \n",
              "1810                          @ChingyJackpot follow me6   \n",
              "1811                                 http://dld.bz/y5fb   \n",
              "1812                                      i'm so hungry   \n",
              "1813                                   its #humpdayyy .   \n",
              "1814       RT @CapoRo722 : Lincecum lookin real anxious   \n",
              "\n",
              "                                                 labels  \n",
              "0                 ^ ^ , ^ V V D ^ ^ $ P ^ $ , ^ V R ~ U  \n",
              "1     ~ @ ~ # P @ O V D A N , V V D N , D N P N , ~ ...  \n",
              "2               R O O V V V P D N P V L A P V O P N @ #  \n",
              "3                         ^ , A N V V P A N P A N U P @  \n",
              "4                                             O V D ^ ,  \n",
              "...                                                 ...  \n",
              "1810                                              @ V O  \n",
              "1811                                                  U  \n",
              "1812                                              L R A  \n",
              "1813                                              L N ,  \n",
              "1814                                      ~ @ ~ ^ V R A  \n",
              "\n",
              "[1815 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-564aa2f0-c488-4502-baec-38dc40a57206\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>APPCRAFT software : T-Mobile Will Deliver The ...</td>\n",
              "      <td>^ ^ , ^ V V D ^ ^ $ P ^ $ , ^ V R ~ U</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RT @AMGoHam : #Shoutouts to @theBEEzneez_ she ...</td>\n",
              "      <td>~ @ ~ # P @ O V D A N , V V D N , D N P N , ~ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>now what i do hate is for a man to think its o...</td>\n",
              "      <td>R O O V V V P D N P V L A P V O P N @ #</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Google : Different Algorithms Can Apply To Dif...</td>\n",
              "      <td>^ , A N V V P A N P A N U P @</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I LOVE MY CELTICS !!!</td>\n",
              "      <td>O V D ^ ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1810</th>\n",
              "      <td>@ChingyJackpot follow me6</td>\n",
              "      <td>@ V O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1811</th>\n",
              "      <td>http://dld.bz/y5fb</td>\n",
              "      <td>U</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1812</th>\n",
              "      <td>i'm so hungry</td>\n",
              "      <td>L R A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1813</th>\n",
              "      <td>its #humpdayyy .</td>\n",
              "      <td>L N ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1814</th>\n",
              "      <td>RT @CapoRo722 : Lincecum lookin real anxious</td>\n",
              "      <td>~ @ ~ ^ V R A</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1815 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-564aa2f0-c488-4502-baec-38dc40a57206')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-564aa2f0-c488-4502-baec-38dc40a57206 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-564aa2f0-c488-4502-baec-38dc40a57206');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of train dataset: {len(df_train)}, Length of test dataset: {len(df_test)} \")"
      ],
      "metadata": {
        "id": "5gE_VmOYPwsN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e1bf99-7386-4d1e-f15e-a995ce5d2635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train dataset: 1815, Length of test dataset: 1727 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change dataframe to PyTorch tensors "
      ],
      "metadata": {
        "id": "hIVKwr0i1ay-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 3\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased') #Bert using wordpiece *** may improve further"
      ],
      "metadata": {
        "id": "0mY602GKF5St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(Dataset):\n",
        "  def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        # step 1: get the sentence and word labels \n",
        "        sentence = self.data.sentence[index].strip().split()  \n",
        "        word_labels = self.data.labels[index].split() \n",
        "\n",
        "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
        "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
        "        encoding = self.tokenizer(sentence,\n",
        "                             is_split_into_words=True, #no is_pretokenlized(Modification), we already have a splitted sentence\n",
        "                             return_offsets_mapping=True, \n",
        "                             padding='max_length', \n",
        "                             truncation=True, \n",
        "                             max_length=self.max_len)\n",
        "        \n",
        "        # step 3: create token labels only for first word pieces of each tokenized word\n",
        "        labels = [labels_to_ids[label] for label in word_labels] \n",
        "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
        "        # create an empty array of -100 of length max_length\n",
        "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
        "        \n",
        "        # set only labels whose first offset position is 0 and the second is not 0\n",
        "        i = 0\n",
        "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
        "          if mapping[0] == 0 and mapping[1] != 0:\n",
        "            # overwrite label\n",
        "            encoded_labels[idx] = labels[i]\n",
        "            i += 1       \n",
        "\n",
        "        # step 4: turn everything into PyTorch tensors\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.as_tensor(encoded_labels)\n",
        "        \n",
        "        return item\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "tEhoO7mcGd4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axcjPXv1J6P_"
      },
      "outputs": [],
      "source": [
        "training_set = dataset(df_train, tokenizer, MAX_LEN)\n",
        "validation_set = dataset(df_valid, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(df_test, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building Train and Test Functions"
      ],
      "metadata": {
        "id": "yzkNLl2W4JXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, modelx):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    modelx.train()\n",
        "    \n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "        labels = batch['labels'].to(device, dtype = torch.long)\n",
        "\n",
        "        #loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "        output = modelx(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "        tr_loss += output[0]\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += labels.size(0)\n",
        "        \n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "           \n",
        "        # compute training accuracy\n",
        "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = output[1].view(-1, modelx.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        \n",
        "        # only compute accuracy at active labels\n",
        "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
        "        \n",
        "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "        \n",
        "        tr_labels.extend(labels)\n",
        "        tr_preds.extend(predictions)\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "    \n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=modelx.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "        \n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        output[0].backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
      ],
      "metadata": {
        "id": "_OOPywTt39zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(modelx, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    modelx.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            \n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['labels'].to(device, dtype = torch.long)\n",
        "            \n",
        "            #loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "            output = modelx(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "\n",
        "            eval_loss += output[0].item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += labels.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = output[1].view(-1, modelx.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            \n",
        "            # only compute accuracy at active labels\n",
        "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        \n",
        "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            \n",
        "            eval_labels.extend(labels)\n",
        "            eval_preds.extend(predictions)\n",
        "            \n",
        "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
        "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions"
      ],
      "metadata": {
        "id": "XxeBhwRa39zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### i) BERT POS"
      ],
      "metadata": {
        "id": "-Ng_Pez64Oa6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training and Testing"
      ],
      "metadata": {
        "id": "Q2f02iQO2xM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer,RobertaTokenizerFast,RobertaForTokenClassification, AutoModelForTokenClassification\n",
        "\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 3\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', add_prefix_space=True) \n",
        "\n",
        "training_set = dataset(df_train, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n",
        "model2 = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(ids_to_labels))\n",
        "model2.resize_token_embeddings(len(tokenizer))\n",
        "device = 'cuda' if cuda.is_available() else 'cpu' #save the processing time\n",
        "model2.to(device)\n",
        "optimizer = torch.optim.Adam(params=model2.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "##Train\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"Training epoch: {epoch + 1}\")\n",
        "  train(epoch,model2)\n",
        "\n",
        "## Test\n",
        "\n",
        "labels, predictions = valid(model2, testing_loader)\n",
        "print(classification_report(list(labels),list(predictions)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7403b9e-1100-4474-d414-d04a148cca88",
        "id": "qUcdqSMg39zv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 3.303007125854492\n",
            "Training loss per 100 training steps: 2.4594967365264893\n",
            "Training loss per 100 training steps: 1.9063241481781006\n",
            "Training loss per 100 training steps: 1.5344643592834473\n",
            "Training loss per 100 training steps: 1.296858549118042\n",
            "Training loss per 100 training steps: 1.1488431692123413\n",
            "Training loss per 100 training steps: 1.0485862493515015\n",
            "Training loss epoch: 1.0449070930480957\n",
            "Training accuracy epoch: 0.7374204236748969\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.5768224000930786\n",
            "Training loss per 100 training steps: 0.40800416469573975\n",
            "Training loss per 100 training steps: 0.4040577709674835\n",
            "Training loss per 100 training steps: 0.3721134662628174\n",
            "Training loss per 100 training steps: 0.3536314368247986\n",
            "Training loss per 100 training steps: 0.34490257501602173\n",
            "Training loss per 100 training steps: 0.33695557713508606\n",
            "Training loss epoch: 0.335644394159317\n",
            "Training accuracy epoch: 0.9190071138296279\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.4744620621204376\n",
            "Training loss per 100 training steps: 0.2738170921802521\n",
            "Training loss per 100 training steps: 0.2825104892253876\n",
            "Training loss per 100 training steps: 0.25962311029434204\n",
            "Training loss per 100 training steps: 0.2474118322134018\n",
            "Training loss per 100 training steps: 0.24012547731399536\n",
            "Training loss per 100 training steps: 0.23093867301940918\n",
            "Training loss epoch: 0.229994997382164\n",
            "Training accuracy epoch: 0.9437951353784813\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.261127769947052\n",
            "Training loss per 100 training steps: 0.1925136297941208\n",
            "Training loss per 100 training steps: 0.2038111835718155\n",
            "Training loss per 100 training steps: 0.19401656091213226\n",
            "Training loss per 100 training steps: 0.18594029545783997\n",
            "Training loss per 100 training steps: 0.17845658957958221\n",
            "Training loss per 100 training steps: 0.16932815313339233\n",
            "Training loss epoch: 0.16854137182235718\n",
            "Training accuracy epoch: 0.9591154559665112\n",
            "Validation loss per 100 evaluation steps: 0.0725264698266983\n",
            "Validation loss per 100 evaluation steps: 0.09542948633229525\n",
            "Validation loss per 100 evaluation steps: 0.11525199118195407\n",
            "Validation loss per 100 evaluation steps: 0.1292016907653886\n",
            "Validation loss per 100 evaluation steps: 0.1237356031994031\n",
            "Validation loss per 100 evaluation steps: 0.11971154059943355\n",
            "Validation loss per 100 evaluation steps: 0.11690338326931099\n",
            "Validation loss per 100 evaluation steps: 0.11410125585395137\n",
            "Validation loss per 100 evaluation steps: 0.11319818705280826\n",
            "Validation Loss: 0.1124270371995711\n",
            "Validation Accuracy: 0.9713808300848075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.93      0.97      0.95       665\n",
            "           #       0.77      0.99      0.86       267\n",
            "           $       0.94      0.98      0.96       380\n",
            "           &       0.99      0.99      0.99       456\n",
            "           ,       0.99      0.98      0.99      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.98      0.97      0.97      1344\n",
            "           D       0.99      0.99      0.99      1614\n",
            "           E       0.79      0.96      0.87       246\n",
            "           G       0.77      0.48      0.59       257\n",
            "           L       0.99      0.97      0.98       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.98      0.97      0.98      3582\n",
            "           O       0.99      0.99      0.99      1848\n",
            "           P       0.99      0.98      0.98      2281\n",
            "           R       0.95      0.97      0.96      1219\n",
            "           S       1.00      0.46      0.63        28\n",
            "           T       0.92      0.96      0.94       163\n",
            "           U       1.00      0.98      0.99       418\n",
            "           V       1.00      0.99      0.99      3954\n",
            "           X       0.71      0.88      0.79        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.93      0.27      0.42        52\n",
            "           ^       0.93      0.96      0.94      1662\n",
            "           ~       0.94      0.97      0.95       910\n",
            "\n",
            "    accuracy                           0.97     26078\n",
            "   macro avg       0.86      0.83      0.83     26078\n",
            "weighted avg       0.97      0.97      0.97     26078\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving to Directory"
      ],
      "metadata": {
        "id": "ZBjPp5J322w9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = \"./pos_Bert_model_supervised\"\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# save vocabulary of the tokenizer\n",
        "tokenizer.save_vocabulary(directory)\n",
        "# save the model weights and its configuration file\n",
        "model2.save_pretrained(directory)\n",
        "print('All files saved')\n",
        "print('This tutorial is completed')"
      ],
      "metadata": {
        "id": "ZDYYWCYoHLzo",
        "outputId": "5f0b0a2f-f252-4c72-8d41-77b8fd89f554",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All files saved\n",
            "This tutorial is completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ii) Bertweet Large POS"
      ],
      "metadata": {
        "id": "z6w5QpY14U_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training and Testing"
      ],
      "metadata": {
        "id": "nxEu9NbD3K-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer,RobertaTokenizerFast,RobertaForTokenClassification, AutoModelForTokenClassification\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('vinai/bertweet-large', add_prefix_space=True) \n",
        "training_set = dataset(df_train, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n",
        "model2 = RobertaForTokenClassification.from_pretrained('vinai/bertweet-large', num_labels=len(ids_to_labels))\n",
        "model2.resize_token_embeddings(len(tokenizer))\n",
        "device = 'cuda' if cuda.is_available() else 'cpu' #save the processing time\n",
        "model2.to(device)\n",
        "optimizer = torch.optim.Adam(params=model2.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "##Train\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"Training epoch: {epoch + 1}\")\n",
        "  train(epoch,model2)\n",
        "\n",
        "## Test\n",
        "\n",
        "labels, predictions = valid(model2, testing_loader)\n",
        "print(classification_report(list(labels),list(predictions)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676,
          "referenced_widgets": [
            "a40dec93726f434ea8405ff6a41c34e6",
            "236f61e6bf65452f85ebb592b034bdad",
            "c60f759089a14e92a5d687ca0077f005",
            "ff5f634984bd412b85b352e4ebcb8179",
            "8d9f1559a26b441b9536023fcf644eb9",
            "2af1faec988a4d7c828cdc19c62abd9a",
            "26ed428aa2394b6a949364aa5a5374a2",
            "5f97405c67eb4449b18782a101164a9d",
            "47b68ec995474e33a1d43e8e6730269b",
            "39592f03251d4adda3f70fd930682291",
            "160e6f8bdcf34d8582bbede976c363da",
            "469248a11ad94447a56f172f3aa6a4c2",
            "7275ea70e9244c8c971c34ea9847ff34",
            "36c8e0fee6174cfc889534867b2c2ce9",
            "ba9a6c25bd884ade97f6384dfa623402",
            "fed9a726039140d58a358e61736df5f1",
            "4b0ed35e02b14167b9b291ce6db32245",
            "1d5ffdcfb1bf46849500e28877979353",
            "94d5a4cf5eb64faea008a81e6d08153f",
            "9de0170f469c420d9c933d887136bb36",
            "efeb5298a9c046a1b0c59563dec91717",
            "f2f7c1c03dd04d68ae39ccfe590d923e",
            "7d7a03d18d3844f3a83986f6d3d60ad0",
            "64d5531a12b94d6d979ca0613745db70",
            "bb8b015b4a004416b177a1f754e1dd0f",
            "f879b811926a4dcea496e874d4b6244a",
            "7268ce7cb3d34be492ea4c64ce5993e1",
            "d1dbfa85813e4c48aed871d05b8a9e33",
            "02a58104d8964cd589374bb569414f71",
            "172bece8b21442b79666db52e41fb859",
            "b942b63bbe834939939ee632cd2db483",
            "c01a5eb3f682496680f6f091b0e7f846",
            "67f33e7dbfe546df933e735110b7314f",
            "564a31ef8ff3480cbffd58cd94273f25",
            "bd37dcb83cec4de28a4cb44379a69155",
            "b81ba9c96c2f4cc1864a467fd68dd7a2",
            "00bbc6a2b8bb4553a41275643e2678ba",
            "c1aeb64ac598442790c9ab44cf60183a",
            "bf468d28fd9a4c4baac2d685e72656f7",
            "68a06d0f2ee145ad839ccb0dc708af5d",
            "f21ec5d7dbfa4835a06af4abae26445c",
            "a239753ee2234d39b807b0a08262ab08",
            "679862685e4b49159444e07dfe48a1ea",
            "8ff8da2e3e664780a09859345622468f",
            "e1cd6cc0417b49389e39dc8ea5f4ee95",
            "a17dd865b2bc4483ae990e61ac7a1ef6",
            "e42356f2950b46f2ba6e31eb1b7dd518",
            "ffa5d4a5590644c990fc1116b54b99ad",
            "d0d0261c75914cb6a18a765cf7bf6937",
            "c7d876d889f74e1caad20f010344e395",
            "26b9385e84ad49b9a21b1cae9f3b199d",
            "a11d1ad5c9c54e4d9fb98e61685513f3",
            "0847c43a91b04f528795374e968df409",
            "9f93919f36204647957954a9d894d064",
            "1cdfefb3137a478db13c823785d5f68d"
          ]
        },
        "outputId": "e328ba63-1add-4ae4-de13-b8a7c3b9fcb5",
        "id": "Jbj63dGvpZox"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a40dec93726f434ea8405ff6a41c34e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "469248a11ad94447a56f172f3aa6a4c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d7a03d18d3844f3a83986f6d3d60ad0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "564a31ef8ff3480cbffd58cd94273f25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.32G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1cd6cc0417b49389e39dc8ea5f4ee95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-large were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-2edaa7a212ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training epoch: {epoch + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m## Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-72a5dc4eb2f8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, modelx)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodelx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-161d03d12856>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     31\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# overwrite label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mencoded_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving to Directory"
      ],
      "metadata": {
        "id": "vOe3OjcX3P_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = \"./pos_Bertweet_model_supervised\"\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# save vocabulary of the tokenizer\n",
        "tokenizer.save_vocabulary(directory)\n",
        "# save the model weights and its configuration file\n",
        "model2.save_pretrained(directory)\n",
        "print('All files saved')\n",
        "print('This tutorial is completed')"
      ],
      "metadata": {
        "outputId": "5f0b0a2f-f252-4c72-8d41-77b8fd89f554",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rChB9LCg3P_j"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All files saved\n",
            "This tutorial is completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iii) Roberta POS"
      ],
      "metadata": {
        "id": "_BlQDdJnnXZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training and Testing"
      ],
      "metadata": {
        "id": "nUVnYQz93cR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer,RobertaTokenizerFast,RobertaForTokenClassification, AutoModelForTokenClassification\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', add_prefix_space=True) \n",
        "training_set = dataset(df_train, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n",
        "model2 = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=len(ids_to_labels))\n",
        "model2.resize_token_embeddings(len(tokenizer))\n",
        "device = 'cuda' if cuda.is_available() else 'cpu' #save the processing time\n",
        "model2.to(device)\n",
        "optimizer = torch.optim.Adam(params=model2.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "##Train\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"Training epoch: {epoch + 1}\")\n",
        "  train(epoch,model2)\n",
        "\n",
        "## Test\n",
        "\n",
        "labels, predictions = valid(model2, testing_loader)\n",
        "print(classification_report(list(labels),list(predictions)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783804e0-7663-4c3b-fdcb-b5fe64c58c5f",
        "id": "r-x5QbQgnWXD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 3.221292495727539\n",
            "Training loss per 100 training steps: 2.284536600112915\n",
            "Training loss per 100 training steps: 1.650243878364563\n",
            "Training loss per 100 training steps: 1.3123759031295776\n",
            "Training loss per 100 training steps: 1.1220260858535767\n",
            "Training loss epoch: 1.0858522653579712\n",
            "Training accuracy epoch: 0.724190485953213\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.8809526562690735\n",
            "Training loss per 100 training steps: 0.42844945192337036\n",
            "Training loss per 100 training steps: 0.39512425661087036\n",
            "Training loss per 100 training steps: 0.3684934973716736\n",
            "Training loss per 100 training steps: 0.3624682128429413\n",
            "Training loss epoch: 0.35746556520462036\n",
            "Training accuracy epoch: 0.9144826681089802\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.8251553177833557\n",
            "Training loss per 100 training steps: 0.29501721262931824\n",
            "Training loss per 100 training steps: 0.2745341360569\n",
            "Training loss per 100 training steps: 0.2582218050956726\n",
            "Training loss per 100 training steps: 0.25942787528038025\n",
            "Training loss epoch: 0.2561388909816742\n",
            "Training accuracy epoch: 0.9390558398356006\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.7164624929428101\n",
            "Training loss per 100 training steps: 0.22854457795619965\n",
            "Training loss per 100 training steps: 0.20871110260486603\n",
            "Training loss per 100 training steps: 0.193425714969635\n",
            "Training loss per 100 training steps: 0.19808076322078705\n",
            "Training loss epoch: 0.19550630450248718\n",
            "Training accuracy epoch: 0.9528133995371515\n",
            "Validation loss per 100 evaluation steps: 0.27416834235191345\n",
            "Validation loss per 100 evaluation steps: 0.5092148914435269\n",
            "Validation loss per 100 evaluation steps: 0.47305821896581657\n",
            "Validation Loss: 0.42369870768182655\n",
            "Validation Accuracy: 0.9110534080020716\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.87      0.86      0.87       198\n",
            "           #       0.76      0.98      0.86        83\n",
            "           $       0.86      0.99      0.92        96\n",
            "           &       0.99      0.98      0.99       145\n",
            "           ,       0.96      0.95      0.95       942\n",
            "           @       0.97      0.97      0.97       372\n",
            "           A       0.87      0.84      0.85       406\n",
            "           D       0.92      0.96      0.94       505\n",
            "           E       0.86      0.93      0.89        67\n",
            "           G       0.38      0.34      0.36        80\n",
            "           L       0.95      0.93      0.94       140\n",
            "           N       0.90      0.89      0.89      1091\n",
            "           O       0.95      0.94      0.95       536\n",
            "           P       0.94      0.96      0.95       687\n",
            "           R       0.89      0.85      0.87       370\n",
            "           S       1.00      0.17      0.29         6\n",
            "           T       0.72      0.83      0.77        41\n",
            "           U       0.98      0.96      0.97       133\n",
            "           V       0.94      0.96      0.95      1154\n",
            "           X       0.00      0.00      0.00         7\n",
            "           Z       0.00      0.00      0.00        24\n",
            "           ^       0.83      0.86      0.84       545\n",
            "           ~       0.92      0.91      0.92       233\n",
            "\n",
            "    accuracy                           0.91      7861\n",
            "   macro avg       0.80      0.78      0.78      7861\n",
            "weighted avg       0.91      0.91      0.91      7861\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving to Directory"
      ],
      "metadata": {
        "id": "_H757C_k3fNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = \"./pos_Roberta_model_supervised\"\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# save vocabulary of the tokenizer\n",
        "tokenizer.save_vocabulary(directory)\n",
        "# save the model weights and its configuration file\n",
        "model2.save_pretrained(directory)\n",
        "print('All files saved')\n",
        "print('This tutorial is completed')"
      ],
      "metadata": {
        "outputId": "5f0b0a2f-f252-4c72-8d41-77b8fd89f554",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOtZNUts3fNs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All files saved\n",
            "This tutorial is completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Unsupervised POS Model with Self-Training"
      ],
      "metadata": {
        "id": "GJmtINY-HYZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a) Step-by-Step"
      ],
      "metadata": {
        "id": "pQpq3stG4GNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "1I672dnP4MM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unlabelled_tweets = drive.CreateFile({'id':\"1yssTP5EJQQLcE3oJi5LKqTYkznqm-pYK\"})\n",
        "unlabelled_tweets.GetContentFile(\"st_tweets.txt\")"
      ],
      "metadata": {
        "id": "P-kr7FXYHUWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Pre-Processing"
      ],
      "metadata": {
        "id": "2lxp21YxJdhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"st_tweets.txt\") as f:\n",
        "  new_tweets = f.readlines()"
      ],
      "metadata": {
        "id": "Qw87pZwMj-sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_raw_df(txt_list):\n",
        "  i = 1\n",
        "  all_list = [token.split() for token in txt_list]\n",
        "  for l in all_list:\n",
        "    if l == []:\n",
        "      i += 1\n",
        "    else:\n",
        "      l.append(f'sentence {i}')\n",
        "  all_list = [x for x in all_list if x]\n",
        "  df = pd.DataFrame(all_list, columns=[\"id\", \"tag\", \"sentence number\"])\n",
        "  return df\n",
        "\n",
        "nt = get_raw_df(new_tweets)"
      ],
      "metadata": {
        "id": "EL6aQPmloCa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nt[\"sentence\"] = nt[[\"sentence number\", \"id\", \"tag\"]].groupby([\"sentence number\"])[\"id\"].transform(lambda x: \" \".join(x))\n",
        "nt[\"labels\"] = nt[[\"sentence number\", \"id\", \"tag\"]].groupby([\"sentence number\"])[\"tag\"].transform(lambda x: \" \".join(x))\n",
        "\n",
        "def get_sentence_labels(raw_df):\n",
        "  return raw_df[[\"sentence\", \"labels\"]].drop_duplicates().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "4wv9ewkToa1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_to_ids = {',':0, '&':1, '$':2, 'D':3,'X':4 ,'G':5, 'P':6, 'A':7, 'V':8, 'N':9, '^':10, 'O':11, 'R':12, '!':13, 'S':14, 'Z':15, 'L':16, 'M':17, '#':18, '@':19, '~':20, 'U':21, 'E':22, 'T':23,'Y':24}\n",
        "ids_to_labels = dict((v,k) for k,v in labels_to_ids.items())"
      ],
      "metadata": {
        "id": "PVje0QTlI3nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_labels(raw_df):\n",
        "  return raw_df[[\"sentence\", \"labels\"]].drop_duplicates().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "ZWmXZudpJKNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = get_sentence_labels(nt)\n",
        "#df_valid = get_sentence_labels(df_valid)\n",
        "df_test = get_sentence_labels(test_tweets)"
      ],
      "metadata": {
        "id": "txw_xP4gJKNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_sim(x):\n",
        "  temp = x.split(\" \")\n",
        "  new = list()\n",
        "  for i in temp:\n",
        "    new.append(\"O\")\n",
        "  return \" \".join(new)\n",
        "\n",
        "df_train['labels'] = df_train['labels'].apply(lambda x:label_sim(x))"
      ],
      "metadata": {
        "id": "yjEiokFftIuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[0:2000]\n",
        "df_train"
      ],
      "metadata": {
        "id": "JdNsUADcwq6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-Shot Predictions"
      ],
      "metadata": {
        "id": "EvCzwa9xJhKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, AutoConfig, BertForTokenClassification\n",
        "pretrained_model_name = \"/content/drive/MyDrive/pos_Bert_model_supervised\"\n",
        "\n",
        "def model_prediction(pretrained_model_name):\n",
        "  from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, add_prefix_space=True) \n",
        "  model = AutoModelForTokenClassification.from_pretrained(pretrained_model_name)\n",
        "  device = 'cuda' if cuda.is_available() else 'cpu' #save the processing time\n",
        "  model.to(device)\n",
        "  return model, tokenizer\n",
        "model, tokenizer = model_prediction(pretrained_model_name)\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "sMriv7svJsZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {'batch_size': 1,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': 1,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "training_set = dataset(df_train, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "metadata": {
        "id": "ageT8Cv0JsW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(model, data_loader):\n",
        "    all_sentences = []\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(data_loader):\n",
        "            sentence_prediction = {}\n",
        "            \n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['labels'].to(device, dtype = torch.long)\n",
        "\n",
        "            #loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "            output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        " \n",
        "            if idx % 100==0:\n",
        "                print(f\"Finished predicting {idx} sentences\")\n",
        "            \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = output[1].view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            \n",
        "            # compute active probability using softmax function\n",
        "            active_probability = torch.nn.functional.softmax(active_logits, dim=1)\n",
        "            flattened_probability = torch.argmax(active_probability, axis=1)\n",
        "            \n",
        "            # only compute accuracy at active labels\n",
        "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        \n",
        "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            probabilities = torch.masked_select(flattened_probability, active_accuracy)\n",
        "            masks = torch.masked_select(mask, active_accuracy).tolist()\n",
        "            input_ids = torch.masked_select(ids, active_accuracy).tolist()\n",
        "\n",
        "            prob_score = []\n",
        "            for ind, prob in enumerate(probabilities):\n",
        "              prob_score.append(float(active_probability[ind][prob]))\n",
        "            \n",
        "            sentence_prediction[\"Attention masks\"] = masks\n",
        "            sentence_prediction[\"Input ids\"] = input_ids\n",
        "            sentence_prediction[\"Token\"] = [tokenizer.convert_ids_to_tokens(input_id) for input_id in input_ids]\n",
        "            sentence_prediction[\"Actual labels\"] = [ids_to_labels[int(x)] for x in labels]\n",
        "            sentence_prediction[\"Probability\"] = [x for x in prob_score]\n",
        "            sentence_prediction[\"Predicted labels\"] = [ids_to_labels[int(x)] for x in predictions]\n",
        "            sentence_prediction[\"Sentence number\"] = [idx] * len(input_ids)\n",
        "            all_sentences.append(sentence_prediction)\n",
        "\n",
        "    return all_sentences"
      ],
      "metadata": {
        "id": "EvHP_j48JsUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = prediction(model, training_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "Flhs-pukJsRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_predictions = pd.DataFrame(predictions)\n"
      ],
      "metadata": {
        "id": "qxtH4oxwJrxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_predictions"
      ],
      "metadata": {
        "id": "Em3chcKW9lze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_directory(df, iteration_time = 0):\n",
        "  df.to_csv(f\"./processed_data_{iteration_time}.csv\")  \n",
        "  import os\n",
        "  datafile = f\"./processed_data_{iteration_time}.csv\"\n",
        "  d = f\"processed_data_{iteration_time}.csv\"\n",
        "  directory = f'./unsupervised_selftraining_iteration_{iteration_time}'\n",
        "  if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "  completeName = os.path.join(directory, d)\n",
        "  f = open(completeName, \"w\")\n",
        "  f.write(datafile)\n",
        "  f.close()\n",
        "  #!cp $datafile directory"
      ],
      "metadata": {
        "id": "6K4Nlege9pFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory(df_predictions, iteration_time = 0)"
      ],
      "metadata": {
        "id": "Sn5RqmGf9sex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "7uZ8twOu982F",
        "outputId": "1b9af31d-6cb8-4ef1-cb87-523c18b0bddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               sentence  \\\n",
              "0     I hate the words chunder , vomit and puke . BU...   \n",
              "1     ♥ . . ) ) ( ♫ . ( ) ♫ . ♥ . « ▓ » ♥ . ♫ . . ╝ ...   \n",
              "2     Alesan kenapa mlm kita lbh srg galau Poconggg ...   \n",
              "3       Complete Tosca on the tube http://t.co/O90deSLB   \n",
              "4     Think you call that smash and grab . # Gateshe...   \n",
              "...                                                 ...   \n",
              "1995  I wrote so much on the Tax Evasion question th...   \n",
              "1996                      Sigh , no1 takes me seriously   \n",
              "1997  Need to revise for my exam on friday but I jus...   \n",
              "1998       revision is destroying me , i need alcohol .   \n",
              "1999  Bitch ass idiots RT @ kukuobienu26 : dear hall...   \n",
              "\n",
              "                                                 labels  \n",
              "0                               O O O O O O O O O O O O  \n",
              "1     O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
              "2     O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
              "3                                           O O O O O O  \n",
              "4     O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
              "...                                                 ...  \n",
              "1995  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
              "1996                                        O O O O O O  \n",
              "1997                            O O O O O O O O O O O O  \n",
              "1998                                  O O O O O O O O O  \n",
              "1999  O O O O O O O O O O O O O O O O O O O O O O O O O  \n",
              "\n",
              "[2000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be7574b9-c4a1-4b26-849d-4c90b078dd62\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I hate the words chunder , vomit and puke . BU...</td>\n",
              "      <td>O O O O O O O O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>♥ . . ) ) ( ♫ . ( ) ♫ . ♥ . « ▓ » ♥ . ♫ . . ╝ ...</td>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Alesan kenapa mlm kita lbh srg galau Poconggg ...</td>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Complete Tosca on the tube http://t.co/O90deSLB</td>\n",
              "      <td>O O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Think you call that smash and grab . # Gateshe...</td>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>I wrote so much on the Tax Evasion question th...</td>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>Sigh , no1 takes me seriously</td>\n",
              "      <td>O O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>Need to revise for my exam on friday but I jus...</td>\n",
              "      <td>O O O O O O O O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>revision is destroying me , i need alcohol .</td>\n",
              "      <td>O O O O O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>Bitch ass idiots RT @ kukuobienu26 : dear hall...</td>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O O O O O O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be7574b9-c4a1-4b26-849d-4c90b078dd62')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-be7574b9-c4a1-4b26-849d-4c90b078dd62 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-be7574b9-c4a1-4b26-849d-4c90b078dd62');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1st Iteration"
      ],
      "metadata": {
        "id": "LqOgpUMc9__h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(iteration_time):\n",
        "  df_predictions = pd.read_csv(f\"./processed_data_{iteration_time - 1}.csv\")\n",
        "  df_predictions = df_predictions.drop(columns=['Unnamed: 0'])\n",
        "  import ast\n",
        "  for column in list(df_predictions.columns):\n",
        "    df_predictions[column] = df_predictions[column].apply(ast.literal_eval)\n",
        "  #get the average predicted probability of each sentence to find the most confident sentences\n",
        "  df_predictions[\"Average probability\"] = df_predictions[\"Probability\"].apply(np.mean)\n",
        "  return df_predictions"
      ],
      "metadata": {
        "id": "M60fe2H398zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_data(1).head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "eFXh0nZG98wT",
        "outputId": "66d185ef-dcb9-42a4-e0fc-6826d7365c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     Attention masks  \\\n",
              "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
              "4                                    [1, 1, 1, 1, 1]   \n",
              "\n",
              "                                           Input ids  \\\n",
              "0  [10439, 4007, 1024, 1056, 2097, 8116, 1996, 25...   \n",
              "1  [19387, 1030, 1024, 1001, 2000, 1030, 2016, 20...   \n",
              "2  [2085, 2054, 1045, 2079, 5223, 2003, 2005, 103...   \n",
              "3  [8224, 1024, 2367, 13792, 2064, 6611, 2000, 23...   \n",
              "4                     [1045, 2293, 2026, 23279, 999]   \n",
              "\n",
              "                                               Token  \\\n",
              "0  [app, software, :, t, will, deliver, the, blac...   \n",
              "1  [rt, @, :, #, to, @, she, is, a, dan, fool, .,...   \n",
              "2  [now, what, i, do, hate, is, for, a, man, to, ...   \n",
              "3  [google, :, different, algorithms, can, apply,...   \n",
              "4                          [i, love, my, celtics, !]   \n",
              "\n",
              "                                       Actual labels  \\\n",
              "0  [^, ^, ,, ^, V, V, D, ^, ^, $, P, ^, $, ,, ^, ...   \n",
              "1  [~, @, ~, #, P, @, O, V, D, A, N, ,, V, V, D, ...   \n",
              "2  [R, O, O, V, V, V, P, D, N, P, V, L, A, P, V, ...   \n",
              "3      [^, ,, A, N, V, V, P, A, N, P, A, N, U, P, @]   \n",
              "4                                    [O, V, D, ^, ,]   \n",
              "\n",
              "                                         Probability  \\\n",
              "0  [0.07004626095294952, 0.9835553765296936, 0.00...   \n",
              "1  [0.01052755769342184, 0.00029815133893862367, ...   \n",
              "2  [0.0101936561986804, 8.414717740379274e-05, 0....   \n",
              "3  [0.13157965242862701, 0.0004314166435506195, 0...   \n",
              "4  [0.004911411087960005, 0.0002935265947598964, ...   \n",
              "\n",
              "                                    Predicted labels  \\\n",
              "0  [^, ^, ,, ^, V, V, D, ^, ^, $, P, ^, $, ,, ^, ...   \n",
              "1  [~, @, ~, #, P, @, O, V, D, A, N, ,, V, V, D, ...   \n",
              "2  [R, O, O, V, V, V, P, D, N, P, V, L, A, P, V, ...   \n",
              "3      [^, ,, A, N, V, V, P, A, N, P, A, N, U, P, @]   \n",
              "4                                    [O, V, D, ^, ,]   \n",
              "\n",
              "                                     Sentence number  Average probability  \n",
              "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             0.243032  \n",
              "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...             0.034283  \n",
              "2  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...             0.150034  \n",
              "3      [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]             0.075836  \n",
              "4                                    [4, 4, 4, 4, 4]             0.001245  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ba7234fd-e4e0-48ba-8ac1-c0912671e72c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Attention masks</th>\n",
              "      <th>Input ids</th>\n",
              "      <th>Token</th>\n",
              "      <th>Actual labels</th>\n",
              "      <th>Probability</th>\n",
              "      <th>Predicted labels</th>\n",
              "      <th>Sentence number</th>\n",
              "      <th>Average probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[10439, 4007, 1024, 1056, 2097, 8116, 1996, 25...</td>\n",
              "      <td>[app, software, :, t, will, deliver, the, blac...</td>\n",
              "      <td>[^, ^, ,, ^, V, V, D, ^, ^, $, P, ^, $, ,, ^, ...</td>\n",
              "      <td>[0.07004626095294952, 0.9835553765296936, 0.00...</td>\n",
              "      <td>[^, ^, ,, ^, V, V, D, ^, ^, $, P, ^, $, ,, ^, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.243032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[19387, 1030, 1024, 1001, 2000, 1030, 2016, 20...</td>\n",
              "      <td>[rt, @, :, #, to, @, she, is, a, dan, fool, .,...</td>\n",
              "      <td>[~, @, ~, #, P, @, O, V, D, A, N, ,, V, V, D, ...</td>\n",
              "      <td>[0.01052755769342184, 0.00029815133893862367, ...</td>\n",
              "      <td>[~, @, ~, #, P, @, O, V, D, A, N, ,, V, V, D, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>0.034283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2085, 2054, 1045, 2079, 5223, 2003, 2005, 103...</td>\n",
              "      <td>[now, what, i, do, hate, is, for, a, man, to, ...</td>\n",
              "      <td>[R, O, O, V, V, V, P, D, N, P, V, L, A, P, V, ...</td>\n",
              "      <td>[0.0101936561986804, 8.414717740379274e-05, 0....</td>\n",
              "      <td>[R, O, O, V, V, V, P, D, N, P, V, L, A, P, V, ...</td>\n",
              "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
              "      <td>0.150034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
              "      <td>[8224, 1024, 2367, 13792, 2064, 6611, 2000, 23...</td>\n",
              "      <td>[google, :, different, algorithms, can, apply,...</td>\n",
              "      <td>[^, ,, A, N, V, V, P, A, N, P, A, N, U, P, @]</td>\n",
              "      <td>[0.13157965242862701, 0.0004314166435506195, 0...</td>\n",
              "      <td>[^, ,, A, N, V, V, P, A, N, P, A, N, U, P, @]</td>\n",
              "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
              "      <td>0.075836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1, 1, 1, 1, 1]</td>\n",
              "      <td>[1045, 2293, 2026, 23279, 999]</td>\n",
              "      <td>[i, love, my, celtics, !]</td>\n",
              "      <td>[O, V, D, ^, ,]</td>\n",
              "      <td>[0.004911411087960005, 0.0002935265947598964, ...</td>\n",
              "      <td>[O, V, D, ^, ,]</td>\n",
              "      <td>[4, 4, 4, 4, 4]</td>\n",
              "      <td>0.001245</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba7234fd-e4e0-48ba-8ac1-c0912671e72c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ba7234fd-e4e0-48ba-8ac1-c0912671e72c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ba7234fd-e4e0-48ba-8ac1-c0912671e72c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence-Level Selection & Prediction"
      ],
      "metadata": {
        "id": "I_P0NcYZ-a-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def new_split(df_train, iteration_time, top_percent = 0.10):\n",
        "  import math\n",
        "  df_predictions = get_data(iteration_time)\n",
        "  #get sentences with predicted labels and replace the original true labels with the predicted most confident labels\n",
        "  df_train[\"labels\"] = df_predictions[\"Predicted labels\"].str.join(\" \")\n",
        "  df_predictions = df_predictions.sort_values(by=['Average probability'], ascending=False)\n",
        "  df_new_train = df_predictions.iloc[:(math.ceil(len(df_predictions) * top_percent)) + 1, : ]\n",
        "  df_prediction = df_predictions.iloc[(math.ceil(len(df_predictions) * top_percent)) + 1:, : ]\n",
        "  df_train_id = list(df_new_train.index)\n",
        "  df_prediction_id = list(df_prediction.index)\n",
        "  new_train = df_train.take(df_train_id)\n",
        "  new_prediction = df_train.take(df_prediction_id)\n",
        "  return new_train, new_prediction"
      ],
      "metadata": {
        "id": "o3WSgmzZ98tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train, new_prediction = new_split(df_train, 1, top_percent = 0.10)"
      ],
      "metadata": {
        "id": "dTQcXybD98n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "KcJoDZU3-jGN",
        "outputId": "5b5cf2b4-19eb-4bb4-b120-4d390ae55bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               sentence  \\\n",
              "1508                                             o____O   \n",
              "1796                                                O_o   \n",
              "1737   T______________________________________________T   \n",
              "1749                                            etc etc   \n",
              "792                              ROCKY HORROR #GLEE !!!   \n",
              "...                                                 ...   \n",
              "510                                  Wait But Why Tho ?   \n",
              "1448                                              Lamar   \n",
              "990   Offer Floating Form Capture Sites to All Sorts...   \n",
              "605   @MsYellaMulann @Ra_StayViolatin call us now lo...   \n",
              "824      I'm selling my snorkle , text me for details .   \n",
              "\n",
              "                             labels  \n",
              "1508                              E  \n",
              "1796                              E  \n",
              "1737                              E  \n",
              "1749                            G G  \n",
              "792                         ^ ^ ^ ,  \n",
              "...                             ...  \n",
              "510                       V & R R ,  \n",
              "1448                              ^  \n",
              "990   V A N N N P D N P N & V V , U  \n",
              "605                 @ @ V O R ! R R  \n",
              "824             L V D N , V O P N ,  \n",
              "\n",
              "[183 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7625a6c2-daf5-4e8a-b8c0-e37ca5761f13\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1508</th>\n",
              "      <td>o____O</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1796</th>\n",
              "      <td>O_o</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1737</th>\n",
              "      <td>T______________________________________________T</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1749</th>\n",
              "      <td>etc etc</td>\n",
              "      <td>G G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792</th>\n",
              "      <td>ROCKY HORROR #GLEE !!!</td>\n",
              "      <td>^ ^ ^ ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>Wait But Why Tho ?</td>\n",
              "      <td>V &amp; R R ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1448</th>\n",
              "      <td>Lamar</td>\n",
              "      <td>^</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990</th>\n",
              "      <td>Offer Floating Form Capture Sites to All Sorts...</td>\n",
              "      <td>V A N N N P D N P N &amp; V V , U</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>605</th>\n",
              "      <td>@MsYellaMulann @Ra_StayViolatin call us now lo...</td>\n",
              "      <td>@ @ V O R ! R R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>824</th>\n",
              "      <td>I'm selling my snorkle , text me for details .</td>\n",
              "      <td>L V D N , V O P N ,</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>183 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7625a6c2-daf5-4e8a-b8c0-e37ca5761f13')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7625a6c2-daf5-4e8a-b8c0-e37ca5761f13 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7625a6c2-daf5-4e8a-b8c0-e37ca5761f13');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def update_model(new_train, tokenizer, iteration_time):\n",
        "  train_params = {'batch_size': 1,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "  training_set = dataset(new_train, tokenizer, MAX_LEN)\n",
        "  training_loader = DataLoader(training_set, **train_params)\n",
        "  EPOCHS = 1\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)\n",
        "  \n",
        "  import os\n",
        "  directory = f\"./unsupervised_selftraining_iteration_{iteration_time}\"\n",
        "\n",
        "  if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "  # save vocabulary of the tokenizer\n",
        "  tokenizer.save_vocabulary(directory)\n",
        "  # save the model weights and its configuration file\n",
        "  model.save_pretrained(directory)\n",
        "  print(f'iteration {iteration_time} updated model saved')"
      ],
      "metadata": {
        "id": "pMlGRfuh-jDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "update_model(new_train, tokenizer, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gobDAwS0-jAw",
        "outputId": "8d834b6f-97f6-4ffe-e8b3-2533475e891f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.06677476316690445\n",
            "Training loss per 100 training steps: 0.1011890172958374\n",
            "Training loss per 100 training steps: 0.10542170703411102\n",
            "Training loss per 100 training steps: 0.09702112525701523\n",
            "Training loss per 100 training steps: 0.10318300127983093\n",
            "Training loss per 100 training steps: 0.09993240982294083\n",
            "Training loss per 100 training steps: 0.10043804347515106\n",
            "Training loss per 100 training steps: 0.09880430996417999\n",
            "Training loss per 100 training steps: 0.09754091501235962\n",
            "Training loss per 100 training steps: 0.09413120895624161\n",
            "Training loss per 100 training steps: 0.09354665130376816\n",
            "Training loss per 100 training steps: 0.09267427027225494\n",
            "Training loss per 100 training steps: 0.09263152629137039\n",
            "Training loss per 100 training steps: 0.09272687137126923\n",
            "Training loss per 100 training steps: 0.08996803313493729\n",
            "Training loss per 100 training steps: 0.0881243646144867\n",
            "Training loss per 100 training steps: 0.08939740061759949\n",
            "Training loss per 100 training steps: 0.08802590519189835\n",
            "Training loss per 100 training steps: 0.09016839414834976\n",
            "Training loss epoch: 0.09031297266483307\n",
            "Training accuracy epoch: 0.9755118167540467\n",
            "iteration 1 updated model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###First test"
      ],
      "metadata": {
        "id": "DdiI0bHz-oYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "metadata": {
        "id": "b0pdzOvA-i-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_1st, predictions_1st = valid(model, testing_loader)\n",
        "print(classification_report(labels_1st, predictions_1st))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BjvFZ_x-i7r",
        "outputId": "7e417017-145f-43a3-b799-98fbbbc75fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss per 100 evaluation steps: 0.07998191565275192\n",
            "Validation loss per 100 evaluation steps: 0.1656762727001442\n",
            "Validation loss per 100 evaluation steps: 0.16346552662449468\n",
            "Validation loss per 100 evaluation steps: 0.18683158838696023\n",
            "Validation loss per 100 evaluation steps: 0.19791256939060986\n",
            "Validation loss per 100 evaluation steps: 0.19198175886540883\n",
            "Validation loss per 100 evaluation steps: 0.20564081422893948\n",
            "Validation loss per 100 evaluation steps: 0.20183412099128242\n",
            "Validation loss per 100 evaluation steps: 0.19707528134859104\n",
            "Validation loss per 100 evaluation steps: 0.196503142492518\n",
            "Validation loss per 100 evaluation steps: 0.1958039433520462\n",
            "Validation loss per 100 evaluation steps: 0.1970498044220147\n",
            "Validation loss per 100 evaluation steps: 0.20006484588934612\n",
            "Validation loss per 100 evaluation steps: 0.20197266906995232\n",
            "Validation loss per 100 evaluation steps: 0.1984124934890917\n",
            "Validation loss per 100 evaluation steps: 0.19710656888221453\n",
            "Validation loss per 100 evaluation steps: 0.19314771371883113\n",
            "Validation loss per 100 evaluation steps: 0.18946734254154268\n",
            "Validation Loss: 0.19552317121013305\n",
            "Validation Accuracy: 0.9589889714813796\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.72      0.98      0.83       665\n",
            "           #       0.73      0.99      0.84       267\n",
            "           $       0.96      0.98      0.97       380\n",
            "           &       0.99      0.99      0.99       456\n",
            "           ,       0.97      0.99      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.98      0.94      0.96      1344\n",
            "           D       0.99      0.98      0.99      1614\n",
            "           E       0.72      0.96      0.82       246\n",
            "           G       0.82      0.31      0.45       257\n",
            "           L       0.99      0.96      0.98       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.98      0.96      0.97      3582\n",
            "           O       0.99      0.98      0.99      1848\n",
            "           P       0.98      0.99      0.98      2281\n",
            "           R       0.94      0.95      0.94      1219\n",
            "           S       1.00      0.36      0.53        28\n",
            "           T       0.91      0.92      0.92       163\n",
            "           U       0.99      0.98      0.98       418\n",
            "           V       0.99      0.99      0.99      3954\n",
            "           X       0.68      0.84      0.75        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.00      0.00      0.00        52\n",
            "           ^       0.94      0.89      0.92      1662\n",
            "           ~       0.97      0.90      0.93       910\n",
            "\n",
            "    accuracy                           0.96     26078\n",
            "   macro avg       0.81      0.79      0.79     26078\n",
            "weighted avg       0.96      0.96      0.96     26078\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Model after first iteration"
      ],
      "metadata": {
        "id": "37y7zf0j-1eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(iteration_time = 1):\n",
        "  import os\n",
        "  #directory_0 = f'./unsupervised_selftraining_iteration_{iteration_time}'\n",
        "  directory_1 = f'./unsupervised_selftraining_iteration_{iteration_time}/model'\n",
        "\n",
        "  #if not os.path.exists(directory_0):\n",
        "      #os.makedirs(directory_0)\n",
        "\n",
        "  if not os.path.exists(directory_1):\n",
        "      os.makedirs(directory_1)\n",
        "\n",
        "  # save vocabulary of the tokenizer\n",
        "  tokenizer.save_vocabulary(directory_1)\n",
        "  # save the model weights and its configuration file\n",
        "  model.save_pretrained(directory_1)\n",
        "  print(f'tokenizer and model saved after {iteration_time} iteration ')"
      ],
      "metadata": {
        "id": "ytsd7SxT-zQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(iteration_time = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_UDfyNz-zOk",
        "outputId": "917be6d2-e942-4f89-85f9-1634b7c27028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer and model saved after 1 iteration \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Produce Data for Next Iteration"
      ],
      "metadata": {
        "id": "4QvxMpbh_DQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_new_prediction(new_prediction):\n",
        "  new_prediction = new_prediction.reset_index(inplace = False)\n",
        "  new_prediction = new_prediction.drop(columns=\"index\")\n",
        "  params = {'batch_size': 1,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "  prediction_set = dataset(new_prediction, tokenizer, MAX_LEN)\n",
        "  prediction_loader = DataLoader(prediction_set, **params)\n",
        "  predictions = prediction(model, prediction_loader)\n",
        "  df_predictions = pd.DataFrame(predictions)\n",
        "  return df_predictions"
      ],
      "metadata": {
        "id": "owDwDhxa-zMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_prediction = new_prediction.reset_index(inplace = False)\n",
        "new_prediction = new_prediction.drop(columns=\"index\")\n",
        "new_prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "cwc3lW95-zJH",
        "outputId": "3512c503-296e-4d64-b115-775f2ae897d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               sentence  \\\n",
              "0     RT @iheartquotes : \" Who are u chasin that's d...   \n",
              "1     Ahh ... Bayou State Politics . Charlie Melanco...   \n",
              "2                               Yll wis u live in MIAMI   \n",
              "3     RT @PayPalx #xinnovate pays the cost of develo...   \n",
              "4     RT @_twinklyToes What was that dance Nicki jus...   \n",
              "...                                                 ...   \n",
              "1627  @Broslife I RODE A TRICYCLE LAST NIGHT ! I'm g...   \n",
              "1628  In November , bring a friend/refer a friend fo...   \n",
              "1629  Inspired after a yummy lunch with @erinepage ....   \n",
              "1630  in town , dropped off photos , now having lunch !   \n",
              "1631  i think its wrong people judge others by there...   \n",
              "\n",
              "                                                 labels  \n",
              "0     ~ @ ~ , O V O V L V O , O V O V L V D N , V , ...  \n",
              "1               ! ~ ^ N N , ^ ^ & ^ ^ V A N P ^ N N , U  \n",
              "2                                           O V O V P ^  \n",
              "3                       ~ @ # V D N P N N R P V $ N P ^  \n",
              "4                             ~ @ O V D N ^ R V O V V T  \n",
              "...                                                 ...  \n",
              "1627                  @ O V D N A N , L V P V O P D N ,  \n",
              "1628                    P ^ , V D N D N P ^ & O V $ P ,  \n",
              "1629                                V P D A N P @ , ! ,  \n",
              "1630                              P N , V T N , R V N ,  \n",
              "1631                  O V L A N V N P D N , O V P D N ,  \n",
              "\n",
              "[1632 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c79f5fa-d3f8-4b2b-b201-e38b54bf98c5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RT @iheartquotes : \" Who are u chasin that's d...</td>\n",
              "      <td>~ @ ~ , O V O V L V O , O V O V L V D N , V , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ahh ... Bayou State Politics . Charlie Melanco...</td>\n",
              "      <td>! ~ ^ N N , ^ ^ &amp; ^ ^ V A N P ^ N N , U</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Yll wis u live in MIAMI</td>\n",
              "      <td>O V O V P ^</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT @PayPalx #xinnovate pays the cost of develo...</td>\n",
              "      <td>~ @ # V D N P N N R P V $ N P ^</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @_twinklyToes What was that dance Nicki jus...</td>\n",
              "      <td>~ @ O V D N ^ R V O V V T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1627</th>\n",
              "      <td>@Broslife I RODE A TRICYCLE LAST NIGHT ! I'm g...</td>\n",
              "      <td>@ O V D N A N , L V P V O P D N ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1628</th>\n",
              "      <td>In November , bring a friend/refer a friend fo...</td>\n",
              "      <td>P ^ , V D N D N P ^ &amp; O V $ P ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1629</th>\n",
              "      <td>Inspired after a yummy lunch with @erinepage ....</td>\n",
              "      <td>V P D A N P @ , ! ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1630</th>\n",
              "      <td>in town , dropped off photos , now having lunch !</td>\n",
              "      <td>P N , V T N , R V N ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1631</th>\n",
              "      <td>i think its wrong people judge others by there...</td>\n",
              "      <td>O V L A N V N P D N , O V P D N ,</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1632 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c79f5fa-d3f8-4b2b-b201-e38b54bf98c5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4c79f5fa-d3f8-4b2b-b201-e38b54bf98c5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4c79f5fa-d3f8-4b2b-b201-e38b54bf98c5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "M4TEmA3i-zG8",
        "outputId": "d3c1384d-c042-4d1b-c4bc-c63fbfec0673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               sentence  \\\n",
              "0     RT @iheartquotes : \" Who are u chasin that's d...   \n",
              "1     Ahh ... Bayou State Politics . Charlie Melanco...   \n",
              "2                               Yll wis u live in MIAMI   \n",
              "3     RT @PayPalx #xinnovate pays the cost of develo...   \n",
              "4     RT @_twinklyToes What was that dance Nicki jus...   \n",
              "...                                                 ...   \n",
              "1627  @Broslife I RODE A TRICYCLE LAST NIGHT ! I'm g...   \n",
              "1628  In November , bring a friend/refer a friend fo...   \n",
              "1629  Inspired after a yummy lunch with @erinepage ....   \n",
              "1630  in town , dropped off photos , now having lunch !   \n",
              "1631  i think its wrong people judge others by there...   \n",
              "\n",
              "                                                 labels  \n",
              "0     ~ @ ~ , O V O V L V O , O V O V L V D N , V , ...  \n",
              "1               ! ~ ^ N N , ^ ^ & ^ ^ V A N P ^ N N , U  \n",
              "2                                           O V O V P ^  \n",
              "3                       ~ @ # V D N P N N R P V $ N P ^  \n",
              "4                             ~ @ O V D N ^ R V O V V T  \n",
              "...                                                 ...  \n",
              "1627                  @ O V D N A N , L V P V O P D N ,  \n",
              "1628                    P ^ , V D N D N P ^ & O V $ P ,  \n",
              "1629                                V P D A N P @ , ! ,  \n",
              "1630                              P N , V T N , R V N ,  \n",
              "1631                  O V L A N V N P D N , O V P D N ,  \n",
              "\n",
              "[1632 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-254f6c85-53cf-4531-97dc-37578e8599d4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RT @iheartquotes : \" Who are u chasin that's d...</td>\n",
              "      <td>~ @ ~ , O V O V L V O , O V O V L V D N , V , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ahh ... Bayou State Politics . Charlie Melanco...</td>\n",
              "      <td>! ~ ^ N N , ^ ^ &amp; ^ ^ V A N P ^ N N , U</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Yll wis u live in MIAMI</td>\n",
              "      <td>O V O V P ^</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT @PayPalx #xinnovate pays the cost of develo...</td>\n",
              "      <td>~ @ # V D N P N N R P V $ N P ^</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @_twinklyToes What was that dance Nicki jus...</td>\n",
              "      <td>~ @ O V D N ^ R V O V V T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1627</th>\n",
              "      <td>@Broslife I RODE A TRICYCLE LAST NIGHT ! I'm g...</td>\n",
              "      <td>@ O V D N A N , L V P V O P D N ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1628</th>\n",
              "      <td>In November , bring a friend/refer a friend fo...</td>\n",
              "      <td>P ^ , V D N D N P ^ &amp; O V $ P ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1629</th>\n",
              "      <td>Inspired after a yummy lunch with @erinepage ....</td>\n",
              "      <td>V P D A N P @ , ! ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1630</th>\n",
              "      <td>in town , dropped off photos , now having lunch !</td>\n",
              "      <td>P N , V T N , R V N ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1631</th>\n",
              "      <td>i think its wrong people judge others by there...</td>\n",
              "      <td>O V L A N V N P D N , O V P D N ,</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1632 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-254f6c85-53cf-4531-97dc-37578e8599d4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-254f6c85-53cf-4531-97dc-37578e8599d4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-254f6c85-53cf-4531-97dc-37578e8599d4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(iteration_time):\n",
        "  from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "  pretrained_model_name = f'./unsupervised_selftraining_iteration_{iteration_time}/model'\n",
        "  tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name, add_prefix_space=True) \n",
        "  model = AutoModelForTokenClassification.from_pretrained(pretrained_model_name)\n",
        "  device = 'cuda' if cuda.is_available() else 'cpu' #save the processing time\n",
        "  model.to(device)\n",
        "  return model, tokenizer\n",
        "\"\"\"model, tokenizer = get_model(1)\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "params = {'batch_size': 1,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "prediction_set = dataset(new_prediction, tokenizer, MAX_LEN)\n",
        "prediction_loader = DataLoader(prediction_set, **params)\n",
        "predictions = prediction(model, prediction_loader)\n",
        "predictions = pd.DataFrame(predictions)\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "FPcUeErT_Mlv",
        "outputId": "7d10478d-900f-48bd-f959-2a1b25e404da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"model, tokenizer = get_model(1)\\noptimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\\n\\nparams = {'batch_size': 1,\\n                'shuffle': False,\\n                'num_workers': 0\\n                }\\nprediction_set = dataset(new_prediction, tokenizer, MAX_LEN)\\nprediction_loader = DataLoader(prediction_set, **params)\\npredictions = prediction(model, prediction_loader)\\npredictions = pd.DataFrame(predictions)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory(predictions, iteration_time = 1)"
      ],
      "metadata": {
        "id": "yuRRPfoI_Mic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train, new_prediction = new_split(new_prediction, 2, top_percent = 0.10)"
      ],
      "metadata": {
        "id": "N4tu1KrR_RrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "xTp7imDG_Rn9",
        "outputId": "e0d69b14-fa98-4107-f8f1-9a8ec053a13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               sentence  \\\n",
              "918                               @ayoki_lee -_- a hole   \n",
              "905                                     Yayyyy Boston !   \n",
              "13                   Unffffffff John Stamos unfffffffff   \n",
              "1044                            @meaghan_face herp derp   \n",
              "79                         BRICK STRONG . RIP EVAN . <3   \n",
              "...                                                 ...   \n",
              "838                               Gothtober #BLANKtober   \n",
              "228   ' Rent Too Damn High ' NY candidate Jimmy McMi...   \n",
              "157     man for petesakes scratchh that sweepstakesss !   \n",
              "598   RT @Shae_FreeDrizzy : No you're not twinn RT @...   \n",
              "142   * Silence* ... \" I love you more than all the ...   \n",
              "\n",
              "                                                 labels  \n",
              "918                                             @ E ! !  \n",
              "905                                               ! ! ,  \n",
              "13                                              ! ^ ^ !  \n",
              "1044                                              @ ! !  \n",
              "79                                        ^ ^ , ^ ^ , E  \n",
              "...                                                 ...  \n",
              "838                                                 ! #  \n",
              "228                 , N R R A , ^ N ^ ^ V N N , D U # #  \n",
              "157                                       ! P N V D N ,  \n",
              "598                           ~ @ ~ ! L R N ~ @ ~ L A ,  \n",
              "142   , N , , O V O R P X D N P D N , P L N P N P D ...  \n",
              "\n",
              "[165 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-538d0663-f0ae-4739-a053-8b54f84511bd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>918</th>\n",
              "      <td>@ayoki_lee -_- a hole</td>\n",
              "      <td>@ E ! !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>905</th>\n",
              "      <td>Yayyyy Boston !</td>\n",
              "      <td>! ! ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Unffffffff John Stamos unfffffffff</td>\n",
              "      <td>! ^ ^ !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1044</th>\n",
              "      <td>@meaghan_face herp derp</td>\n",
              "      <td>@ ! !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>BRICK STRONG . RIP EVAN . &lt;3</td>\n",
              "      <td>^ ^ , ^ ^ , E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>838</th>\n",
              "      <td>Gothtober #BLANKtober</td>\n",
              "      <td>! #</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>' Rent Too Damn High ' NY candidate Jimmy McMi...</td>\n",
              "      <td>, N R R A , ^ N ^ ^ V N N , D U # #</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>man for petesakes scratchh that sweepstakesss !</td>\n",
              "      <td>! P N V D N ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598</th>\n",
              "      <td>RT @Shae_FreeDrizzy : No you're not twinn RT @...</td>\n",
              "      <td>~ @ ~ ! L R N ~ @ ~ L A ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>* Silence* ... \" I love you more than all the ...</td>\n",
              "      <td>, N , , O V O R P X D N P D N , P L N P N P D ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>165 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-538d0663-f0ae-4739-a053-8b54f84511bd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-538d0663-f0ae-4739-a053-8b54f84511bd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-538d0663-f0ae-4739-a053-8b54f84511bd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "UAPMEjkF_Rjk",
        "outputId": "55b06133-f4e5-48b4-e514-944c6cdfcf62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               sentence  \\\n",
              "156   Everyone keeps telling me i need Jesus , i don...   \n",
              "825   Great Review on #JBSpinshttp :// jbspins.blogs...   \n",
              "212   @SkyHearDOTcom Your local and that's cool , hi...   \n",
              "160   I had forgotten how much i looove coffee cake ...   \n",
              "161   now what i do hate is for a man to think its o...   \n",
              "...                                                 ...   \n",
              "1628  In November , bring a friend/refer a friend fo...   \n",
              "1623               What is Paradise ... in the world ??   \n",
              "1631  i think its wrong people judge others by there...   \n",
              "1630  in town , dropped off photos , now having lunch !   \n",
              "1626                   Its time to make big changes ...   \n",
              "\n",
              "                                       labels  \n",
              "156   N V V O O V ^ , O V V D ^ , O D N V O E  \n",
              "825                               A N P # U !  \n",
              "212   @ D A & L A , V O T A V V P V R O V V ,  \n",
              "160   O V V R R O V N N , ! , O V N N , ! ! ,  \n",
              "161   R O O V V V P D N P V L A P V O P N @ #  \n",
              "...                                       ...  \n",
              "1628          P ^ , V D N D N P ^ & O V $ P ,  \n",
              "1623                          O V N , P D N ,  \n",
              "1631        O V L A N V N P D N , O V P D N ,  \n",
              "1630                    P N , V T N , R V N ,  \n",
              "1626                            L N P V A N ,  \n",
              "\n",
              "[1467 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f972ba3-a539-4179-9c0e-ddeda33ac4fe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>Everyone keeps telling me i need Jesus , i don...</td>\n",
              "      <td>N V V O O V ^ , O V V D ^ , O D N V O E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>825</th>\n",
              "      <td>Great Review on #JBSpinshttp :// jbspins.blogs...</td>\n",
              "      <td>A N P # U !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>@SkyHearDOTcom Your local and that's cool , hi...</td>\n",
              "      <td>@ D A &amp; L A , V O T A V V P V R O V V ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>I had forgotten how much i looove coffee cake ...</td>\n",
              "      <td>O V V R R O V N N , ! , O V N N , ! ! ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>now what i do hate is for a man to think its o...</td>\n",
              "      <td>R O O V V V P D N P V L A P V O P N @ #</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1628</th>\n",
              "      <td>In November , bring a friend/refer a friend fo...</td>\n",
              "      <td>P ^ , V D N D N P ^ &amp; O V $ P ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1623</th>\n",
              "      <td>What is Paradise ... in the world ??</td>\n",
              "      <td>O V N , P D N ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1631</th>\n",
              "      <td>i think its wrong people judge others by there...</td>\n",
              "      <td>O V L A N V N P D N , O V P D N ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1630</th>\n",
              "      <td>in town , dropped off photos , now having lunch !</td>\n",
              "      <td>P N , V T N , R V N ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1626</th>\n",
              "      <td>Its time to make big changes ...</td>\n",
              "      <td>L N P V A N ,</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1467 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f972ba3-a539-4179-9c0e-ddeda33ac4fe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9f972ba3-a539-4179-9c0e-ddeda33ac4fe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9f972ba3-a539-4179-9c0e-ddeda33ac4fe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#b) Iteration - Aggregated\n"
      ],
      "metadata": {
        "id": "9ujxIkyC_ena"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load all Necessary Data"
      ],
      "metadata": {
        "id": "gYDiSCHE5SFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gimpel_pos = drive.CreateFile({'id':\"14UNwfhWzYYILrc3IG6frc-XyxU1x6MnL\"})\n",
        "gimpel_pos.GetContentFile(\"oct27.supertsv\")\n",
        "\n",
        "unlabelled_tweets = drive.CreateFile({'id':\"1yssTP5EJQQLcE3oJi5LKqTYkznqm-pYK\"})\n",
        "unlabelled_tweets.GetContentFile(\"st_tweets.txt\")\n",
        "\n",
        "\n",
        "def get_raw_df_gimpel(txt_list):\n",
        "  i = 0\n",
        "  all_list = [sentence.strip().split() for sentence in txt_list]\n",
        "  for l in all_list:\n",
        "    if l == []:\n",
        "      i += 1\n",
        "    else:\n",
        "      l.append(f'sentence {i}')\n",
        "  all_list = [x for x in all_list if x and x[0] != '-TWEETSTART-']\n",
        "  df = pd.DataFrame(all_list, columns=[\"id\", \"pos_tags\", \"sentence number\",\"delete\"])\n",
        "  df = df.drop(['delete'],axis=1)\n",
        "  return df\n",
        "\n",
        "def get_raw_df_new(txt_list):\n",
        "  i = 1\n",
        "  all_list = [token.split() for token in txt_list]\n",
        "  for l in all_list:\n",
        "    if l == []:\n",
        "      i += 1\n",
        "    else:\n",
        "      l.append(f'sentence {i}')\n",
        "  all_list = [x for x in all_list if x]\n",
        "  df = pd.DataFrame(all_list, columns=[\"id\", \"tag\", \"sentence number\"])\n",
        "  return df\n",
        "\n",
        "labels_to_ids = {',':0, '&':1, '$':2, 'D':3,'X':4 ,'G':5, 'P':6, 'A':7, 'V':8, 'N':9, '^':10, 'O':11, 'R':12, '!':13, 'S':14, 'Z':15, 'L':16, 'M':17, '#':18, '@':19, '~':20, 'U':21, 'E':22, 'T':23,'Y':24}\n",
        "ids_to_labels = dict((v,k) for k,v in labels_to_ids.items())\n",
        "\n",
        "def get_preprocessed_df_gimpel(txt_list, tag_name):\n",
        "  raw_df = get_raw_df_gimpel(txt_list)\n",
        "  \"\"\"tag_name is a string representing tag name from \"pos_tags\",\"chunk_tags\",\"ner_tags\" \"\"\"\n",
        "  raw_df[\"sentence\"] = raw_df[[\"sentence number\", \"id\", \"pos_tags\"]].groupby([\"sentence number\"])[\"id\"].transform(lambda x: \" \".join(x))\n",
        "  raw_df[\"labels\"] = raw_df[[\"sentence number\", \"id\", \"pos_tags\"]].groupby([\"sentence number\"])[tag_name].transform(lambda x: \" \".join(x))\n",
        "  return raw_df\n",
        "\n",
        "data = open(\"oct27.supertsv\", \"r\",encoding='utf-8')\n",
        "text = data.read()\n",
        "texts = text.split('\\n')\n",
        "text_edit = list()\n",
        "for item in texts:\n",
        "  if item.startswith('TWEET\\t'):\n",
        "    text_edit.append('-TWEETSTART- -X-\\n')\n",
        "  elif item == 'TOKENS':\n",
        "    text_edit.append('\\n')\n",
        "  elif item != '':\n",
        "    token = item[2:]\n",
        "    tag = item[0]\n",
        "    text_edit.append(f'{token} {tag}\\n')\n",
        "\n",
        "with open(\"st_tweets.txt\") as f:\n",
        "  new_tweets = f.readlines()\n",
        "nt = get_raw_df_new(new_tweets)\n",
        "nt[\"sentence\"] = nt[[\"sentence number\", \"id\", \"tag\"]].groupby([\"sentence number\"])[\"id\"].transform(lambda x: \" \".join(x))\n",
        "nt[\"labels\"] = nt[[\"sentence number\", \"id\", \"tag\"]].groupby([\"sentence number\"])[\"tag\"].transform(lambda x: \" \".join(x))\n",
        "\n",
        "import sklearn.model_selection as sk_ms\n",
        "full_tweets = get_preprocessed_df_gimpel(text_edit,'pos_tags')\n",
        "train_tweets, test_tweets = sk_ms.train_test_split(full_tweets, train_size=0.7,random_state=123,shuffle=True)\n",
        "train_tweets = train_tweets[train_tweets['pos_tags'].isin(labels_to_ids.keys())]\n",
        "test_tweets = test_tweets[test_tweets['pos_tags'].isin(labels_to_ids.keys())]\n",
        "test_tweets = test_tweets.reset_index()\n",
        "train_tweets = train_tweets.reset_index()\n",
        "\n",
        "def get_sentence_labels(raw_df):\n",
        "  return raw_df[[\"sentence\", \"labels\"]].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "df_train = get_sentence_labels(nt)\n",
        "def label_sim(x):\n",
        "  temp = x.split(\" \")\n",
        "  new = list()\n",
        "  for i in temp:\n",
        "    new.append(\"O\")\n",
        "  return \" \".join(new)\n",
        "\n",
        "df_train['labels'] = df_train['labels'].apply(lambda x:label_sim(x))\n",
        "df_train = df_train[0:2000]\n",
        "#df_valid = get_sentence_labels(df_valid)\n",
        "df_test = get_sentence_labels(test_tweets)"
      ],
      "metadata": {
        "id": "FFBvlr8q_RhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Functions"
      ],
      "metadata": {
        "id": "Kmib3OPV53wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(Dataset):\n",
        "  def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        # step 1: get the sentence and word labels \n",
        "        sentence = self.data.sentence[index].strip().split()  \n",
        "        word_labels = self.data.labels[index].split() \n",
        "\n",
        "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
        "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
        "        encoding = self.tokenizer(sentence,\n",
        "                             is_split_into_words=True, #no is_pretokenlized(Modification), we already have a splitted sentence\n",
        "                             return_offsets_mapping=True, \n",
        "                             padding='max_length', \n",
        "                             truncation=True, \n",
        "                             max_length=self.max_len)\n",
        "        \n",
        "        # step 3: create token labels only for first word pieces of each tokenized word\n",
        "        labels = [labels_to_ids[label] for label in word_labels] \n",
        "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
        "        # create an empty array of -100 of length max_length\n",
        "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
        "        \n",
        "        # set only labels whose first offset position is 0 and the second is not 0\n",
        "        i = 0\n",
        "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
        "          if mapping[0] == 0 and mapping[1] != 0:\n",
        "            # overwrite label\n",
        "            encoded_labels[idx] = labels[i]\n",
        "            i += 1       \n",
        "\n",
        "        # step 4: turn everything into PyTorch tensors\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.as_tensor(encoded_labels)\n",
        "        \n",
        "        return item\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "Qne6oEAI5sH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "    \n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        \n",
        "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "        labels = batch['labels'].to(device, dtype = torch.long)\n",
        "\n",
        "        #loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "        output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "        tr_loss += output[0]\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += labels.size(0)\n",
        "        \n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "           \n",
        "        # compute training accuracy\n",
        "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = output[1].view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        \n",
        "        # only compute accuracy at active labels\n",
        "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
        "        \n",
        "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "        \n",
        "        tr_labels.extend(labels)\n",
        "        tr_preds.extend(predictions)\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "    \n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "        \n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        output[0].backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
      ],
      "metadata": {
        "id": "eSC8ttCKERev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FsWZS8TT5lYb"
      },
      "outputs": [],
      "source": [
        "def valid(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            \n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['labels'].to(device, dtype = torch.long)\n",
        "            \n",
        "            #loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "            output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "\n",
        "            eval_loss += output[0].item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += labels.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = output[1].view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            \n",
        "            # only compute accuracy at active labels\n",
        "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        \n",
        "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            \n",
        "            eval_labels.extend(labels)\n",
        "            eval_preds.extend(predictions)\n",
        "            \n",
        "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
        "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(model, data_loader):\n",
        "    all_sentences = []\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(data_loader):\n",
        "            sentence_prediction = {}\n",
        "            \n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['labels'].to(device, dtype = torch.long)\n",
        "\n",
        "            #loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "            output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        " \n",
        "            if idx % 100==0:\n",
        "                print(f\"Finished predicting {idx} sentences\")\n",
        "            \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = output[1].view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            \n",
        "            # compute active probability using softmax function\n",
        "            active_probability = torch.nn.functional.softmax(active_logits, dim=1)\n",
        "            flattened_probability = torch.argmax(active_probability, axis=1)\n",
        "            \n",
        "            # only compute accuracy at active labels\n",
        "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        \n",
        "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            probabilities = torch.masked_select(flattened_probability, active_accuracy)\n",
        "            masks = torch.masked_select(mask, active_accuracy).tolist()\n",
        "            input_ids = torch.masked_select(ids, active_accuracy).tolist()\n",
        "\n",
        "            prob_score = []\n",
        "            for ind, prob in enumerate(probabilities):\n",
        "              prob_score.append(float(active_probability[ind][prob]))\n",
        "            \n",
        "            sentence_prediction[\"Attention masks\"] = masks\n",
        "            sentence_prediction[\"Input ids\"] = input_ids\n",
        "            sentence_prediction[\"Token\"] = [tokenizer.convert_ids_to_tokens(input_id) for input_id in input_ids]\n",
        "            sentence_prediction[\"Actual labels\"] = [ids_to_labels[int(x)] for x in labels]\n",
        "            sentence_prediction[\"Probability\"] = [x for x in prob_score]\n",
        "            sentence_prediction[\"Predicted labels\"] = [ids_to_labels[int(x)] for x in predictions]\n",
        "            sentence_prediction[\"Sentence number\"] = [idx] * len(input_ids)\n",
        "            all_sentences.append(sentence_prediction)\n",
        "\n",
        "    return all_sentences"
      ],
      "metadata": {
        "id": "PatgRIeZAq3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(iteration_time, model_name):\n",
        "  df_predictions = pd.read_csv(f\"./unsupervised_selftraining_{model_name}_iteration_{iteration_time-1}/processed_data_{iteration_time - 1}.csv\")\n",
        "  df_predictions = df_predictions.drop(columns=['Unnamed: 0'])\n",
        "  import ast\n",
        "  for column in list(df_predictions.columns):\n",
        "    df_predictions[column] = df_predictions[column].apply(ast.literal_eval)\n",
        "  #get the average predicted probability of each sentence to find the most confident sentences\n",
        "  df_predictions[\"Average probability\"] = df_predictions[\"Probability\"].apply(np.mean)\n",
        "  return df_predictions"
      ],
      "metadata": {
        "id": "6Vbwe6AnAq05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def new_split(df_train, iteration_time, model_name, top_percent = 0.10):\n",
        "  import math\n",
        "  df_predictions = get_data(iteration_time, model_name)\n",
        "  #get sentences with predicted labels and replace the original true labels with the predicted most confident labels\n",
        "  df_train[\"labels\"] = df_predictions[\"Predicted labels\"].str.join(\" \")\n",
        "  df_predictions = df_predictions.sort_values(by=['Average probability'], ascending=False)\n",
        "  df_new_train = df_predictions.iloc[:(math.ceil(len(df_predictions) * top_percent)) + 1, : ]\n",
        "  df_prediction = df_predictions.iloc[(math.ceil(len(df_predictions) * top_percent)) + 1:, : ]\n",
        "  df_train_id = list(df_new_train.index)\n",
        "  df_prediction_id = list(df_prediction.index)\n",
        "  new_train = df_train.take(df_train_id)\n",
        "  new_prediction = df_train.take(df_prediction_id)\n",
        "  new_train = new_train.reset_index(drop=True)\n",
        "  new_prediction = new_prediction.reset_index(drop=True)\n",
        "  return new_train, new_prediction"
      ],
      "metadata": {
        "id": "5ay98nZ4Aqxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def create_dir(i, tokenizer, model, model_name):\n",
        "  directory_0 = f'./unsupervised_selftraining_{model_name}_iteration_{i}'\n",
        "  directory_1 = f'./unsupervised_selftraining_{model_name}_iteration_{i}/model'\n",
        "\n",
        "  if not os.path.exists(directory_0):\n",
        "    os.makedirs(directory_0)\n",
        "\n",
        "  if not os.path.exists(directory_1):\n",
        "    os.makedirs(directory_1)\n",
        "\n",
        "  # save vocabulary of the tokenizer\n",
        "  tokenizer.save_vocabulary(directory_1)\n",
        "  # save the model weights and its configuration file\n",
        "  model.save_pretrained(directory_1)"
      ],
      "metadata": {
        "id": "RzzeofkTAqtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_model(new_train, tokenizer, iteration_time, model_name):\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)\n",
        "  \n",
        "  create_dir(iteration_time, tokenizer, model, model_name)\n",
        "  \n",
        "  print(f'iteration {iteration_time} updated model saved')"
      ],
      "metadata": {
        "id": "uOOeMi-KA9xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def self_training_model(df_train, pretrained_model_path, model_name, top_percent = 0.20, total_iterations = 25):\n",
        "  \n",
        "  for i in range(total_iterations):\n",
        "    print(\"==========================================================================\")\n",
        "    print(f\"ITERATION {i} STARTS!\")\n",
        "    \n",
        "    #zero-shot prediction\n",
        "    if i == 0:\n",
        "      #if model_name == \"BERT\":\n",
        "        #tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path)\n",
        "      #else:\n",
        "      \n",
        "      tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_path)\n",
        "      print(f\"tokenizer before iteration {i} loaded\") \n",
        "      model = AutoModelForTokenClassification.from_pretrained(pretrained_model_path)\n",
        "      print(f\"model before iteration {i} loaded\")\n",
        "      model.to(device)\n",
        "      \n",
        "      #save the model and tokenizer in directory for further use\n",
        "      create_dir(i, tokenizer, model, model_name)\n",
        "      print(f'tokenizer and model saved after {i} iteration')\n",
        "      \n",
        "      training_set = dataset(df_train, tokenizer, MAX_LEN)\n",
        "      training_loader = DataLoader(training_set, **params)\n",
        "      \n",
        "      predictions = prediction(model, training_loader)\n",
        "      predictions = pd.DataFrame(predictions)\n",
        "      #save_directory(predictions, iteration_time = i)\n",
        "      predictions.to_csv(f\"./unsupervised_selftraining_{model_name}_iteration_{i}/processed_data_{i}.csv\") \n",
        "      print(f'Iteration {i} Prediction data saved')\n",
        "      print(f'ITERATION {i} ENDS!')\n",
        "    \n",
        "    #iteration starts\n",
        "    else:\n",
        "      from transformers import BertTokenizer\n",
        "      model_previous = f'./unsupervised_selftraining_{model_name}_iteration_{i - 1}/model'\n",
        "      tokenizer = BertTokenizerFast.from_pretrained(model_previous)\n",
        "      model = AutoModelForTokenClassification.from_pretrained(model_previous)\n",
        "      model.to(device)\n",
        "      optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "      #get the top percent most confident predictions from sentences and replace the labels with predictions\n",
        "      new_train, new_prediction = new_split(df_train, i, model_name, top_percent)\n",
        "      training_set = dataset(new_train, tokenizer, MAX_LEN)\n",
        "      training_loader = DataLoader(training_set, **params)\n",
        "      #return new_train, new_prediction\n",
        "      #update the training model\n",
        "      update_model(new_train, tokenizer, i, model_name)\n",
        "\n",
        "      df_train = new_prediction\n",
        "      training_set = dataset(df_train, tokenizer, MAX_LEN)\n",
        "      training_loader = DataLoader(training_set, **params)\n",
        "      \n",
        "      predictions = prediction(model, training_loader)\n",
        "      predictions = pd.DataFrame(predictions)\n",
        "      \n",
        "      #save_directory(predictions, iteration_time = i)\n",
        "      predictions.to_csv(f\"./unsupervised_selftraining_{model_name}_iteration_{i}/processed_data_{i}.csv\") \n",
        "      print(f'Iteration {i} Prediction data saved')\n",
        "      print(f'ITERATION {i} ENDS!')\n",
        "\n"
      ],
      "metadata": {
        "id": "aVj6F15oA_XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Saved Model & Predict Labels"
      ],
      "metadata": {
        "id": "uyicOt-hAtME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "# choose BERT model from the previous model results\n",
        "pretrained_model_path = \"/content/drive/MyDrive/pos_Bert_model_supervised\"\n",
        "params = {'batch_size': 1, 'shuffle': False, 'num_workers': 0}\n",
        "MAX_LEN = 128\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "NfcrmuosslQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self_training_model(df_train, pretrained_model_path = f\"/content/drive/MyDrive/pos_Bert_model_supervised\", top_percent = 0.10, total_iterations = 20, model_name = 'bert')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpr0FdNfBJ_U",
        "outputId": "51a1cde6-b233-4f2e-99bd-65b6963e0a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================\n",
            "ITERATION 0 STARTS!\n",
            "tokenizer before iteration 0 loaded\n",
            "model before iteration 0 loaded\n",
            "tokenizer and model saved after 0 iteration\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Finished predicting 500 sentences\n",
            "Finished predicting 600 sentences\n",
            "Finished predicting 700 sentences\n",
            "Finished predicting 800 sentences\n",
            "Finished predicting 900 sentences\n",
            "Finished predicting 1000 sentences\n",
            "Finished predicting 1100 sentences\n",
            "Finished predicting 1200 sentences\n",
            "Finished predicting 1300 sentences\n",
            "Finished predicting 1400 sentences\n",
            "Finished predicting 1500 sentences\n",
            "Finished predicting 1600 sentences\n",
            "Finished predicting 1700 sentences\n",
            "Finished predicting 1800 sentences\n",
            "Finished predicting 1900 sentences\n",
            "Iteration 0 Prediction data saved\n",
            "ITERATION 0 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 1 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.1650741994380951\n",
            "Training loss per 100 training steps: 0.19990307092666626\n",
            "Training loss per 100 training steps: 0.18851442635059357\n",
            "Training loss per 100 training steps: 0.1741580218076706\n",
            "Training loss per 100 training steps: 0.1871103048324585\n",
            "Training loss per 100 training steps: 0.19282856583595276\n",
            "Training loss per 100 training steps: 0.18756918609142303\n",
            "Training loss per 100 training steps: 0.18300841748714447\n",
            "Training loss per 100 training steps: 0.18374750018119812\n",
            "Training loss per 100 training steps: 0.17882631719112396\n",
            "Training loss per 100 training steps: 0.17693674564361572\n",
            "Training loss per 100 training steps: 0.17375019192695618\n",
            "Training loss per 100 training steps: 0.1715921014547348\n",
            "Training loss per 100 training steps: 0.17037269473075867\n",
            "Training loss per 100 training steps: 0.16883240640163422\n",
            "Training loss per 100 training steps: 0.17110656201839447\n",
            "Training loss per 100 training steps: 0.16941162943840027\n",
            "Training loss per 100 training steps: 0.16753387451171875\n",
            "Training loss per 100 training steps: 0.16569149494171143\n",
            "Training loss per 100 training steps: 0.1646261066198349\n",
            "Training loss epoch: 0.16444706916809082\n",
            "Training accuracy epoch: 0.9491154899017024\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.01717849262058735\n",
            "Training loss per 100 training steps: 0.11731691658496857\n",
            "Training loss per 100 training steps: 0.11257817596197128\n",
            "Training loss per 100 training steps: 0.09904976189136505\n",
            "Training loss per 100 training steps: 0.09541429579257965\n",
            "Training loss per 100 training steps: 0.09905724972486496\n",
            "Training loss per 100 training steps: 0.09791093319654465\n",
            "Training loss per 100 training steps: 0.10004644840955734\n",
            "Training loss per 100 training steps: 0.10332431644201279\n",
            "Training loss per 100 training steps: 0.10096461325883865\n",
            "Training loss per 100 training steps: 0.09815079718828201\n",
            "Training loss per 100 training steps: 0.0967278927564621\n",
            "Training loss per 100 training steps: 0.09476692974567413\n",
            "Training loss per 100 training steps: 0.09198161959648132\n",
            "Training loss per 100 training steps: 0.09214556962251663\n",
            "Training loss per 100 training steps: 0.0915307104587555\n",
            "Training loss per 100 training steps: 0.09059350937604904\n",
            "Training loss per 100 training steps: 0.0910639539361\n",
            "Training loss per 100 training steps: 0.09170591086149216\n",
            "Training loss per 100 training steps: 0.09061528742313385\n",
            "Training loss epoch: 0.08972568064928055\n",
            "Training accuracy epoch: 0.9735270777705671\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.11128982901573181\n",
            "Training loss per 100 training steps: 0.0595414936542511\n",
            "Training loss per 100 training steps: 0.06317878514528275\n",
            "Training loss per 100 training steps: 0.06050705909729004\n",
            "Training loss per 100 training steps: 0.059790704399347305\n",
            "Training loss per 100 training steps: 0.0651831403374672\n",
            "Training loss per 100 training steps: 0.06321275234222412\n",
            "Training loss per 100 training steps: 0.06525713950395584\n",
            "Training loss per 100 training steps: 0.06440477073192596\n",
            "Training loss per 100 training steps: 0.062241457402706146\n",
            "Training loss per 100 training steps: 0.0593496710062027\n",
            "Training loss per 100 training steps: 0.059299368411302567\n",
            "Training loss per 100 training steps: 0.05891679972410202\n",
            "Training loss per 100 training steps: 0.05908222869038582\n",
            "Training loss per 100 training steps: 0.05989806354045868\n",
            "Training loss per 100 training steps: 0.059700120240449905\n",
            "Training loss per 100 training steps: 0.05881231650710106\n",
            "Training loss per 100 training steps: 0.05869883671402931\n",
            "Training loss per 100 training steps: 0.0589703805744648\n",
            "Training loss per 100 training steps: 0.05894315242767334\n",
            "Training loss epoch: 0.05913950502872467\n",
            "Training accuracy epoch: 0.9832061816675643\n",
            "iteration 1 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Finished predicting 500 sentences\n",
            "Finished predicting 600 sentences\n",
            "Finished predicting 700 sentences\n",
            "Finished predicting 800 sentences\n",
            "Finished predicting 900 sentences\n",
            "Finished predicting 1000 sentences\n",
            "Finished predicting 1100 sentences\n",
            "Finished predicting 1200 sentences\n",
            "Finished predicting 1300 sentences\n",
            "Finished predicting 1400 sentences\n",
            "Finished predicting 1500 sentences\n",
            "Finished predicting 1600 sentences\n",
            "Finished predicting 1700 sentences\n",
            "Iteration 1 Prediction data saved\n",
            "ITERATION 1 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 2 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.003941319417208433\n",
            "Training loss per 100 training steps: 0.032371360808610916\n",
            "Training loss per 100 training steps: 0.04425899311900139\n",
            "Training loss per 100 training steps: 0.04159938916563988\n",
            "Training loss per 100 training steps: 0.04171526059508324\n",
            "Training loss per 100 training steps: 0.04280174523591995\n",
            "Training loss per 100 training steps: 0.0420578308403492\n",
            "Training loss per 100 training steps: 0.04431209713220596\n",
            "Training loss per 100 training steps: 0.045118071138858795\n",
            "Training loss per 100 training steps: 0.04697504639625549\n",
            "Training loss per 100 training steps: 0.04782050475478172\n",
            "Training loss per 100 training steps: 0.04925789311528206\n",
            "Training loss per 100 training steps: 0.04920860752463341\n",
            "Training loss per 100 training steps: 0.0477580800652504\n",
            "Training loss per 100 training steps: 0.04758138954639435\n",
            "Training loss per 100 training steps: 0.046938296407461166\n",
            "Training loss per 100 training steps: 0.04577333852648735\n",
            "Training loss per 100 training steps: 0.046343106776475906\n",
            "Training loss per 100 training steps: 0.04568420723080635\n",
            "Training loss per 100 training steps: 0.045478612184524536\n",
            "Training loss epoch: 0.04571017622947693\n",
            "Training accuracy epoch: 0.9873277878448687\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.0024081342853605747\n",
            "Training loss per 100 training steps: 0.020797157660126686\n",
            "Training loss per 100 training steps: 0.030258702114224434\n",
            "Training loss per 100 training steps: 0.03474484011530876\n",
            "Training loss per 100 training steps: 0.03310559317469597\n",
            "Training loss per 100 training steps: 0.036220476031303406\n",
            "Training loss per 100 training steps: 0.03553428873419762\n",
            "Training loss per 100 training steps: 0.035664547234773636\n",
            "Training loss per 100 training steps: 0.03527427464723587\n",
            "Training loss per 100 training steps: 0.035850849002599716\n",
            "Training loss per 100 training steps: 0.03461523354053497\n",
            "Training loss per 100 training steps: 0.03441291302442551\n",
            "Training loss per 100 training steps: 0.03418278694152832\n",
            "Training loss per 100 training steps: 0.03382452577352524\n",
            "Training loss per 100 training steps: 0.0331413559615612\n",
            "Training loss per 100 training steps: 0.03233926743268967\n",
            "Training loss per 100 training steps: 0.03233436122536659\n",
            "Training loss per 100 training steps: 0.03235839307308197\n",
            "Training loss per 100 training steps: 0.03213091194629669\n",
            "Training loss per 100 training steps: 0.031855545938014984\n",
            "Training loss epoch: 0.03161902353167534\n",
            "Training accuracy epoch: 0.9910852109586418\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.002425686689093709\n",
            "Training loss per 100 training steps: 0.04659682512283325\n",
            "Training loss per 100 training steps: 0.043224453926086426\n",
            "Training loss per 100 training steps: 0.035947926342487335\n",
            "Training loss per 100 training steps: 0.03295712172985077\n",
            "Training loss per 100 training steps: 0.031557295471429825\n",
            "Training loss per 100 training steps: 0.03242533653974533\n",
            "Training loss per 100 training steps: 0.03154169023036957\n",
            "Training loss per 100 training steps: 0.03390488773584366\n",
            "Training loss per 100 training steps: 0.03385687991976738\n",
            "Training loss per 100 training steps: 0.03335386887192726\n",
            "Training loss per 100 training steps: 0.03359034284949303\n",
            "Training loss per 100 training steps: 0.032550692558288574\n",
            "Training loss per 100 training steps: 0.03209438920021057\n",
            "Training loss per 100 training steps: 0.03210289031267166\n",
            "Training loss per 100 training steps: 0.03252777084708214\n",
            "Training loss per 100 training steps: 0.031846046447753906\n",
            "Training loss per 100 training steps: 0.03245393931865692\n",
            "Training loss per 100 training steps: 0.03152329847216606\n",
            "Training loss per 100 training steps: 0.031247636303305626\n",
            "Training loss epoch: 0.030700119212269783\n",
            "Training accuracy epoch: 0.9915116148886468\n",
            "iteration 2 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Finished predicting 500 sentences\n",
            "Finished predicting 600 sentences\n",
            "Finished predicting 700 sentences\n",
            "Finished predicting 800 sentences\n",
            "Finished predicting 900 sentences\n",
            "Finished predicting 1000 sentences\n",
            "Finished predicting 1100 sentences\n",
            "Finished predicting 1200 sentences\n",
            "Finished predicting 1300 sentences\n",
            "Finished predicting 1400 sentences\n",
            "Finished predicting 1500 sentences\n",
            "Finished predicting 1600 sentences\n",
            "Iteration 2 Prediction data saved\n",
            "ITERATION 2 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 3 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.0014399803476408124\n",
            "Training loss per 100 training steps: 0.027208292856812477\n",
            "Training loss per 100 training steps: 0.030090125277638435\n",
            "Training loss per 100 training steps: 0.026614369824528694\n",
            "Training loss per 100 training steps: 0.024685746058821678\n",
            "Training loss per 100 training steps: 0.02473059482872486\n",
            "Training loss per 100 training steps: 0.025319885462522507\n",
            "Training loss per 100 training steps: 0.023948034271597862\n",
            "Training loss per 100 training steps: 0.023373009636998177\n",
            "Training loss per 100 training steps: 0.02414572611451149\n",
            "Training loss per 100 training steps: 0.02370779775083065\n",
            "Training loss per 100 training steps: 0.02386566810309887\n",
            "Training loss per 100 training steps: 0.02379784919321537\n",
            "Training loss per 100 training steps: 0.025172078981995583\n",
            "Training loss per 100 training steps: 0.024992506951093674\n",
            "Training loss per 100 training steps: 0.025410614907741547\n",
            "Training loss per 100 training steps: 0.024678999558091164\n",
            "Training loss per 100 training steps: 0.024755137041211128\n",
            "Training loss per 100 training steps: 0.02419419027864933\n",
            "Training loss per 100 training steps: 0.023811599239706993\n",
            "Training loss epoch: 0.023079970851540565\n",
            "Training accuracy epoch: 0.9941278992958742\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.0020840370561927557\n",
            "Training loss per 100 training steps: 0.010261413641273975\n",
            "Training loss per 100 training steps: 0.016537440940737724\n",
            "Training loss per 100 training steps: 0.02199900895357132\n",
            "Training loss per 100 training steps: 0.019155772402882576\n",
            "Training loss per 100 training steps: 0.01851755939424038\n",
            "Training loss per 100 training steps: 0.01715593785047531\n",
            "Training loss per 100 training steps: 0.019354818388819695\n",
            "Training loss per 100 training steps: 0.02100253477692604\n",
            "Training loss per 100 training steps: 0.02013043686747551\n",
            "Training loss per 100 training steps: 0.019007565453648567\n",
            "Training loss per 100 training steps: 0.018323615193367004\n",
            "Training loss per 100 training steps: 0.017906129360198975\n",
            "Training loss per 100 training steps: 0.018576154485344887\n",
            "Training loss per 100 training steps: 0.01872318796813488\n",
            "Training loss per 100 training steps: 0.017964068800210953\n",
            "Training loss per 100 training steps: 0.017343148589134216\n",
            "Training loss per 100 training steps: 0.017435507848858833\n",
            "Training loss per 100 training steps: 0.017348064109683037\n",
            "Training loss per 100 training steps: 0.01755061000585556\n",
            "Training loss epoch: 0.01748274452984333\n",
            "Training accuracy epoch: 0.9952821317003872\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.0011226568603888154\n",
            "Training loss per 100 training steps: 0.007036589551717043\n",
            "Training loss per 100 training steps: 0.00836961716413498\n",
            "Training loss per 100 training steps: 0.00844539888203144\n",
            "Training loss per 100 training steps: 0.008067984133958817\n",
            "Training loss per 100 training steps: 0.010833607986569405\n",
            "Training loss per 100 training steps: 0.010974961332976818\n",
            "Training loss per 100 training steps: 0.011081506498157978\n",
            "Training loss per 100 training steps: 0.011402182281017303\n",
            "Training loss per 100 training steps: 0.012019348330795765\n",
            "Training loss per 100 training steps: 0.011972655542194843\n",
            "Training loss per 100 training steps: 0.012060563080012798\n",
            "Training loss per 100 training steps: 0.012007751502096653\n",
            "Training loss per 100 training steps: 0.012836701236665249\n",
            "Training loss per 100 training steps: 0.01279347576200962\n",
            "Training loss per 100 training steps: 0.013369622640311718\n",
            "Training loss per 100 training steps: 0.013460609130561352\n",
            "Training loss per 100 training steps: 0.013510685414075851\n",
            "Training loss per 100 training steps: 0.013942703604698181\n",
            "Training loss per 100 training steps: 0.014120143838226795\n",
            "Training loss epoch: 0.014474829658865929\n",
            "Training accuracy epoch: 0.9963239813027643\n",
            "iteration 3 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Finished predicting 500 sentences\n",
            "Finished predicting 600 sentences\n",
            "Finished predicting 700 sentences\n",
            "Finished predicting 800 sentences\n",
            "Finished predicting 900 sentences\n",
            "Finished predicting 1000 sentences\n",
            "Finished predicting 1100 sentences\n",
            "Finished predicting 1200 sentences\n",
            "Finished predicting 1300 sentences\n",
            "Finished predicting 1400 sentences\n",
            "Iteration 3 Prediction data saved\n",
            "ITERATION 3 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 4 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.0007678291294723749\n",
            "Training loss per 100 training steps: 0.010776815935969353\n",
            "Training loss per 100 training steps: 0.012168508023023605\n",
            "Training loss per 100 training steps: 0.018844692036509514\n",
            "Training loss per 100 training steps: 0.018607692793011665\n",
            "Training loss per 100 training steps: 0.017717288807034492\n",
            "Training loss per 100 training steps: 0.017924927175045013\n",
            "Training loss per 100 training steps: 0.018369439989328384\n",
            "Training loss per 100 training steps: 0.017502591013908386\n",
            "Training loss per 100 training steps: 0.01828804425895214\n",
            "Training loss per 100 training steps: 0.018003826960921288\n",
            "Training loss per 100 training steps: 0.017096873372793198\n",
            "Training loss per 100 training steps: 0.01608552224934101\n",
            "Training loss per 100 training steps: 0.015917865559458733\n",
            "Training loss per 100 training steps: 0.015425675548613071\n",
            "Training loss per 100 training steps: 0.014874902553856373\n",
            "Training loss per 100 training steps: 0.014600829221308231\n",
            "Training loss per 100 training steps: 0.01468606386333704\n",
            "Training loss per 100 training steps: 0.014764992520213127\n",
            "Training loss per 100 training steps: 0.01453774981200695\n",
            "Training loss epoch: 0.014233853667974472\n",
            "Training accuracy epoch: 0.9963016840483734\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.0008576138061471283\n",
            "Training loss per 100 training steps: 0.020757846534252167\n",
            "Training loss per 100 training steps: 0.019080201163887978\n",
            "Training loss per 100 training steps: 0.014080975204706192\n",
            "Training loss per 100 training steps: 0.014991670846939087\n",
            "Training loss per 100 training steps: 0.01700478233397007\n",
            "Training loss per 100 training steps: 0.016449660062789917\n",
            "Training loss per 100 training steps: 0.01538901124149561\n",
            "Training loss per 100 training steps: 0.014216129668056965\n",
            "Training loss per 100 training steps: 0.013966472819447517\n",
            "Training loss per 100 training steps: 0.01431314367800951\n",
            "Training loss per 100 training steps: 0.014073288068175316\n",
            "Training loss per 100 training steps: 0.015658199787139893\n",
            "Training loss per 100 training steps: 0.015193512663245201\n",
            "Training loss per 100 training steps: 0.014827398583292961\n",
            "Training loss per 100 training steps: 0.014438639394938946\n",
            "Training loss per 100 training steps: 0.013812774792313576\n",
            "Training loss per 100 training steps: 0.013378875330090523\n",
            "Training loss per 100 training steps: 0.01336747407913208\n",
            "Training loss per 100 training steps: 0.013375132344663143\n",
            "Training loss epoch: 0.013359185308218002\n",
            "Training accuracy epoch: 0.9966511152579894\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.0008465238497592509\n",
            "Training loss per 100 training steps: 0.02103269100189209\n",
            "Training loss per 100 training steps: 0.016931038349866867\n",
            "Training loss per 100 training steps: 0.016779188066720963\n",
            "Training loss per 100 training steps: 0.01521266344934702\n",
            "Training loss per 100 training steps: 0.015073362737894058\n",
            "Training loss per 100 training steps: 0.01361523661762476\n",
            "Training loss per 100 training steps: 0.012972766533493996\n",
            "Training loss per 100 training steps: 0.01276412047445774\n",
            "Training loss per 100 training steps: 0.012376283295452595\n",
            "Training loss per 100 training steps: 0.01344101782888174\n",
            "Training loss per 100 training steps: 0.012758418917655945\n",
            "Training loss per 100 training steps: 0.013089473359286785\n",
            "Training loss per 100 training steps: 0.013057545758783817\n",
            "Training loss per 100 training steps: 0.013235756196081638\n",
            "Training loss per 100 training steps: 0.012653643265366554\n",
            "Training loss per 100 training steps: 0.01209049392491579\n",
            "Training loss per 100 training steps: 0.011952261440455914\n",
            "Training loss per 100 training steps: 0.011860469356179237\n",
            "Training loss per 100 training steps: 0.01191618014127016\n",
            "Training loss epoch: 0.01193317025899887\n",
            "Training accuracy epoch: 0.996896048968625\n",
            "iteration 4 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Finished predicting 500 sentences\n",
            "Finished predicting 600 sentences\n",
            "Finished predicting 700 sentences\n",
            "Finished predicting 800 sentences\n",
            "Finished predicting 900 sentences\n",
            "Finished predicting 1000 sentences\n",
            "Finished predicting 1100 sentences\n",
            "Finished predicting 1200 sentences\n",
            "Finished predicting 1300 sentences\n",
            "Iteration 4 Prediction data saved\n",
            "ITERATION 4 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 5 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.0013751249061897397\n",
            "Training loss per 100 training steps: 0.0070624155923724174\n",
            "Training loss per 100 training steps: 0.018068542703986168\n",
            "Training loss per 100 training steps: 0.014505886472761631\n",
            "Training loss per 100 training steps: 0.012801969423890114\n",
            "Training loss per 100 training steps: 0.01129124779254198\n",
            "Training loss per 100 training steps: 0.011284182779490948\n",
            "Training loss per 100 training steps: 0.010651135817170143\n",
            "Training loss per 100 training steps: 0.010495895519852638\n",
            "Training loss per 100 training steps: 0.00990835390985012\n",
            "Training loss per 100 training steps: 0.009864521212875843\n",
            "Training loss per 100 training steps: 0.009491992183029652\n",
            "Training loss per 100 training steps: 0.009222527034580708\n",
            "Training loss per 100 training steps: 0.008946058340370655\n",
            "Training loss per 100 training steps: 0.009035790339112282\n",
            "Training loss per 100 training steps: 0.008817060850560665\n",
            "Training loss per 100 training steps: 0.008387599140405655\n",
            "Training loss per 100 training steps: 0.008182908408343792\n",
            "Training loss per 100 training steps: 0.00789046473801136\n",
            "Training loss per 100 training steps: 0.007998029701411724\n",
            "Training loss epoch: 0.007838098332285881\n",
            "Training accuracy epoch: 0.9981988422111099\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.0012870969949290156\n",
            "Training loss per 100 training steps: 0.004431081935763359\n",
            "Training loss per 100 training steps: 0.0081874905154109\n",
            "Training loss per 100 training steps: 0.00734978262335062\n",
            "Training loss per 100 training steps: 0.008776853792369366\n",
            "Training loss per 100 training steps: 0.009126298129558563\n",
            "Training loss per 100 training steps: 0.009717688895761967\n",
            "Training loss per 100 training steps: 0.011935877613723278\n",
            "Training loss per 100 training steps: 0.011523250490427017\n",
            "Training loss per 100 training steps: 0.010871164500713348\n",
            "Training loss per 100 training steps: 0.010096064768731594\n",
            "Training loss per 100 training steps: 0.009407071396708488\n",
            "Training loss per 100 training steps: 0.008870533667504787\n",
            "Training loss per 100 training steps: 0.008257350884377956\n",
            "Training loss per 100 training steps: 0.00798028614372015\n",
            "Training loss per 100 training steps: 0.007637230679392815\n",
            "Training loss per 100 training steps: 0.007571556139737368\n",
            "Training loss per 100 training steps: 0.007347220089286566\n",
            "Training loss per 100 training steps: 0.007210463751107454\n",
            "Training loss per 100 training steps: 0.0072754984721541405\n",
            "Training loss epoch: 0.0071424334309995174\n",
            "Training accuracy epoch: 0.9983628959910532\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.00047359763993881643\n",
            "Training loss per 100 training steps: 0.006150693632662296\n",
            "Training loss per 100 training steps: 0.008554012514650822\n",
            "Training loss per 100 training steps: 0.009162633679807186\n",
            "Training loss per 100 training steps: 0.011939721181988716\n",
            "Training loss per 100 training steps: 0.01289798878133297\n",
            "Training loss per 100 training steps: 0.014052856713533401\n",
            "Training loss per 100 training steps: 0.013442796654999256\n",
            "Training loss per 100 training steps: 0.01333976723253727\n",
            "Training loss per 100 training steps: 0.013057922944426537\n",
            "Training loss per 100 training steps: 0.012431511655449867\n",
            "Training loss per 100 training steps: 0.01155977975577116\n",
            "Training loss per 100 training steps: 0.010930003598332405\n",
            "Training loss per 100 training steps: 0.010246401652693748\n",
            "Training loss per 100 training steps: 0.00989606510847807\n",
            "Training loss per 100 training steps: 0.010351329110562801\n",
            "Training loss per 100 training steps: 0.009929935447871685\n",
            "Training loss per 100 training steps: 0.00947868824005127\n",
            "Training loss per 100 training steps: 0.009267579764127731\n",
            "Training loss per 100 training steps: 0.009171987883746624\n",
            "Training loss epoch: 0.008847101591527462\n",
            "Training accuracy epoch: 0.997583019148653\n",
            "iteration 5 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Finished predicting 500 sentences\n",
            "Finished predicting 600 sentences\n",
            "Finished predicting 700 sentences\n",
            "Finished predicting 800 sentences\n",
            "Finished predicting 900 sentences\n",
            "Finished predicting 1000 sentences\n",
            "Finished predicting 1100 sentences\n",
            "Iteration 5 Prediction data saved\n",
            "ITERATION 5 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 6 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.0676228478550911\n",
            "Training loss per 100 training steps: 0.021275514736771584\n",
            "Training loss per 100 training steps: 0.012359663844108582\n",
            "Training loss per 100 training steps: 0.013164440169930458\n",
            "Training loss per 100 training steps: 0.012299403548240662\n",
            "Training loss per 100 training steps: 0.012077500112354755\n",
            "Training loss per 100 training steps: 0.011143202893435955\n",
            "Training loss per 100 training steps: 0.010051023215055466\n",
            "Training loss per 100 training steps: 0.009472152218222618\n",
            "Training loss per 100 training steps: 0.010460488498210907\n",
            "Training loss per 100 training steps: 0.010107535868883133\n",
            "Training loss per 100 training steps: 0.010145626962184906\n",
            "Training loss per 100 training steps: 0.009392380714416504\n",
            "Training loss per 100 training steps: 0.008852725848555565\n",
            "Training loss per 100 training steps: 0.00836616288870573\n",
            "Training loss per 100 training steps: 0.00807981751859188\n",
            "Training loss per 100 training steps: 0.009204447269439697\n",
            "Training loss per 100 training steps: 0.00888963881880045\n",
            "Training loss per 100 training steps: 0.008522570133209229\n",
            "Training loss per 100 training steps: 0.008125006221234798\n",
            "Training loss epoch: 0.008379723876714706\n",
            "Training accuracy epoch: 0.9981940657462542\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.00038799041067250073\n",
            "Training loss per 100 training steps: 0.004603994078934193\n",
            "Training loss per 100 training steps: 0.0041053094901144505\n",
            "Training loss per 100 training steps: 0.003866602433845401\n",
            "Training loss per 100 training steps: 0.004138378892093897\n",
            "Training loss per 100 training steps: 0.007780740968883038\n",
            "Training loss per 100 training steps: 0.007990097627043724\n",
            "Training loss per 100 training steps: 0.0076013728976249695\n",
            "Training loss per 100 training steps: 0.00806936714798212\n",
            "Training loss per 100 training steps: 0.00768126780167222\n",
            "Training loss per 100 training steps: 0.007287421263754368\n",
            "Training loss per 100 training steps: 0.006890494376420975\n",
            "Training loss per 100 training steps: 0.006429168861359358\n",
            "Training loss per 100 training steps: 0.006232028361409903\n",
            "Training loss per 100 training steps: 0.006185353267937899\n",
            "Training loss per 100 training steps: 0.006119979079812765\n",
            "Training loss per 100 training steps: 0.006021647714078426\n",
            "Training loss per 100 training steps: 0.00622806278988719\n",
            "Training loss per 100 training steps: 0.005984467454254627\n",
            "Training loss per 100 training steps: 0.0059301722794771194\n",
            "Training loss epoch: 0.006051504984498024\n",
            "Training accuracy epoch: 0.9984239152258173\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.0003184474480804056\n",
            "Training loss per 100 training steps: 0.001016281545162201\n",
            "Training loss per 100 training steps: 0.004055257886648178\n",
            "Training loss per 100 training steps: 0.003863664111122489\n",
            "Training loss per 100 training steps: 0.0031692609190940857\n",
            "Training loss per 100 training steps: 0.003720044158399105\n",
            "Training loss per 100 training steps: 0.004098317585885525\n",
            "Training loss per 100 training steps: 0.004927998408675194\n",
            "Training loss per 100 training steps: 0.004781399853527546\n",
            "Training loss per 100 training steps: 0.005533556919544935\n",
            "Training loss per 100 training steps: 0.005176367238163948\n",
            "Training loss per 100 training steps: 0.00490246107801795\n",
            "Training loss per 100 training steps: 0.005027038045227528\n",
            "Training loss per 100 training steps: 0.004852456506341696\n",
            "Training loss per 100 training steps: 0.0046729133464396\n",
            "Training loss per 100 training steps: 0.004780935123562813\n",
            "Training loss per 100 training steps: 0.004986326210200787\n",
            "Training loss per 100 training steps: 0.0050917863845825195\n",
            "Training loss per 100 training steps: 0.006357523612678051\n",
            "Training loss per 100 training steps: 0.0068887341767549515\n",
            "Training loss epoch: 0.006832394748926163\n",
            "Training accuracy epoch: 0.9982412081235348\n",
            "iteration 6 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Finished predicting 500 sentences\n",
            "Finished predicting 600 sentences\n",
            "Finished predicting 700 sentences\n",
            "Finished predicting 800 sentences\n",
            "Finished predicting 900 sentences\n",
            "Finished predicting 1000 sentences\n",
            "Iteration 6 Prediction data saved\n",
            "ITERATION 6 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 7 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.0003195189929101616\n",
            "Training loss per 100 training steps: 0.002061430597677827\n",
            "Training loss per 100 training steps: 0.007725168485194445\n",
            "Training loss per 100 training steps: 0.007903116755187511\n",
            "Training loss per 100 training steps: 0.007904271595180035\n",
            "Training loss per 100 training steps: 0.007705050520598888\n",
            "Training loss per 100 training steps: 0.007125020492821932\n",
            "Training loss per 100 training steps: 0.0068194689229130745\n",
            "Training loss per 100 training steps: 0.007172565441578627\n",
            "Training loss per 100 training steps: 0.00839496124535799\n",
            "Training loss per 100 training steps: 0.007725948933511972\n",
            "Training loss per 100 training steps: 0.007112984545528889\n",
            "Training loss per 100 training steps: 0.007066929712891579\n",
            "Training loss per 100 training steps: 0.006602573674172163\n",
            "Training loss per 100 training steps: 0.007099977694451809\n",
            "Training loss per 100 training steps: 0.007148035801947117\n",
            "Training loss per 100 training steps: 0.007308612577617168\n",
            "Training loss per 100 training steps: 0.007274199277162552\n",
            "Training loss per 100 training steps: 0.007493353448808193\n",
            "Training loss per 100 training steps: 0.007384903263300657\n",
            "Training loss epoch: 0.0073745595291256905\n",
            "Training accuracy epoch: 0.9983322469961785\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.0003823331499006599\n",
            "Training loss per 100 training steps: 0.009553031995892525\n",
            "Training loss per 100 training steps: 0.007250043097883463\n",
            "Training loss per 100 training steps: 0.00813355017453432\n",
            "Training loss per 100 training steps: 0.008801495656371117\n",
            "Training loss per 100 training steps: 0.007473220583051443\n",
            "Training loss per 100 training steps: 0.006366437766700983\n",
            "Training loss per 100 training steps: 0.005717313848435879\n",
            "Training loss per 100 training steps: 0.006354200188070536\n",
            "Training loss per 100 training steps: 0.0057731871493160725\n",
            "Training loss per 100 training steps: 0.005445833317935467\n",
            "Training loss per 100 training steps: 0.005015307106077671\n",
            "Training loss per 100 training steps: 0.00464615086093545\n",
            "Training loss per 100 training steps: 0.004320608451962471\n",
            "Training loss per 100 training steps: 0.004098046571016312\n",
            "Training loss per 100 training steps: 0.0038992392364889383\n",
            "Training loss per 100 training steps: 0.0047936029732227325\n",
            "Training loss per 100 training steps: 0.0057259611785411835\n",
            "Training loss per 100 training steps: 0.005606380756944418\n",
            "Training loss per 100 training steps: 0.005973036400973797\n",
            "Training loss epoch: 0.006049399264156818\n",
            "Training accuracy epoch: 0.9985797732990563\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.00022530269052367657\n",
            "Training loss per 100 training steps: 0.0016045310767367482\n",
            "Training loss per 100 training steps: 0.004067374859005213\n",
            "Training loss per 100 training steps: 0.008110743947327137\n",
            "Training loss per 100 training steps: 0.007498776074498892\n",
            "Training loss per 100 training steps: 0.0064924187026917934\n",
            "Training loss per 100 training steps: 0.006320176180452108\n",
            "Training loss per 100 training steps: 0.0057818996720016\n",
            "Training loss per 100 training steps: 0.005758734419941902\n",
            "Training loss per 100 training steps: 0.005776197649538517\n",
            "Training loss per 100 training steps: 0.005422088783234358\n",
            "Training loss per 100 training steps: 0.0053734853863716125\n",
            "Training loss per 100 training steps: 0.005611125845462084\n",
            "Training loss per 100 training steps: 0.005464253015816212\n",
            "Training loss per 100 training steps: 0.005119886249303818\n",
            "Training loss per 100 training steps: 0.0048861573450267315\n",
            "Training loss per 100 training steps: 0.005099061410874128\n",
            "Training loss per 100 training steps: 0.005140435416251421\n",
            "Training loss per 100 training steps: 0.0054920027032494545\n",
            "Training loss per 100 training steps: 0.005447425879538059\n",
            "Training loss epoch: 0.005564048886299133\n",
            "Training accuracy epoch: 0.9986991767146035\n",
            "iteration 7 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Finished predicting 500 sentences\n",
            "Finished predicting 600 sentences\n",
            "Finished predicting 700 sentences\n",
            "Finished predicting 800 sentences\n",
            "Finished predicting 900 sentences\n",
            "Iteration 7 Prediction data saved\n",
            "ITERATION 7 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 8 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.00035126888542436063\n",
            "Training loss per 100 training steps: 0.002089700661599636\n",
            "Training loss per 100 training steps: 0.00221124105155468\n",
            "Training loss per 100 training steps: 0.003170591313391924\n",
            "Training loss per 100 training steps: 0.004288834519684315\n",
            "Training loss per 100 training steps: 0.004581153392791748\n",
            "Training loss per 100 training steps: 0.004292245022952557\n",
            "Training loss per 100 training steps: 0.0042815376073122025\n",
            "Training loss per 100 training steps: 0.004955335520207882\n",
            "Training loss per 100 training steps: 0.00462276441976428\n",
            "Training loss per 100 training steps: 0.004650988150388002\n",
            "Training loss per 100 training steps: 0.004577136132866144\n",
            "Training loss per 100 training steps: 0.005104835145175457\n",
            "Training loss per 100 training steps: 0.005435363855212927\n",
            "Training loss per 100 training steps: 0.005230903159826994\n",
            "Training loss per 100 training steps: 0.004950238857418299\n",
            "Training loss per 100 training steps: 0.004764620680361986\n",
            "Training loss per 100 training steps: 0.004618404433131218\n",
            "Training loss per 100 training steps: 0.004501668736338615\n",
            "Training loss per 100 training steps: 0.00428428128361702\n",
            "Training loss epoch: 0.004163998179137707\n",
            "Training accuracy epoch: 0.9990964514361225\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.0002005239512072876\n",
            "Training loss per 100 training steps: 0.0023651523515582085\n",
            "Training loss per 100 training steps: 0.008578132838010788\n",
            "Training loss per 100 training steps: 0.007037095259875059\n",
            "Training loss per 100 training steps: 0.007940736599266529\n",
            "Training loss per 100 training steps: 0.00803196057677269\n",
            "Training loss per 100 training steps: 0.008395791985094547\n",
            "Training loss per 100 training steps: 0.008525789715349674\n",
            "Training loss per 100 training steps: 0.007768432144075632\n",
            "Training loss per 100 training steps: 0.007039297372102737\n",
            "Training loss per 100 training steps: 0.006467696279287338\n",
            "Training loss per 100 training steps: 0.005947617348283529\n",
            "Training loss per 100 training steps: 0.005753812845796347\n",
            "Training loss per 100 training steps: 0.005568026099354029\n",
            "Training loss per 100 training steps: 0.005204684101045132\n",
            "Training loss per 100 training steps: 0.0049716816283762455\n",
            "Training loss per 100 training steps: 0.004795730579644442\n",
            "Training loss per 100 training steps: 0.005099479109048843\n",
            "Training loss per 100 training steps: 0.005693366751074791\n",
            "Training loss per 100 training steps: 0.0059304372407495975\n",
            "Training loss epoch: 0.0057884990237653255\n",
            "Training accuracy epoch: 0.9985303806323946\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.0004170292231719941\n",
            "Training loss per 100 training steps: 0.004746629856526852\n",
            "Training loss per 100 training steps: 0.010010996833443642\n",
            "Training loss per 100 training steps: 0.007623016368597746\n",
            "Training loss per 100 training steps: 0.006085887551307678\n",
            "Training loss per 100 training steps: 0.005565099883824587\n",
            "Training loss per 100 training steps: 0.004902493674308062\n",
            "Training loss per 100 training steps: 0.005584972910583019\n",
            "Training loss per 100 training steps: 0.005997419357299805\n",
            "Training loss per 100 training steps: 0.005660900380462408\n",
            "Training loss per 100 training steps: 0.005785966292023659\n",
            "Training loss per 100 training steps: 0.005859272088855505\n",
            "Training loss per 100 training steps: 0.006857869680970907\n",
            "Training loss per 100 training steps: 0.0071569327265024185\n",
            "Training loss per 100 training steps: 0.0073369513265788555\n",
            "Training loss per 100 training steps: 0.007023664191365242\n",
            "Training loss per 100 training steps: 0.007480080239474773\n",
            "Training loss per 100 training steps: 0.007904534228146076\n",
            "Training loss per 100 training steps: 0.007676940876990557\n",
            "Training loss per 100 training steps: 0.007573689799755812\n",
            "Training loss epoch: 0.0072234151884913445\n",
            "Training accuracy epoch: 0.9984213172189726\n",
            "iteration 8 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Finished predicting 500 sentences\n",
            "Finished predicting 600 sentences\n",
            "Finished predicting 700 sentences\n",
            "Finished predicting 800 sentences\n",
            "Iteration 8 Prediction data saved\n",
            "ITERATION 8 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 9 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.00023505701392423362\n",
            "Training loss per 100 training steps: 0.022059274837374687\n",
            "Training loss per 100 training steps: 0.011894755065441132\n",
            "Training loss per 100 training steps: 0.010261962190270424\n",
            "Training loss per 100 training steps: 0.010852682404220104\n",
            "Training loss per 100 training steps: 0.009598907083272934\n",
            "Training loss per 100 training steps: 0.008416006341576576\n",
            "Training loss per 100 training steps: 0.008807502686977386\n",
            "Training loss per 100 training steps: 0.00832345336675644\n",
            "Training loss per 100 training steps: 0.007904858328402042\n",
            "Training loss per 100 training steps: 0.007651207968592644\n",
            "Training loss per 100 training steps: 0.007197118829935789\n",
            "Training loss per 100 training steps: 0.006652099546045065\n",
            "Training loss per 100 training steps: 0.006531966850161552\n",
            "Training loss per 100 training steps: 0.006178122013807297\n",
            "Training loss per 100 training steps: 0.005815169308334589\n",
            "Training loss per 100 training steps: 0.006208653096109629\n",
            "Training loss per 100 training steps: 0.006894337944686413\n",
            "Training loss per 100 training steps: 0.0068964287638664246\n",
            "Training loss per 100 training steps: 0.00659831753000617\n",
            "Training loss epoch: 0.006564493291079998\n",
            "Training accuracy epoch: 0.9986450789039484\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.00033254027948714793\n",
            "Training loss per 100 training steps: 0.003831910900771618\n",
            "Training loss per 100 training steps: 0.005184275563806295\n",
            "Training loss per 100 training steps: 0.007364600896835327\n",
            "Training loss per 100 training steps: 0.008069142699241638\n",
            "Training loss per 100 training steps: 0.006837385706603527\n",
            "Training loss per 100 training steps: 0.005800716113299131\n",
            "Training loss per 100 training steps: 0.005705374293029308\n",
            "Training loss per 100 training steps: 0.005106332246214151\n",
            "Training loss per 100 training steps: 0.005031996872276068\n",
            "Training loss per 100 training steps: 0.004798961337655783\n",
            "Training loss per 100 training steps: 0.005186509806662798\n",
            "Training loss per 100 training steps: 0.005317153409123421\n",
            "Training loss per 100 training steps: 0.005215547047555447\n",
            "Training loss per 100 training steps: 0.005280289798974991\n",
            "Training loss per 100 training steps: 0.005418712273240089\n",
            "Training loss per 100 training steps: 0.0056222169660031796\n",
            "Training loss per 100 training steps: 0.005501307547092438\n",
            "Training loss per 100 training steps: 0.00535967480391264\n",
            "Training loss per 100 training steps: 0.005101591814309359\n",
            "Training loss epoch: 0.004910055547952652\n",
            "Training accuracy epoch: 0.9989078494689294\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.0002501839480828494\n",
            "Training loss per 100 training steps: 0.0052817994728684425\n",
            "Training loss per 100 training steps: 0.0037522430066019297\n",
            "Training loss per 100 training steps: 0.002702906960621476\n",
            "Training loss per 100 training steps: 0.002965497551485896\n",
            "Training loss per 100 training steps: 0.003028996055945754\n",
            "Training loss per 100 training steps: 0.005408570636063814\n",
            "Training loss per 100 training steps: 0.005371229723095894\n",
            "Training loss per 100 training steps: 0.006412007845938206\n",
            "Training loss per 100 training steps: 0.006345752160996199\n",
            "Training loss per 100 training steps: 0.006461792625486851\n",
            "Training loss per 100 training steps: 0.006014436483383179\n",
            "Training loss per 100 training steps: 0.005554712377488613\n",
            "Training loss per 100 training steps: 0.005195487290620804\n",
            "Training loss per 100 training steps: 0.005133317317813635\n",
            "Training loss per 100 training steps: 0.006021861918270588\n",
            "Training loss per 100 training steps: 0.005939597263932228\n",
            "Training loss per 100 training steps: 0.005616921931505203\n",
            "Training loss per 100 training steps: 0.0055976444855332375\n",
            "Training loss per 100 training steps: 0.00537786865606904\n",
            "Training loss epoch: 0.005143606569617987\n",
            "Training accuracy epoch: 0.9987608050583717\n",
            "iteration 9 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Finished predicting 500 sentences\n",
            "Finished predicting 600 sentences\n",
            "Finished predicting 700 sentences\n",
            "Iteration 9 Prediction data saved\n",
            "ITERATION 9 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 10 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.00017331952403765172\n",
            "Training loss per 100 training steps: 0.0030663753859698772\n",
            "Training loss per 100 training steps: 0.0029062682297080755\n",
            "Training loss per 100 training steps: 0.004731193650513887\n",
            "Training loss per 100 training steps: 0.0037555324379354715\n",
            "Training loss per 100 training steps: 0.00343409669585526\n",
            "Training loss per 100 training steps: 0.003491339972242713\n",
            "Training loss per 100 training steps: 0.004191744141280651\n",
            "Training loss per 100 training steps: 0.004741046577692032\n",
            "Training loss per 100 training steps: 0.004869591910392046\n",
            "Training loss per 100 training steps: 0.004537809640169144\n",
            "Training loss per 100 training steps: 0.004226036369800568\n",
            "Training loss per 100 training steps: 0.0041177463717758656\n",
            "Training loss per 100 training steps: 0.003963230177760124\n",
            "Training loss per 100 training steps: 0.0037964200600981712\n",
            "Training loss per 100 training steps: 0.003646654775366187\n",
            "Training loss per 100 training steps: 0.003448953852057457\n",
            "Training loss per 100 training steps: 0.0037370906211435795\n",
            "Training loss per 100 training steps: 0.0035755732096731663\n",
            "Training loss per 100 training steps: 0.003632443957030773\n",
            "Training loss epoch: 0.003472053911536932\n",
            "Training accuracy epoch: 0.9991613397274982\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.00021992134861648083\n",
            "Training loss per 100 training steps: 0.0025597992353141308\n",
            "Training loss per 100 training steps: 0.0031989915296435356\n",
            "Training loss per 100 training steps: 0.0028466074727475643\n",
            "Training loss per 100 training steps: 0.002755909226834774\n",
            "Training loss per 100 training steps: 0.002821520669385791\n",
            "Training loss per 100 training steps: 0.002475780202075839\n",
            "Training loss per 100 training steps: 0.002435847418382764\n",
            "Training loss per 100 training steps: 0.0052554006688296795\n",
            "Training loss per 100 training steps: 0.004931120667606592\n",
            "Training loss per 100 training steps: 0.0045066410675644875\n",
            "Training loss per 100 training steps: 0.00417408999055624\n",
            "Training loss per 100 training steps: 0.004012409131973982\n",
            "Training loss per 100 training steps: 0.003853869391605258\n",
            "Training loss per 100 training steps: 0.0036328460555523634\n",
            "Training loss per 100 training steps: 0.00386060425080359\n",
            "Training loss per 100 training steps: 0.0036942914593964815\n",
            "Training loss per 100 training steps: 0.004324121866375208\n",
            "Training loss per 100 training steps: 0.004133136942982674\n",
            "Training loss per 100 training steps: 0.003947691060602665\n",
            "Training loss epoch: 0.0037768399342894554\n",
            "Training accuracy epoch: 0.9989776587264405\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.000194385604117997\n",
            "Training loss per 100 training steps: 0.002452260348945856\n",
            "Training loss per 100 training steps: 0.0021960928570479155\n",
            "Training loss per 100 training steps: 0.002596925711259246\n",
            "Training loss per 100 training steps: 0.002382866572588682\n",
            "Training loss per 100 training steps: 0.002109625842422247\n",
            "Training loss per 100 training steps: 0.0019603564869612455\n",
            "Training loss per 100 training steps: 0.001935898675583303\n",
            "Training loss per 100 training steps: 0.002075229538604617\n",
            "Training loss per 100 training steps: 0.0020725890062749386\n",
            "Training loss per 100 training steps: 0.0020024662371724844\n",
            "Training loss per 100 training steps: 0.002148671308532357\n",
            "Training loss per 100 training steps: 0.0025346961338073015\n",
            "Training loss per 100 training steps: 0.0027874913066625595\n",
            "Training loss per 100 training steps: 0.0026546805165708065\n",
            "Training loss per 100 training steps: 0.0025762065779417753\n",
            "Training loss per 100 training steps: 0.002873476594686508\n",
            "Training loss per 100 training steps: 0.003280326258391142\n",
            "Training loss per 100 training steps: 0.003135206876322627\n",
            "Training loss per 100 training steps: 0.0030358980875462294\n",
            "Training loss epoch: 0.0028938378673046827\n",
            "Training accuracy epoch: 0.9992337275768479\n",
            "iteration 10 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Finished predicting 500 sentences\n",
            "Finished predicting 600 sentences\n",
            "Iteration 10 Prediction data saved\n",
            "ITERATION 10 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 11 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.00015486657503060997\n",
            "Training loss per 100 training steps: 0.00037036483990959823\n",
            "Training loss per 100 training steps: 0.0005440855748020113\n",
            "Training loss per 100 training steps: 0.0013755986001342535\n",
            "Training loss per 100 training steps: 0.0040881880559027195\n",
            "Training loss per 100 training steps: 0.0052824681624770164\n",
            "Training loss per 100 training steps: 0.005079254973679781\n",
            "Training loss per 100 training steps: 0.006065241526812315\n",
            "Training loss per 100 training steps: 0.007174707017838955\n",
            "Training loss per 100 training steps: 0.006742500234395266\n",
            "Training loss per 100 training steps: 0.006603828631341457\n",
            "Training loss per 100 training steps: 0.00708012655377388\n",
            "Training loss per 100 training steps: 0.006730100139975548\n",
            "Training loss per 100 training steps: 0.006607224233448505\n",
            "Training loss per 100 training steps: 0.006929296068847179\n",
            "Training loss per 100 training steps: 0.0073243300430476665\n",
            "Training loss per 100 training steps: 0.007057005539536476\n",
            "Training loss per 100 training steps: 0.007542228326201439\n",
            "Training loss per 100 training steps: 0.007563879247754812\n",
            "Training loss per 100 training steps: 0.007345530204474926\n",
            "Training loss epoch: 0.007121735252439976\n",
            "Training accuracy epoch: 0.9985385486244052\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.00027502875309437513\n",
            "Training loss per 100 training steps: 0.006091836374253035\n",
            "Training loss per 100 training steps: 0.0036064761225134134\n",
            "Training loss per 100 training steps: 0.00574791943654418\n",
            "Training loss per 100 training steps: 0.009677113965153694\n",
            "Training loss per 100 training steps: 0.009722149930894375\n",
            "Training loss per 100 training steps: 0.008622221648693085\n",
            "Training loss per 100 training steps: 0.008447464555501938\n",
            "Training loss per 100 training steps: 0.007563802879303694\n",
            "Training loss per 100 training steps: 0.006989171728491783\n",
            "Training loss per 100 training steps: 0.006385947577655315\n",
            "Training loss per 100 training steps: 0.006019034422934055\n",
            "Training loss per 100 training steps: 0.005789174698293209\n",
            "Training loss per 100 training steps: 0.005493242293596268\n",
            "Training loss per 100 training steps: 0.0051190839149057865\n",
            "Training loss per 100 training steps: 0.004819398280233145\n",
            "Training loss per 100 training steps: 0.004552457947283983\n",
            "Training loss per 100 training steps: 0.0043463269248604774\n",
            "Training loss per 100 training steps: 0.004136783070862293\n",
            "Training loss per 100 training steps: 0.004004341550171375\n",
            "Training loss epoch: 0.0038431533612310886\n",
            "Training accuracy epoch: 0.9990526126856084\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.00011906891450053081\n",
            "Training loss per 100 training steps: 0.008136984892189503\n",
            "Training loss per 100 training steps: 0.004683834500610828\n",
            "Training loss per 100 training steps: 0.005065535195171833\n",
            "Training loss per 100 training steps: 0.004097958095371723\n",
            "Training loss per 100 training steps: 0.003502471838146448\n",
            "Training loss per 100 training steps: 0.0032136121299117804\n",
            "Training loss per 100 training steps: 0.002949496963992715\n",
            "Training loss per 100 training steps: 0.002889232477173209\n",
            "Training loss per 100 training steps: 0.0026021020021289587\n",
            "Training loss per 100 training steps: 0.002372700721025467\n",
            "Training loss per 100 training steps: 0.0023475035559386015\n",
            "Training loss per 100 training steps: 0.002255464205518365\n",
            "Training loss per 100 training steps: 0.0021110777743160725\n",
            "Training loss per 100 training steps: 0.0020527117885649204\n",
            "Training loss per 100 training steps: 0.0019305560272186995\n",
            "Training loss per 100 training steps: 0.0018395265797153115\n",
            "Training loss per 100 training steps: 0.0017551881028339267\n",
            "Training loss per 100 training steps: 0.0016713705845177174\n",
            "Training loss per 100 training steps: 0.0015952385729178786\n",
            "Training loss epoch: 0.0015497778076678514\n",
            "Training accuracy epoch: 0.999613115923455\n",
            "iteration 11 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Finished predicting 500 sentences\n",
            "Finished predicting 600 sentences\n",
            "Iteration 11 Prediction data saved\n",
            "ITERATION 11 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 12 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.00011524708679644391\n",
            "Training loss per 100 training steps: 0.00017430017760489136\n",
            "Training loss per 100 training steps: 0.005305936094373465\n",
            "Training loss per 100 training steps: 0.0038053023163229227\n",
            "Training loss per 100 training steps: 0.004740564618259668\n",
            "Training loss per 100 training steps: 0.005395065527409315\n",
            "Training loss per 100 training steps: 0.004751000553369522\n",
            "Training loss per 100 training steps: 0.005244536325335503\n",
            "Training loss per 100 training steps: 0.004785178229212761\n",
            "Training loss per 100 training steps: 0.004713525529950857\n",
            "Training loss per 100 training steps: 0.004917910788208246\n",
            "Training loss per 100 training steps: 0.0050966874696314335\n",
            "Training loss per 100 training steps: 0.009096893481910229\n",
            "Training loss per 100 training steps: 0.008769807405769825\n",
            "Training loss per 100 training steps: 0.009472889825701714\n",
            "Training loss per 100 training steps: 0.008895767852663994\n",
            "Training loss per 100 training steps: 0.008906735107302666\n",
            "Training loss per 100 training steps: 0.008467487059533596\n",
            "Training loss per 100 training steps: 0.008066890761256218\n",
            "Training loss per 100 training steps: 0.00770460395142436\n",
            "Training loss epoch: 0.007399705238640308\n",
            "Training accuracy epoch: 0.9985039988358905\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.0001600403047632426\n",
            "Training loss per 100 training steps: 0.00578062841668725\n",
            "Training loss per 100 training steps: 0.005066676996648312\n",
            "Training loss per 100 training steps: 0.0038015246391296387\n",
            "Training loss per 100 training steps: 0.0035760861355811357\n",
            "Training loss per 100 training steps: 0.004174277186393738\n",
            "Training loss per 100 training steps: 0.003556361421942711\n",
            "Training loss per 100 training steps: 0.003986265044659376\n",
            "Training loss per 100 training steps: 0.00463822903111577\n",
            "Training loss per 100 training steps: 0.004353778902441263\n",
            "Training loss per 100 training steps: 0.004067654721438885\n",
            "Training loss per 100 training steps: 0.0038152083288878202\n",
            "Training loss per 100 training steps: 0.003910192754119635\n",
            "Training loss per 100 training steps: 0.003654968924820423\n",
            "Training loss per 100 training steps: 0.003757090773433447\n",
            "Training loss per 100 training steps: 0.0036051548086106777\n",
            "Training loss per 100 training steps: 0.0036127606872469187\n",
            "Training loss per 100 training steps: 0.004425558727234602\n",
            "Training loss per 100 training steps: 0.004440392833203077\n",
            "Training loss per 100 training steps: 0.004336068406701088\n",
            "Training loss epoch: 0.004439881071448326\n",
            "Training accuracy epoch: 0.9989346832918701\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.00016662267444189638\n",
            "Training loss per 100 training steps: 0.003488523419946432\n",
            "Training loss per 100 training steps: 0.0021820191759616137\n",
            "Training loss per 100 training steps: 0.0021194627042859793\n",
            "Training loss per 100 training steps: 0.0035193616058677435\n",
            "Training loss per 100 training steps: 0.0029335019644349813\n",
            "Training loss per 100 training steps: 0.002964435378089547\n",
            "Training loss per 100 training steps: 0.003026968566700816\n",
            "Training loss per 100 training steps: 0.0027957644779235125\n",
            "Training loss per 100 training steps: 0.002548133721575141\n",
            "Training loss per 100 training steps: 0.002341632964089513\n",
            "Training loss per 100 training steps: 0.002147265477105975\n",
            "Training loss per 100 training steps: 0.0020037218928337097\n",
            "Training loss per 100 training steps: 0.0020504873245954514\n",
            "Training loss per 100 training steps: 0.00301061081700027\n",
            "Training loss per 100 training steps: 0.002981541445478797\n",
            "Training loss per 100 training steps: 0.003206609981134534\n",
            "Training loss per 100 training steps: 0.0032155371736735106\n",
            "Training loss per 100 training steps: 0.0033598255831748247\n",
            "Training loss per 100 training steps: 0.003539260243996978\n",
            "Training loss epoch: 0.0034705873113125563\n",
            "Training accuracy epoch: 0.9992951771067535\n",
            "iteration 12 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Finished predicting 500 sentences\n",
            "Iteration 12 Prediction data saved\n",
            "ITERATION 12 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 13 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.00026822774088941514\n",
            "Training loss per 100 training steps: 0.001173815573565662\n",
            "Training loss per 100 training steps: 0.0017420562217012048\n",
            "Training loss per 100 training steps: 0.001818200689740479\n",
            "Training loss per 100 training steps: 0.0017827466363087296\n",
            "Training loss per 100 training steps: 0.0017787106335163116\n",
            "Training loss per 100 training steps: 0.001536158611997962\n",
            "Training loss per 100 training steps: 0.001503773732110858\n",
            "Training loss per 100 training steps: 0.005114580038934946\n",
            "Training loss per 100 training steps: 0.0051512885838747025\n",
            "Training loss per 100 training steps: 0.004783246200531721\n",
            "Training loss per 100 training steps: 0.004894305486232042\n",
            "Training loss per 100 training steps: 0.004794880282133818\n",
            "Training loss per 100 training steps: 0.004446510225534439\n",
            "Training loss per 100 training steps: 0.004302866291254759\n",
            "Training loss per 100 training steps: 0.004076377023011446\n",
            "Training loss per 100 training steps: 0.003841323545202613\n",
            "Training loss per 100 training steps: 0.00363690173253417\n",
            "Training loss per 100 training steps: 0.0037834427785128355\n",
            "Training loss per 100 training steps: 0.0038414164446294308\n",
            "Training loss epoch: 0.003692281199619174\n",
            "Training accuracy epoch: 0.9990705902447486\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.00014988768089096993\n",
            "Training loss per 100 training steps: 0.010676922276616096\n",
            "Training loss per 100 training steps: 0.010818214155733585\n",
            "Training loss per 100 training steps: 0.007410542108118534\n",
            "Training loss per 100 training steps: 0.006257221102714539\n",
            "Training loss per 100 training steps: 0.0051924423314630985\n",
            "Training loss per 100 training steps: 0.00454531516879797\n",
            "Training loss per 100 training steps: 0.003947304096072912\n",
            "Training loss per 100 training steps: 0.006908487994223833\n",
            "Training loss per 100 training steps: 0.0063607278279960155\n",
            "Training loss per 100 training steps: 0.005785112269222736\n",
            "Training loss per 100 training steps: 0.00535666523501277\n",
            "Training loss per 100 training steps: 0.004935977980494499\n",
            "Training loss per 100 training steps: 0.0045799207873642445\n",
            "Training loss per 100 training steps: 0.004273813217878342\n",
            "Training loss per 100 training steps: 0.004056788515299559\n",
            "Training loss per 100 training steps: 0.0038455897010862827\n",
            "Training loss per 100 training steps: 0.003635905683040619\n",
            "Training loss per 100 training steps: 0.0034427016507834196\n",
            "Training loss per 100 training steps: 0.0032685711048543453\n",
            "Training loss epoch: 0.003136958461254835\n",
            "Training accuracy epoch: 0.9994261796457524\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 8.691727998666465e-05\n",
            "Training loss per 100 training steps: 0.0005282804486341774\n",
            "Training loss per 100 training steps: 0.0009263427346013486\n",
            "Training loss per 100 training steps: 0.001065877266228199\n",
            "Training loss per 100 training steps: 0.0009135953732766211\n",
            "Training loss per 100 training steps: 0.0016865241341292858\n",
            "Training loss per 100 training steps: 0.003330864477902651\n",
            "Training loss per 100 training steps: 0.0030637418385595083\n",
            "Training loss per 100 training steps: 0.0027277437038719654\n",
            "Training loss per 100 training steps: 0.002489719307050109\n",
            "Training loss per 100 training steps: 0.002526174997910857\n",
            "Training loss per 100 training steps: 0.002711937529966235\n",
            "Training loss per 100 training steps: 0.0029961818363517523\n",
            "Training loss per 100 training steps: 0.0030287650879472494\n",
            "Training loss per 100 training steps: 0.0028735960368067026\n",
            "Training loss per 100 training steps: 0.002691191853955388\n",
            "Training loss per 100 training steps: 0.002534387866035104\n",
            "Training loss per 100 training steps: 0.0024652776774019003\n",
            "Training loss per 100 training steps: 0.0023557760287076235\n",
            "Training loss per 100 training steps: 0.0026048035360872746\n",
            "Training loss epoch: 0.0029647466726601124\n",
            "Training accuracy epoch: 0.9993470284696347\n",
            "iteration 13 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Iteration 13 Prediction data saved\n",
            "ITERATION 13 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 14 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 9.082952601602301e-05\n",
            "Training loss per 100 training steps: 0.01632530800998211\n",
            "Training loss per 100 training steps: 0.009658901020884514\n",
            "Training loss per 100 training steps: 0.007969984784722328\n",
            "Training loss per 100 training steps: 0.006122868042439222\n",
            "Training loss per 100 training steps: 0.007315338589251041\n",
            "Training loss per 100 training steps: 0.00666945381090045\n",
            "Training loss per 100 training steps: 0.006145423743873835\n",
            "Training loss per 100 training steps: 0.005615840665996075\n",
            "Training loss per 100 training steps: 0.00545982550829649\n",
            "Training loss per 100 training steps: 0.005158363841474056\n",
            "Training loss per 100 training steps: 0.005077546462416649\n",
            "Training loss per 100 training steps: 0.005109901539981365\n",
            "Training loss per 100 training steps: 0.004742014221847057\n",
            "Training loss per 100 training steps: 0.004454440902918577\n",
            "Training loss per 100 training steps: 0.004229322075843811\n",
            "Training loss per 100 training steps: 0.004251172300428152\n",
            "Training loss per 100 training steps: 0.004020269960165024\n",
            "Training loss per 100 training steps: 0.004182965960353613\n",
            "Training loss per 100 training steps: 0.004054068122059107\n",
            "Training loss epoch: 0.004009806551039219\n",
            "Training accuracy epoch: 0.9991486261262247\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.0001516319316579029\n",
            "Training loss per 100 training steps: 0.000403968762839213\n",
            "Training loss per 100 training steps: 0.0037640719674527645\n",
            "Training loss per 100 training steps: 0.002659313380718231\n",
            "Training loss per 100 training steps: 0.0021953179966658354\n",
            "Training loss per 100 training steps: 0.003632731270045042\n",
            "Training loss per 100 training steps: 0.003160333028063178\n",
            "Training loss per 100 training steps: 0.00292247929610312\n",
            "Training loss per 100 training steps: 0.002600917825475335\n",
            "Training loss per 100 training steps: 0.0034024349879473448\n",
            "Training loss per 100 training steps: 0.003899509087204933\n",
            "Training loss per 100 training steps: 0.003640406997874379\n",
            "Training loss per 100 training steps: 0.003669027704745531\n",
            "Training loss per 100 training steps: 0.003989689983427525\n",
            "Training loss per 100 training steps: 0.003765858942642808\n",
            "Training loss per 100 training steps: 0.004892940632998943\n",
            "Training loss per 100 training steps: 0.005327609367668629\n",
            "Training loss per 100 training steps: 0.005099847912788391\n",
            "Training loss per 100 training steps: 0.005633131600916386\n",
            "Training loss per 100 training steps: 0.0053543997928500175\n",
            "Training loss epoch: 0.0051404135301709175\n",
            "Training accuracy epoch: 0.9990378857220212\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 8.083674765657634e-05\n",
            "Training loss per 100 training steps: 0.0015821045963093638\n",
            "Training loss per 100 training steps: 0.00321267731487751\n",
            "Training loss per 100 training steps: 0.002289058640599251\n",
            "Training loss per 100 training steps: 0.0028445112984627485\n",
            "Training loss per 100 training steps: 0.004160965792834759\n",
            "Training loss per 100 training steps: 0.003948944620788097\n",
            "Training loss per 100 training steps: 0.004919011145830154\n",
            "Training loss per 100 training steps: 0.004564931616187096\n",
            "Training loss per 100 training steps: 0.004464620724320412\n",
            "Training loss per 100 training steps: 0.004095214419066906\n",
            "Training loss per 100 training steps: 0.003817320568487048\n",
            "Training loss per 100 training steps: 0.0035152453929185867\n",
            "Training loss per 100 training steps: 0.003260211553424597\n",
            "Training loss per 100 training steps: 0.0030403449200093746\n",
            "Training loss per 100 training steps: 0.0028476750012487173\n",
            "Training loss per 100 training steps: 0.0026826905086636543\n",
            "Training loss per 100 training steps: 0.0025348078925162554\n",
            "Training loss per 100 training steps: 0.0029535749927163124\n",
            "Training loss per 100 training steps: 0.0028143355157226324\n",
            "Training loss epoch: 0.0026859755162149668\n",
            "Training accuracy epoch: 0.9994586071534086\n",
            "iteration 14 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Iteration 14 Prediction data saved\n",
            "ITERATION 14 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 15 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 7.719237328274176e-05\n",
            "Training loss per 100 training steps: 0.0046870592050254345\n",
            "Training loss per 100 training steps: 0.002553804311901331\n",
            "Training loss per 100 training steps: 0.001822455320507288\n",
            "Training loss per 100 training steps: 0.0017172649968415499\n",
            "Training loss per 100 training steps: 0.0029633515514433384\n",
            "Training loss per 100 training steps: 0.003239324316382408\n",
            "Training loss per 100 training steps: 0.002852656180039048\n",
            "Training loss per 100 training steps: 0.0029986449517309666\n",
            "Training loss per 100 training steps: 0.0031254123896360397\n",
            "Training loss per 100 training steps: 0.003350541228428483\n",
            "Training loss per 100 training steps: 0.0033351993188261986\n",
            "Training loss per 100 training steps: 0.0030772758182138205\n",
            "Training loss per 100 training steps: 0.0033309152349829674\n",
            "Training loss per 100 training steps: 0.0035809725522994995\n",
            "Training loss per 100 training steps: 0.00343514047563076\n",
            "Training loss per 100 training steps: 0.0035846915561705828\n",
            "Training loss per 100 training steps: 0.004505068063735962\n",
            "Training loss per 100 training steps: 0.004870195873081684\n",
            "Training loss per 100 training steps: 0.004737359005957842\n",
            "Training loss epoch: 0.0045151845552027225\n",
            "Training accuracy epoch: 0.9991583308873933\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 6.68915527057834e-05\n",
            "Training loss per 100 training steps: 0.006788762751966715\n",
            "Training loss per 100 training steps: 0.017021335661411285\n",
            "Training loss per 100 training steps: 0.011738291010260582\n",
            "Training loss per 100 training steps: 0.009314783848822117\n",
            "Training loss per 100 training steps: 0.007507836911827326\n",
            "Training loss per 100 training steps: 0.006325801368802786\n",
            "Training loss per 100 training steps: 0.0054698521271348\n",
            "Training loss per 100 training steps: 0.004839473403990269\n",
            "Training loss per 100 training steps: 0.005061300005763769\n",
            "Training loss per 100 training steps: 0.005210441537201405\n",
            "Training loss per 100 training steps: 0.004802777897566557\n",
            "Training loss per 100 training steps: 0.0050981175154447556\n",
            "Training loss per 100 training steps: 0.006667132023721933\n",
            "Training loss per 100 training steps: 0.006254736799746752\n",
            "Training loss per 100 training steps: 0.006042349152266979\n",
            "Training loss per 100 training steps: 0.005824623629450798\n",
            "Training loss per 100 training steps: 0.005533281713724136\n",
            "Training loss per 100 training steps: 0.005251683760434389\n",
            "Training loss per 100 training steps: 0.005015747155994177\n",
            "Training loss epoch: 0.004775077570229769\n",
            "Training accuracy epoch: 0.9993036407231582\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.00010156477219425142\n",
            "Training loss per 100 training steps: 0.00043931312393397093\n",
            "Training loss per 100 training steps: 0.0004272444057278335\n",
            "Training loss per 100 training steps: 0.00032958807423710823\n",
            "Training loss per 100 training steps: 0.00033576012356206775\n",
            "Training loss per 100 training steps: 0.0005516920937225223\n",
            "Training loss per 100 training steps: 0.000523794733453542\n",
            "Training loss per 100 training steps: 0.00046939903404563665\n",
            "Training loss per 100 training steps: 0.0004379579040687531\n",
            "Training loss per 100 training steps: 0.001032441621646285\n",
            "Training loss per 100 training steps: 0.0019111899891868234\n",
            "Training loss per 100 training steps: 0.0018010899657383561\n",
            "Training loss per 100 training steps: 0.0030093479435890913\n",
            "Training loss per 100 training steps: 0.0035661335568875074\n",
            "Training loss per 100 training steps: 0.003439336782321334\n",
            "Training loss per 100 training steps: 0.003250313922762871\n",
            "Training loss per 100 training steps: 0.003065550234168768\n",
            "Training loss per 100 training steps: 0.0028970427811145782\n",
            "Training loss per 100 training steps: 0.002782227238640189\n",
            "Training loss per 100 training steps: 0.0031070003751665354\n",
            "Training loss epoch: 0.0035232852678745985\n",
            "Training accuracy epoch: 0.9995001245571722\n",
            "iteration 15 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Finished predicting 400 sentences\n",
            "Iteration 15 Prediction data saved\n",
            "ITERATION 15 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 16 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.00017768096586223692\n",
            "Training loss per 100 training steps: 0.0037377174012362957\n",
            "Training loss per 100 training steps: 0.002431335859000683\n",
            "Training loss per 100 training steps: 0.0018408719915896654\n",
            "Training loss per 100 training steps: 0.0014315019361674786\n",
            "Training loss per 100 training steps: 0.0018800715915858746\n",
            "Training loss per 100 training steps: 0.0016134883044287562\n",
            "Training loss per 100 training steps: 0.0036311144940555096\n",
            "Training loss per 100 training steps: 0.003484563436359167\n",
            "Training loss per 100 training steps: 0.003243387909606099\n",
            "Training loss per 100 training steps: 0.00295169185847044\n",
            "Training loss per 100 training steps: 0.002696574665606022\n",
            "Training loss per 100 training steps: 0.002519944217056036\n",
            "Training loss per 100 training steps: 0.0031117922626435757\n",
            "Training loss per 100 training steps: 0.003059410722926259\n",
            "Training loss per 100 training steps: 0.002876162063330412\n",
            "Training loss per 100 training steps: 0.0027377523947507143\n",
            "Training loss per 100 training steps: 0.0030159936286509037\n",
            "Training loss per 100 training steps: 0.00290640234015882\n",
            "Training loss per 100 training steps: 0.002800834598019719\n",
            "Training loss epoch: 0.002687396015971899\n",
            "Training accuracy epoch: 0.9994442748169676\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.00023567966127302498\n",
            "Training loss per 100 training steps: 0.00016244183643721044\n",
            "Training loss per 100 training steps: 0.000780256581492722\n",
            "Training loss per 100 training steps: 0.0006986805819906294\n",
            "Training loss per 100 training steps: 0.0006201944779604673\n",
            "Training loss per 100 training steps: 0.0005269909743219614\n",
            "Training loss per 100 training steps: 0.00046457440475933254\n",
            "Training loss per 100 training steps: 0.00048215920105576515\n",
            "Training loss per 100 training steps: 0.00045358928036876023\n",
            "Training loss per 100 training steps: 0.0005191746167838573\n",
            "Training loss per 100 training steps: 0.0005072754574939609\n",
            "Training loss per 100 training steps: 0.0004835427680518478\n",
            "Training loss per 100 training steps: 0.000451684434665367\n",
            "Training loss per 100 training steps: 0.0005930516635999084\n",
            "Training loss per 100 training steps: 0.0005598334828391671\n",
            "Training loss per 100 training steps: 0.0006087071960791945\n",
            "Training loss per 100 training steps: 0.0012220591306686401\n",
            "Training loss per 100 training steps: 0.001351290731690824\n",
            "Training loss per 100 training steps: 0.0014325560769066215\n",
            "Training loss per 100 training steps: 0.001414244412444532\n",
            "Training loss epoch: 0.001360896392725408\n",
            "Training accuracy epoch: 0.9996014780144777\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.00010220013791695237\n",
            "Training loss per 100 training steps: 0.0009973009582608938\n",
            "Training loss per 100 training steps: 0.0005576930125243962\n",
            "Training loss per 100 training steps: 0.0004437380121089518\n",
            "Training loss per 100 training steps: 0.001379602588713169\n",
            "Training loss per 100 training steps: 0.0015071098459884524\n",
            "Training loss per 100 training steps: 0.001549025997519493\n",
            "Training loss per 100 training steps: 0.0027594559360295534\n",
            "Training loss per 100 training steps: 0.0029784899670630693\n",
            "Training loss per 100 training steps: 0.0030330924782902002\n",
            "Training loss per 100 training steps: 0.002793246414512396\n",
            "Training loss per 100 training steps: 0.004335377365350723\n",
            "Training loss per 100 training steps: 0.004452370572835207\n",
            "Training loss per 100 training steps: 0.004306524060666561\n",
            "Training loss per 100 training steps: 0.004498418886214495\n",
            "Training loss per 100 training steps: 0.004334141965955496\n",
            "Training loss per 100 training steps: 0.004857236053794622\n",
            "Training loss per 100 training steps: 0.004595890175551176\n",
            "Training loss per 100 training steps: 0.004443610552698374\n",
            "Training loss per 100 training steps: 0.004220880102366209\n",
            "Training loss epoch: 0.0042532808147370815\n",
            "Training accuracy epoch: 0.9990049958292523\n",
            "iteration 16 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Iteration 16 Prediction data saved\n",
            "ITERATION 16 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 17 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.00010459477925905958\n",
            "Training loss per 100 training steps: 0.00044149853056296706\n",
            "Training loss per 100 training steps: 0.0007598791271448135\n",
            "Training loss per 100 training steps: 0.0006548107485286891\n",
            "Training loss per 100 training steps: 0.0008861750829964876\n",
            "Training loss per 100 training steps: 0.0010192522313445807\n",
            "Training loss per 100 training steps: 0.000976528855971992\n",
            "Training loss per 100 training steps: 0.0027471892535686493\n",
            "Training loss per 100 training steps: 0.003206918016076088\n",
            "Training loss per 100 training steps: 0.0028922101482748985\n",
            "Training loss per 100 training steps: 0.0026305357459932566\n",
            "Training loss per 100 training steps: 0.0030637464951723814\n",
            "Training loss per 100 training steps: 0.0042202468030154705\n",
            "Training loss per 100 training steps: 0.003912314306944609\n",
            "Training loss per 100 training steps: 0.004629028495401144\n",
            "Training loss per 100 training steps: 0.004656525794416666\n",
            "Training loss per 100 training steps: 0.004397453740239143\n",
            "Training loss per 100 training steps: 0.0042650941759347916\n",
            "Training loss per 100 training steps: 0.00454393494874239\n",
            "Training loss per 100 training steps: 0.004319124389439821\n",
            "Training loss epoch: 0.004114555660635233\n",
            "Training accuracy epoch: 0.9991913925162086\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.00011379089119145647\n",
            "Training loss per 100 training steps: 0.0003426837211009115\n",
            "Training loss per 100 training steps: 0.012978502549231052\n",
            "Training loss per 100 training steps: 0.01205168105661869\n",
            "Training loss per 100 training steps: 0.009591002017259598\n",
            "Training loss per 100 training steps: 0.008098605088889599\n",
            "Training loss per 100 training steps: 0.006875328253954649\n",
            "Training loss per 100 training steps: 0.005934341344982386\n",
            "Training loss per 100 training steps: 0.0052232383750379086\n",
            "Training loss per 100 training steps: 0.004691086709499359\n",
            "Training loss per 100 training steps: 0.004307836294174194\n",
            "Training loss per 100 training steps: 0.0040041315369307995\n",
            "Training loss per 100 training steps: 0.005011424422264099\n",
            "Training loss per 100 training steps: 0.005061411298811436\n",
            "Training loss per 100 training steps: 0.005061911419034004\n",
            "Training loss per 100 training steps: 0.0048294104635715485\n",
            "Training loss per 100 training steps: 0.004589684307575226\n",
            "Training loss per 100 training steps: 0.004462207667529583\n",
            "Training loss per 100 training steps: 0.004230697173625231\n",
            "Training loss per 100 training steps: 0.004016574937850237\n",
            "Training loss epoch: 0.003822918515652418\n",
            "Training accuracy epoch: 0.999177431232108\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.00010204696445725858\n",
            "Training loss per 100 training steps: 0.00046757503878325224\n",
            "Training loss per 100 training steps: 0.0014703433262184262\n",
            "Training loss per 100 training steps: 0.0013384846970438957\n",
            "Training loss per 100 training steps: 0.0012561348266899586\n",
            "Training loss per 100 training steps: 0.0010411397088319063\n",
            "Training loss per 100 training steps: 0.0020569947082549334\n",
            "Training loss per 100 training steps: 0.0018927976489067078\n",
            "Training loss per 100 training steps: 0.0018632826395332813\n",
            "Training loss per 100 training steps: 0.0018770638853311539\n",
            "Training loss per 100 training steps: 0.0017039990052580833\n",
            "Training loss per 100 training steps: 0.0016854099230840802\n",
            "Training loss per 100 training steps: 0.0016159217339009047\n",
            "Training loss per 100 training steps: 0.0015351000474765897\n",
            "Training loss per 100 training steps: 0.001465394627302885\n",
            "Training loss per 100 training steps: 0.0016267934115603566\n",
            "Training loss per 100 training steps: 0.0017410442233085632\n",
            "Training loss per 100 training steps: 0.001678228029049933\n",
            "Training loss per 100 training steps: 0.001627473277039826\n",
            "Training loss per 100 training steps: 0.0017377721378579736\n",
            "Training loss epoch: 0.0017145804595202208\n",
            "Training accuracy epoch: 0.9994978013111345\n",
            "iteration 17 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Finished predicting 300 sentences\n",
            "Iteration 17 Prediction data saved\n",
            "ITERATION 17 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 18 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 6.374376971507445e-05\n",
            "Training loss per 100 training steps: 9.538426820654422e-05\n",
            "Training loss per 100 training steps: 0.0014277888694778085\n",
            "Training loss per 100 training steps: 0.003124165814369917\n",
            "Training loss per 100 training steps: 0.004544427152723074\n",
            "Training loss per 100 training steps: 0.003792806062847376\n",
            "Training loss per 100 training steps: 0.0032210175413638353\n",
            "Training loss per 100 training steps: 0.0027822747360914946\n",
            "Training loss per 100 training steps: 0.0024839253164827824\n",
            "Training loss per 100 training steps: 0.0024780884850770235\n",
            "Training loss per 100 training steps: 0.002304368419572711\n",
            "Training loss per 100 training steps: 0.003742193803191185\n",
            "Training loss per 100 training steps: 0.003559259930625558\n",
            "Training loss per 100 training steps: 0.0034931935369968414\n",
            "Training loss per 100 training steps: 0.0032943664118647575\n",
            "Training loss per 100 training steps: 0.004247601144015789\n",
            "Training loss per 100 training steps: 0.003993811551481485\n",
            "Training loss per 100 training steps: 0.0037772723007947206\n",
            "Training loss per 100 training steps: 0.004040743224322796\n",
            "Training loss per 100 training steps: 0.0043768854811787605\n",
            "Training loss epoch: 0.004230726510286331\n",
            "Training accuracy epoch: 0.9992628466719831\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.00010052244033431634\n",
            "Training loss per 100 training steps: 0.0029969820752739906\n",
            "Training loss per 100 training steps: 0.00176348933018744\n",
            "Training loss per 100 training steps: 0.0017133643850684166\n",
            "Training loss per 100 training steps: 0.0014896392822265625\n",
            "Training loss per 100 training steps: 0.0013649050379171968\n",
            "Training loss per 100 training steps: 0.0012367621529847383\n",
            "Training loss per 100 training steps: 0.00155917729716748\n",
            "Training loss per 100 training steps: 0.0014363662339746952\n",
            "Training loss per 100 training steps: 0.001665212563239038\n",
            "Training loss per 100 training steps: 0.0015179491601884365\n",
            "Training loss per 100 training steps: 0.0015720720402896404\n",
            "Training loss per 100 training steps: 0.0041350144892930984\n",
            "Training loss per 100 training steps: 0.003823793726041913\n",
            "Training loss per 100 training steps: 0.003723388072103262\n",
            "Training loss per 100 training steps: 0.0040633175522089005\n",
            "Training loss per 100 training steps: 0.0038469878491014242\n",
            "Training loss per 100 training steps: 0.0036470089107751846\n",
            "Training loss per 100 training steps: 0.0034986354876309633\n",
            "Training loss per 100 training steps: 0.0036539339926093817\n",
            "Training loss epoch: 0.0036906518507748842\n",
            "Training accuracy epoch: 0.9993571733514336\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.00015046542102936655\n",
            "Training loss per 100 training steps: 0.00021635767188854516\n",
            "Training loss per 100 training steps: 0.000664173741824925\n",
            "Training loss per 100 training steps: 0.0008293072460219264\n",
            "Training loss per 100 training steps: 0.000897175632417202\n",
            "Training loss per 100 training steps: 0.000759866030421108\n",
            "Training loss per 100 training steps: 0.0016723553417250514\n",
            "Training loss per 100 training steps: 0.0014666771749034524\n",
            "Training loss per 100 training steps: 0.0014911459293216467\n",
            "Training loss per 100 training steps: 0.001344679156318307\n",
            "Training loss per 100 training steps: 0.001868010382167995\n",
            "Training loss per 100 training steps: 0.002058529993519187\n",
            "Training loss per 100 training steps: 0.001926877535879612\n",
            "Training loss per 100 training steps: 0.001990029588341713\n",
            "Training loss per 100 training steps: 0.0020295411814004183\n",
            "Training loss per 100 training steps: 0.0023411752190440893\n",
            "Training loss per 100 training steps: 0.0023158451076596975\n",
            "Training loss per 100 training steps: 0.002202525269240141\n",
            "Training loss per 100 training steps: 0.0021605310030281544\n",
            "Training loss per 100 training steps: 0.002140163676813245\n",
            "Training loss epoch: 0.002230866812169552\n",
            "Training accuracy epoch: 0.9996505143884221\n",
            "iteration 18 updated model saved\n",
            "Finished predicting 0 sentences\n",
            "Finished predicting 100 sentences\n",
            "Finished predicting 200 sentences\n",
            "Iteration 18 Prediction data saved\n",
            "ITERATION 18 ENDS!\n",
            "==========================================================================\n",
            "ITERATION 19 STARTS!\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.00025976175675168633\n",
            "Training loss per 100 training steps: 0.000923970656003803\n",
            "Training loss per 100 training steps: 0.0011682250769808888\n",
            "Training loss per 100 training steps: 0.0017399872886016965\n",
            "Training loss per 100 training steps: 0.002052247291430831\n",
            "Training loss per 100 training steps: 0.0022108261473476887\n",
            "Training loss per 100 training steps: 0.001863703248091042\n",
            "Training loss per 100 training steps: 0.001654486870393157\n",
            "Training loss per 100 training steps: 0.0018362590344622731\n",
            "Training loss per 100 training steps: 0.001667102100327611\n",
            "Training loss per 100 training steps: 0.0015732974279671907\n",
            "Training loss per 100 training steps: 0.001440507941879332\n",
            "Training loss per 100 training steps: 0.00136967608705163\n",
            "Training loss per 100 training steps: 0.0012859567068517208\n",
            "Training loss per 100 training steps: 0.0012027508346363902\n",
            "Training loss per 100 training steps: 0.0014477045042440295\n",
            "Training loss per 100 training steps: 0.0014071501791477203\n",
            "Training loss per 100 training steps: 0.0014264690689742565\n",
            "Training loss per 100 training steps: 0.0016187687870115042\n",
            "Training loss per 100 training steps: 0.0018113035475835204\n",
            "Training loss epoch: 0.002771341009065509\n",
            "Training accuracy epoch: 0.9994173092361026\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.00021635642042383552\n",
            "Training loss per 100 training steps: 0.008057483471930027\n",
            "Training loss per 100 training steps: 0.004927996080368757\n",
            "Training loss per 100 training steps: 0.004673040006309748\n",
            "Training loss per 100 training steps: 0.00471553485840559\n",
            "Training loss per 100 training steps: 0.00395906250923872\n",
            "Training loss per 100 training steps: 0.00442415289580822\n",
            "Training loss per 100 training steps: 0.0038899369537830353\n",
            "Training loss per 100 training steps: 0.003483540378510952\n",
            "Training loss per 100 training steps: 0.0031184011604636908\n",
            "Training loss per 100 training steps: 0.0028358022682368755\n",
            "Training loss per 100 training steps: 0.002588312840089202\n",
            "Training loss per 100 training steps: 0.0025316691026091576\n",
            "Training loss per 100 training steps: 0.002345446962863207\n",
            "Training loss per 100 training steps: 0.002185411751270294\n",
            "Training loss per 100 training steps: 0.002078550634905696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Each Iteration on Test Set"
      ],
      "metadata": {
        "id": "LcJr1T286HxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iteration = list()\n",
        "validation_acc = list()\n",
        "for i in [0,1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,19]:\n",
        "  iteration.append(i)\n",
        "  print(f'TEST FOR ITERATION {i}')\n",
        "  pretrained_model_path = f\"/content/unsupervised_selftraining_bert_iteration_{i}/model\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path, add_prefix_space=True)\n",
        "  model = AutoModelForTokenClassification.from_pretrained(pretrained_model_path)\n",
        "  model.to(device)\n",
        "  testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
        "  testing_loader = DataLoader(testing_set, **params)\n",
        "  labels_10_percent, predictions_10_percent = valid(model, testing_loader)\n",
        "  cr = classification_report(labels_10_percent, predictions_10_percent, output_dict=True)\n",
        "  validation_acc.append(cr['accuracy'])\n",
        "  print(classification_report(labels_10_percent, predictions_10_percent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPYvY0tJBSS6",
        "outputId": "52cbd2f2-d2b1-422a-de00-0b98132b66a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST FOR ITERATION 0\n",
            "Validation loss per 100 evaluation steps: 0.08317039906978607\n",
            "Validation loss per 100 evaluation steps: 0.09991497596688938\n",
            "Validation loss per 100 evaluation steps: 0.09557142025508468\n",
            "Validation loss per 100 evaluation steps: 0.1120252816650113\n",
            "Validation loss per 100 evaluation steps: 0.11830760703823921\n",
            "Validation loss per 100 evaluation steps: 0.11703591154583408\n",
            "Validation loss per 100 evaluation steps: 0.12831346885364708\n",
            "Validation loss per 100 evaluation steps: 0.1248453179654664\n",
            "Validation loss per 100 evaluation steps: 0.12195654263715265\n",
            "Validation loss per 100 evaluation steps: 0.12261124253820847\n",
            "Validation loss per 100 evaluation steps: 0.12095476276465468\n",
            "Validation loss per 100 evaluation steps: 0.11974230854197673\n",
            "Validation loss per 100 evaluation steps: 0.12146936445811037\n",
            "Validation loss per 100 evaluation steps: 0.12097090970886604\n",
            "Validation loss per 100 evaluation steps: 0.11804427861898797\n",
            "Validation loss per 100 evaluation steps: 0.11635015904562154\n",
            "Validation loss per 100 evaluation steps: 0.1160987196209492\n",
            "Validation loss per 100 evaluation steps: 0.11352023741037096\n",
            "Validation Loss: 0.11596238828904831\n",
            "Validation Accuracy: 0.9713147667927579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.93      0.97      0.95       665\n",
            "           #       0.77      0.99      0.86       267\n",
            "           $       0.94      0.98      0.96       380\n",
            "           &       0.99      0.99      0.99       456\n",
            "           ,       0.99      0.98      0.99      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.98      0.97      0.97      1344\n",
            "           D       0.99      0.99      0.99      1614\n",
            "           E       0.79      0.96      0.87       246\n",
            "           G       0.77      0.48      0.59       257\n",
            "           L       0.99      0.97      0.98       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.98      0.97      0.98      3582\n",
            "           O       0.99      0.99      0.99      1848\n",
            "           P       0.99      0.98      0.98      2281\n",
            "           R       0.95      0.97      0.96      1219\n",
            "           S       1.00      0.46      0.63        28\n",
            "           T       0.92      0.96      0.94       163\n",
            "           U       1.00      0.98      0.99       418\n",
            "           V       1.00      0.99      0.99      3954\n",
            "           X       0.71      0.88      0.79        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.93      0.27      0.42        52\n",
            "           ^       0.93      0.96      0.94      1662\n",
            "           ~       0.94      0.97      0.95       910\n",
            "\n",
            "    accuracy                           0.97     26078\n",
            "   macro avg       0.86      0.83      0.83     26078\n",
            "weighted avg       0.97      0.97      0.97     26078\n",
            "\n",
            "TEST FOR ITERATION 1\n",
            "Validation loss per 100 evaluation steps: 0.24564361572265625\n",
            "Validation loss per 100 evaluation steps: 0.19630905764758366\n",
            "Validation loss per 100 evaluation steps: 0.19703965073750257\n",
            "Validation loss per 100 evaluation steps: 0.2191685673264799\n",
            "Validation loss per 100 evaluation steps: 0.2304534807326876\n",
            "Validation loss per 100 evaluation steps: 0.22990037891860426\n",
            "Validation loss per 100 evaluation steps: 0.2495829006241512\n",
            "Validation loss per 100 evaluation steps: 0.24263573864223462\n",
            "Validation loss per 100 evaluation steps: 0.23628955346902195\n",
            "Validation loss per 100 evaluation steps: 0.23506721865353986\n",
            "Validation loss per 100 evaluation steps: 0.2297200438094681\n",
            "Validation loss per 100 evaluation steps: 0.22809540555188787\n",
            "Validation loss per 100 evaluation steps: 0.22716090074870918\n",
            "Validation loss per 100 evaluation steps: 0.2289427550646076\n",
            "Validation loss per 100 evaluation steps: 0.22636299000425697\n",
            "Validation loss per 100 evaluation steps: 0.22390596680266756\n",
            "Validation loss per 100 evaluation steps: 0.2217838637971124\n",
            "Validation loss per 100 evaluation steps: 0.2178362433529884\n",
            "Validation Loss: 0.22453327717518848\n",
            "Validation Accuracy: 0.9522664414673919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.83      0.96      0.89       665\n",
            "           #       0.71      1.00      0.83       267\n",
            "           $       0.92      0.97      0.94       380\n",
            "           &       0.98      0.99      0.98       456\n",
            "           ,       0.98      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.96      0.94      0.95      1344\n",
            "           D       0.99      0.98      0.98      1614\n",
            "           E       0.65      0.96      0.78       246\n",
            "           G       0.71      0.37      0.49       257\n",
            "           L       0.98      0.96      0.97       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.98      0.94      0.96      3582\n",
            "           O       0.99      0.98      0.98      1848\n",
            "           P       0.99      0.96      0.98      2281\n",
            "           R       0.92      0.93      0.93      1219\n",
            "           S       1.00      0.50      0.67        28\n",
            "           T       0.81      0.94      0.87       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.99      0.98      0.99      3954\n",
            "           X       0.55      0.84      0.67        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       1.00      0.10      0.18        52\n",
            "           ^       0.87      0.94      0.91      1662\n",
            "           ~       0.94      0.92      0.93       910\n",
            "\n",
            "    accuracy                           0.95     26078\n",
            "   macro avg       0.83      0.80      0.79     26078\n",
            "weighted avg       0.96      0.95      0.95     26078\n",
            "\n",
            "TEST FOR ITERATION 2\n",
            "Validation loss per 100 evaluation steps: 0.3969544470310211\n",
            "Validation loss per 100 evaluation steps: 0.22920529215912436\n",
            "Validation loss per 100 evaluation steps: 0.24017327308072128\n",
            "Validation loss per 100 evaluation steps: 0.26259176402271145\n",
            "Validation loss per 100 evaluation steps: 0.2719635660089483\n",
            "Validation loss per 100 evaluation steps: 0.2702700791043042\n",
            "Validation loss per 100 evaluation steps: 0.2925611516623874\n",
            "Validation loss per 100 evaluation steps: 0.28527399122230235\n",
            "Validation loss per 100 evaluation steps: 0.2780757300344894\n",
            "Validation loss per 100 evaluation steps: 0.27644346198321607\n",
            "Validation loss per 100 evaluation steps: 0.2712824375593228\n",
            "Validation loss per 100 evaluation steps: 0.27084945168060787\n",
            "Validation loss per 100 evaluation steps: 0.26953922692792653\n",
            "Validation loss per 100 evaluation steps: 0.27198154916124667\n",
            "Validation loss per 100 evaluation steps: 0.2688312457436842\n",
            "Validation loss per 100 evaluation steps: 0.2671317641681022\n",
            "Validation loss per 100 evaluation steps: 0.265109591470068\n",
            "Validation loss per 100 evaluation steps: 0.2623808687672544\n",
            "Validation Loss: 0.26992980675521006\n",
            "Validation Accuracy: 0.951165980080201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.90      0.94      0.92       665\n",
            "           #       0.70      1.00      0.82       267\n",
            "           $       0.89      0.98      0.93       380\n",
            "           &       0.98      0.99      0.98       456\n",
            "           ,       0.98      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.95      0.92      0.94      1344\n",
            "           D       0.98      0.97      0.98      1614\n",
            "           E       0.75      0.96      0.85       246\n",
            "           G       0.64      0.42      0.51       257\n",
            "           L       0.99      0.91      0.95       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.98      0.93      0.95      3582\n",
            "           O       0.98      0.98      0.98      1848\n",
            "           P       0.98      0.97      0.97      2281\n",
            "           R       0.93      0.92      0.92      1219\n",
            "           S       1.00      0.39      0.56        28\n",
            "           T       0.86      0.94      0.90       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.99      0.98      0.98      3954\n",
            "           X       0.58      0.88      0.70        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       1.00      0.29      0.45        52\n",
            "           ^       0.84      0.94      0.89      1662\n",
            "           ~       0.94      0.95      0.95       910\n",
            "\n",
            "    accuracy                           0.95     26078\n",
            "   macro avg       0.83      0.81      0.80     26078\n",
            "weighted avg       0.95      0.95      0.95     26078\n",
            "\n",
            "TEST FOR ITERATION 3\n",
            "Validation loss per 100 evaluation steps: 0.27089813351631165\n",
            "Validation loss per 100 evaluation steps: 0.27356760090765647\n",
            "Validation loss per 100 evaluation steps: 0.27934025019142006\n",
            "Validation loss per 100 evaluation steps: 0.3070964683648342\n",
            "Validation loss per 100 evaluation steps: 0.31390315292190013\n",
            "Validation loss per 100 evaluation steps: 0.3115219188223341\n",
            "Validation loss per 100 evaluation steps: 0.33883934327475407\n",
            "Validation loss per 100 evaluation steps: 0.32945448136612354\n",
            "Validation loss per 100 evaluation steps: 0.32241390967944283\n",
            "Validation loss per 100 evaluation steps: 0.320715543931194\n",
            "Validation loss per 100 evaluation steps: 0.3126294317376991\n",
            "Validation loss per 100 evaluation steps: 0.31310023148025234\n",
            "Validation loss per 100 evaluation steps: 0.31136195354085033\n",
            "Validation loss per 100 evaluation steps: 0.3132796191904029\n",
            "Validation loss per 100 evaluation steps: 0.3126041295734488\n",
            "Validation loss per 100 evaluation steps: 0.30911795818536775\n",
            "Validation loss per 100 evaluation steps: 0.3070732377532928\n",
            "Validation loss per 100 evaluation steps: 0.3038060843494749\n",
            "Validation Loss: 0.31304574879869335\n",
            "Validation Accuracy: 0.9482002517688972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.87      0.94      0.90       665\n",
            "           #       0.70      1.00      0.82       267\n",
            "           $       0.90      0.97      0.94       380\n",
            "           &       0.97      0.99      0.98       456\n",
            "           ,       0.98      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.95      0.93      0.94      1344\n",
            "           D       0.98      0.97      0.98      1614\n",
            "           E       0.75      0.96      0.84       246\n",
            "           G       0.66      0.40      0.50       257\n",
            "           L       1.00      0.94      0.96       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.97      0.92      0.95      3582\n",
            "           O       0.98      0.98      0.98      1848\n",
            "           P       0.98      0.97      0.97      2281\n",
            "           R       0.93      0.93      0.93      1219\n",
            "           S       0.90      0.32      0.47        28\n",
            "           T       0.83      0.92      0.87       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.99      0.97      0.98      3954\n",
            "           X       0.54      0.88      0.67        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       1.00      0.13      0.24        52\n",
            "           ^       0.84      0.94      0.89      1662\n",
            "           ~       0.94      0.94      0.94       910\n",
            "\n",
            "    accuracy                           0.95     26078\n",
            "   macro avg       0.83      0.80      0.79     26078\n",
            "weighted avg       0.95      0.95      0.95     26078\n",
            "\n",
            "TEST FOR ITERATION 4\n",
            "Validation loss per 100 evaluation steps: 0.3406142294406891\n",
            "Validation loss per 100 evaluation steps: 0.32671860775202605\n",
            "Validation loss per 100 evaluation steps: 0.3065712078602895\n",
            "Validation loss per 100 evaluation steps: 0.330173552321619\n",
            "Validation loss per 100 evaluation steps: 0.33427242770230897\n",
            "Validation loss per 100 evaluation steps: 0.33413690764682036\n",
            "Validation loss per 100 evaluation steps: 0.3587654537179708\n",
            "Validation loss per 100 evaluation steps: 0.35255447215491226\n",
            "Validation loss per 100 evaluation steps: 0.34402424328643716\n",
            "Validation loss per 100 evaluation steps: 0.34233764714087567\n",
            "Validation loss per 100 evaluation steps: 0.3367082176070956\n",
            "Validation loss per 100 evaluation steps: 0.33735844220301786\n",
            "Validation loss per 100 evaluation steps: 0.33488260079727566\n",
            "Validation loss per 100 evaluation steps: 0.33774942175504624\n",
            "Validation loss per 100 evaluation steps: 0.33587418177699774\n",
            "Validation loss per 100 evaluation steps: 0.33218391771445555\n",
            "Validation loss per 100 evaluation steps: 0.32831484068966704\n",
            "Validation loss per 100 evaluation steps: 0.32642601381126307\n",
            "Validation Loss: 0.3347570695996588\n",
            "Validation Accuracy: 0.9493146726126729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.91      0.93      0.92       665\n",
            "           #       0.70      1.00      0.82       267\n",
            "           $       0.86      0.98      0.92       380\n",
            "           &       0.98      0.99      0.98       456\n",
            "           ,       0.98      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.93      0.93      0.93      1344\n",
            "           D       0.98      0.97      0.98      1614\n",
            "           E       0.78      0.96      0.86       246\n",
            "           G       0.55      0.45      0.49       257\n",
            "           L       0.97      0.97      0.97       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.97      0.94      0.95      3582\n",
            "           O       0.98      0.98      0.98      1848\n",
            "           P       0.98      0.96      0.97      2281\n",
            "           R       0.91      0.93      0.92      1219\n",
            "           S       0.94      0.54      0.68        28\n",
            "           T       0.84      0.94      0.88       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.99      0.97      0.98      3954\n",
            "           X       0.58      0.88      0.70        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.93      0.25      0.39        52\n",
            "           ^       0.89      0.92      0.90      1662\n",
            "           ~       0.93      0.94      0.94       910\n",
            "\n",
            "    accuracy                           0.95     26078\n",
            "   macro avg       0.82      0.81      0.80     26078\n",
            "weighted avg       0.95      0.95      0.95     26078\n",
            "\n",
            "TEST FOR ITERATION 5\n",
            "Validation loss per 100 evaluation steps: 0.6584530472755432\n",
            "Validation loss per 100 evaluation steps: 0.40018717787591024\n",
            "Validation loss per 100 evaluation steps: 0.368181957912936\n",
            "Validation loss per 100 evaluation steps: 0.38992385632172655\n",
            "Validation loss per 100 evaluation steps: 0.38809655726609454\n",
            "Validation loss per 100 evaluation steps: 0.3855337607239631\n",
            "Validation loss per 100 evaluation steps: 0.4083504743566134\n",
            "Validation loss per 100 evaluation steps: 0.3986293153307185\n",
            "Validation loss per 100 evaluation steps: 0.3892419001540236\n",
            "Validation loss per 100 evaluation steps: 0.38628903778906587\n",
            "Validation loss per 100 evaluation steps: 0.3819424471692875\n",
            "Validation loss per 100 evaluation steps: 0.3807422590321357\n",
            "Validation loss per 100 evaluation steps: 0.37772601989180005\n",
            "Validation loss per 100 evaluation steps: 0.38274708177093686\n",
            "Validation loss per 100 evaluation steps: 0.38217637677145994\n",
            "Validation loss per 100 evaluation steps: 0.3802521097022521\n",
            "Validation loss per 100 evaluation steps: 0.3769015914553981\n",
            "Validation loss per 100 evaluation steps: 0.3743893204374099\n",
            "Validation Loss: 0.38359080922939004\n",
            "Validation Accuracy: 0.9475100180055261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.89      0.93      0.91       665\n",
            "           #       0.71      1.00      0.83       267\n",
            "           $       0.90      0.97      0.93       380\n",
            "           &       0.97      0.99      0.98       456\n",
            "           ,       0.98      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.93      0.93      0.93      1344\n",
            "           D       0.98      0.97      0.98      1614\n",
            "           E       0.75      0.96      0.84       246\n",
            "           G       0.60      0.42      0.50       257\n",
            "           L       0.99      0.93      0.96       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.97      0.93      0.95      3582\n",
            "           O       0.98      0.97      0.98      1848\n",
            "           P       0.97      0.97      0.97      2281\n",
            "           R       0.92      0.93      0.93      1219\n",
            "           S       0.93      0.46      0.62        28\n",
            "           T       0.82      0.94      0.88       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.99      0.97      0.98      3954\n",
            "           X       0.56      0.80      0.66        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       1.00      0.19      0.32        52\n",
            "           ^       0.87      0.92      0.89      1662\n",
            "           ~       0.94      0.93      0.93       910\n",
            "\n",
            "    accuracy                           0.95     26078\n",
            "   macro avg       0.82      0.80      0.80     26078\n",
            "weighted avg       0.95      0.95      0.95     26078\n",
            "\n",
            "TEST FOR ITERATION 6\n",
            "Validation loss per 100 evaluation steps: 0.5092023611068726\n",
            "Validation loss per 100 evaluation steps: 0.38643580844633324\n",
            "Validation loss per 100 evaluation steps: 0.37095828226728966\n",
            "Validation loss per 100 evaluation steps: 0.39171085618318857\n",
            "Validation loss per 100 evaluation steps: 0.3956534549166404\n",
            "Validation loss per 100 evaluation steps: 0.39725495817054585\n",
            "Validation loss per 100 evaluation steps: 0.41847743622611155\n",
            "Validation loss per 100 evaluation steps: 0.40949084835505056\n",
            "Validation loss per 100 evaluation steps: 0.3980429042099552\n",
            "Validation loss per 100 evaluation steps: 0.39335573839365656\n",
            "Validation loss per 100 evaluation steps: 0.3880194723678724\n",
            "Validation loss per 100 evaluation steps: 0.38762897221560216\n",
            "Validation loss per 100 evaluation steps: 0.38389844656134503\n",
            "Validation loss per 100 evaluation steps: 0.38675270921756616\n",
            "Validation loss per 100 evaluation steps: 0.38611959548912994\n",
            "Validation loss per 100 evaluation steps: 0.38157660654080916\n",
            "Validation loss per 100 evaluation steps: 0.38020422976859763\n",
            "Validation loss per 100 evaluation steps: 0.37733462864321116\n",
            "Validation Loss: 0.38577760645141157\n",
            "Validation Accuracy: 0.9465913077827152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.84      0.94      0.89       665\n",
            "           #       0.71      1.00      0.83       267\n",
            "           $       0.89      0.97      0.93       380\n",
            "           &       0.98      0.98      0.98       456\n",
            "           ,       0.98      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.94      0.93      0.93      1344\n",
            "           D       0.98      0.97      0.97      1614\n",
            "           E       0.69      0.95      0.80       246\n",
            "           G       0.51      0.43      0.47       257\n",
            "           L       0.98      0.94      0.96       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.96      0.94      0.95      3582\n",
            "           O       0.98      0.97      0.98      1848\n",
            "           P       0.98      0.96      0.97      2281\n",
            "           R       0.92      0.92      0.92      1219\n",
            "           S       0.93      0.46      0.62        28\n",
            "           T       0.85      0.93      0.89       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.99      0.97      0.98      3954\n",
            "           X       0.58      0.84      0.69        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.90      0.17      0.29        52\n",
            "           ^       0.89      0.91      0.90      1662\n",
            "           ~       0.94      0.90      0.92       910\n",
            "\n",
            "    accuracy                           0.95     26078\n",
            "   macro avg       0.82      0.80      0.79     26078\n",
            "weighted avg       0.95      0.95      0.95     26078\n",
            "\n",
            "TEST FOR ITERATION 7\n",
            "Validation loss per 100 evaluation steps: 0.75799560546875\n",
            "Validation loss per 100 evaluation steps: 0.402909882369899\n",
            "Validation loss per 100 evaluation steps: 0.4124293414511633\n",
            "Validation loss per 100 evaluation steps: 0.42987828868061484\n",
            "Validation loss per 100 evaluation steps: 0.4383783645312734\n",
            "Validation loss per 100 evaluation steps: 0.43527559515831726\n",
            "Validation loss per 100 evaluation steps: 0.46220889547547994\n",
            "Validation loss per 100 evaluation steps: 0.4560668481213924\n",
            "Validation loss per 100 evaluation steps: 0.44452864750570364\n",
            "Validation loss per 100 evaluation steps: 0.43948351509375805\n",
            "Validation loss per 100 evaluation steps: 0.43469208870745196\n",
            "Validation loss per 100 evaluation steps: 0.4357036800421312\n",
            "Validation loss per 100 evaluation steps: 0.43156946705481525\n",
            "Validation loss per 100 evaluation steps: 0.4355799053823587\n",
            "Validation loss per 100 evaluation steps: 0.44097187164019114\n",
            "Validation loss per 100 evaluation steps: 0.4332897054941\n",
            "Validation loss per 100 evaluation steps: 0.43622903862255596\n",
            "Validation loss per 100 evaluation steps: 0.4343866296838875\n",
            "Validation Loss: 0.44435436855102295\n",
            "Validation Accuracy: 0.941546125442239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.80      0.95      0.87       665\n",
            "           #       0.69      1.00      0.81       267\n",
            "           $       0.89      0.97      0.93       380\n",
            "           &       0.97      0.99      0.98       456\n",
            "           ,       0.97      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.95      0.90      0.92      1344\n",
            "           D       0.97      0.98      0.97      1614\n",
            "           E       0.69      0.95      0.80       246\n",
            "           G       0.59      0.37      0.45       257\n",
            "           L       0.98      0.93      0.96       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.97      0.93      0.95      3582\n",
            "           O       0.99      0.95      0.97      1848\n",
            "           P       0.97      0.96      0.97      2281\n",
            "           R       0.89      0.92      0.91      1219\n",
            "           S       0.88      0.54      0.67        28\n",
            "           T       0.83      0.92      0.87       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.99      0.97      0.98      3954\n",
            "           X       0.59      0.88      0.71        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.93      0.27      0.42        52\n",
            "           ^       0.87      0.92      0.89      1662\n",
            "           ~       0.93      0.92      0.93       910\n",
            "\n",
            "    accuracy                           0.94     26078\n",
            "   macro avg       0.81      0.81      0.80     26078\n",
            "weighted avg       0.95      0.94      0.94     26078\n",
            "\n",
            "TEST FOR ITERATION 8\n",
            "Validation loss per 100 evaluation steps: 0.3709373474121094\n",
            "Validation loss per 100 evaluation steps: 0.38197441527586534\n",
            "Validation loss per 100 evaluation steps: 0.39319632325806647\n",
            "Validation loss per 100 evaluation steps: 0.4237321600834234\n",
            "Validation loss per 100 evaluation steps: 0.4303291378750046\n",
            "Validation loss per 100 evaluation steps: 0.4238734503918107\n",
            "Validation loss per 100 evaluation steps: 0.44938987792758445\n",
            "Validation loss per 100 evaluation steps: 0.4432227835948369\n",
            "Validation loss per 100 evaluation steps: 0.432443349705423\n",
            "Validation loss per 100 evaluation steps: 0.4274283002727695\n",
            "Validation loss per 100 evaluation steps: 0.4219189497001513\n",
            "Validation loss per 100 evaluation steps: 0.4234359841163391\n",
            "Validation loss per 100 evaluation steps: 0.41834397233181425\n",
            "Validation loss per 100 evaluation steps: 0.422088151646005\n",
            "Validation loss per 100 evaluation steps: 0.41940180957991324\n",
            "Validation loss per 100 evaluation steps: 0.41808400310284394\n",
            "Validation loss per 100 evaluation steps: 0.41708967129185076\n",
            "Validation loss per 100 evaluation steps: 0.41506023862055863\n",
            "Validation Loss: 0.4260520780993446\n",
            "Validation Accuracy: 0.9450961019947882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.84      0.94      0.88       665\n",
            "           #       0.71      1.00      0.83       267\n",
            "           $       0.92      0.96      0.94       380\n",
            "           &       0.96      0.99      0.97       456\n",
            "           ,       0.97      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.93      0.91      0.92      1344\n",
            "           D       0.98      0.97      0.97      1614\n",
            "           E       0.73      0.93      0.81       246\n",
            "           G       0.59      0.41      0.48       257\n",
            "           L       0.98      0.95      0.96       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.96      0.93      0.95      3582\n",
            "           O       0.99      0.97      0.98      1848\n",
            "           P       0.98      0.97      0.97      2281\n",
            "           R       0.90      0.93      0.91      1219\n",
            "           S       0.94      0.54      0.68        28\n",
            "           T       0.86      0.93      0.89       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.99      0.97      0.98      3954\n",
            "           X       0.61      0.80      0.69        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.91      0.19      0.32        52\n",
            "           ^       0.89      0.91      0.90      1662\n",
            "           ~       0.94      0.92      0.93       910\n",
            "\n",
            "    accuracy                           0.95     26078\n",
            "   macro avg       0.82      0.80      0.80     26078\n",
            "weighted avg       0.95      0.95      0.95     26078\n",
            "\n",
            "TEST FOR ITERATION 9\n",
            "Validation loss per 100 evaluation steps: 0.5202181339263916\n",
            "Validation loss per 100 evaluation steps: 0.4449003915785334\n",
            "Validation loss per 100 evaluation steps: 0.43949685813991957\n",
            "Validation loss per 100 evaluation steps: 0.45604666376516056\n",
            "Validation loss per 100 evaluation steps: 0.45446591341753806\n",
            "Validation loss per 100 evaluation steps: 0.45140586608301136\n",
            "Validation loss per 100 evaluation steps: 0.48141060354783577\n",
            "Validation loss per 100 evaluation steps: 0.477896876162555\n",
            "Validation loss per 100 evaluation steps: 0.4669463170864086\n",
            "Validation loss per 100 evaluation steps: 0.4640986411914521\n",
            "Validation loss per 100 evaluation steps: 0.4587357858642511\n",
            "Validation loss per 100 evaluation steps: 0.45641200055825115\n",
            "Validation loss per 100 evaluation steps: 0.4516694576861979\n",
            "Validation loss per 100 evaluation steps: 0.4544439537052056\n",
            "Validation loss per 100 evaluation steps: 0.4542327338895891\n",
            "Validation loss per 100 evaluation steps: 0.447422097158265\n",
            "Validation loss per 100 evaluation steps: 0.4526000371766595\n",
            "Validation loss per 100 evaluation steps: 0.44667145661008006\n",
            "Validation Loss: 0.45746835708205236\n",
            "Validation Accuracy: 0.9431538900120442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.82      0.94      0.88       665\n",
            "           #       0.69      1.00      0.81       267\n",
            "           $       0.87      0.98      0.92       380\n",
            "           &       0.97      0.99      0.98       456\n",
            "           ,       0.97      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.94      0.91      0.92      1344\n",
            "           D       0.98      0.97      0.98      1614\n",
            "           E       0.73      0.95      0.82       246\n",
            "           G       0.55      0.42      0.47       257\n",
            "           L       0.97      0.94      0.95       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.96      0.94      0.95      3582\n",
            "           O       0.99      0.97      0.98      1848\n",
            "           P       0.97      0.97      0.97      2281\n",
            "           R       0.92      0.91      0.92      1219\n",
            "           S       0.92      0.43      0.59        28\n",
            "           T       0.82      0.92      0.86       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.99      0.97      0.98      3954\n",
            "           X       0.60      0.84      0.70        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.86      0.23      0.36        52\n",
            "           ^       0.89      0.89      0.89      1662\n",
            "           ~       0.92      0.93      0.92       910\n",
            "\n",
            "    accuracy                           0.95     26078\n",
            "   macro avg       0.81      0.80      0.79     26078\n",
            "weighted avg       0.95      0.95      0.95     26078\n",
            "\n",
            "TEST FOR ITERATION 11\n",
            "Validation loss per 100 evaluation steps: 0.4778156578540802\n",
            "Validation loss per 100 evaluation steps: 0.4787073059385803\n",
            "Validation loss per 100 evaluation steps: 0.469205918648896\n",
            "Validation loss per 100 evaluation steps: 0.48789277012671495\n",
            "Validation loss per 100 evaluation steps: 0.4925719168453375\n",
            "Validation loss per 100 evaluation steps: 0.4816691628087041\n",
            "Validation loss per 100 evaluation steps: 0.5126184920356684\n",
            "Validation loss per 100 evaluation steps: 0.5091309181251785\n",
            "Validation loss per 100 evaluation steps: 0.4973096588966948\n",
            "Validation loss per 100 evaluation steps: 0.49513570081698943\n",
            "Validation loss per 100 evaluation steps: 0.48512882676966673\n",
            "Validation loss per 100 evaluation steps: 0.4911229962132441\n",
            "Validation loss per 100 evaluation steps: 0.4861199509975546\n",
            "Validation loss per 100 evaluation steps: 0.4885276595510381\n",
            "Validation loss per 100 evaluation steps: 0.4895577203761094\n",
            "Validation loss per 100 evaluation steps: 0.48158619198322206\n",
            "Validation loss per 100 evaluation steps: 0.48201943094388394\n",
            "Validation loss per 100 evaluation steps: 0.4775123329440901\n",
            "Validation Loss: 0.4893028680790811\n",
            "Validation Accuracy: 0.9423463939616106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.81      0.95      0.87       665\n",
            "           #       0.69      1.00      0.82       267\n",
            "           $       0.88      0.97      0.92       380\n",
            "           &       0.97      0.99      0.98       456\n",
            "           ,       0.97      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.93      0.92      0.92      1344\n",
            "           D       0.98      0.97      0.98      1614\n",
            "           E       0.72      0.95      0.82       246\n",
            "           G       0.57      0.39      0.47       257\n",
            "           L       0.98      0.94      0.96       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.97      0.92      0.94      3582\n",
            "           O       0.98      0.98      0.98      1848\n",
            "           P       0.98      0.95      0.96      2281\n",
            "           R       0.91      0.92      0.91      1219\n",
            "           S       0.93      0.50      0.65        28\n",
            "           T       0.82      0.91      0.87       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.98      0.97      0.98      3954\n",
            "           X       0.61      0.80      0.69        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.88      0.27      0.41        52\n",
            "           ^       0.87      0.91      0.89      1662\n",
            "           ~       0.94      0.92      0.93       910\n",
            "\n",
            "    accuracy                           0.95     26078\n",
            "   macro avg       0.81      0.80      0.80     26078\n",
            "weighted avg       0.95      0.95      0.94     26078\n",
            "\n",
            "TEST FOR ITERATION 12\n",
            "Validation loss per 100 evaluation steps: 0.4700707197189331\n",
            "Validation loss per 100 evaluation steps: 0.46572637651569504\n",
            "Validation loss per 100 evaluation steps: 0.4583352538487888\n",
            "Validation loss per 100 evaluation steps: 0.4755047219586113\n",
            "Validation loss per 100 evaluation steps: 0.46571337551216757\n",
            "Validation loss per 100 evaluation steps: 0.4645036682409236\n",
            "Validation loss per 100 evaluation steps: 0.4920048901653574\n",
            "Validation loss per 100 evaluation steps: 0.4880317406739897\n",
            "Validation loss per 100 evaluation steps: 0.47811641336006694\n",
            "Validation loss per 100 evaluation steps: 0.474941009073389\n",
            "Validation loss per 100 evaluation steps: 0.464733547228113\n",
            "Validation loss per 100 evaluation steps: 0.46737706265595985\n",
            "Validation loss per 100 evaluation steps: 0.4592669741104881\n",
            "Validation loss per 100 evaluation steps: 0.46235039242344356\n",
            "Validation loss per 100 evaluation steps: 0.46358908493006334\n",
            "Validation loss per 100 evaluation steps: 0.46040578317345987\n",
            "Validation loss per 100 evaluation steps: 0.4610232306543206\n",
            "Validation loss per 100 evaluation steps: 0.4555711193464331\n",
            "Validation Loss: 0.46597549037234864\n",
            "Validation Accuracy: 0.9424815479504146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.87      0.93      0.90       665\n",
            "           #       0.71      1.00      0.83       267\n",
            "           $       0.90      0.97      0.93       380\n",
            "           &       0.97      0.99      0.98       456\n",
            "           ,       0.97      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.94      0.90      0.92      1344\n",
            "           D       0.98      0.97      0.98      1614\n",
            "           E       0.74      0.96      0.83       246\n",
            "           G       0.53      0.43      0.48       257\n",
            "           L       0.97      0.95      0.96       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.95      0.93      0.94      3582\n",
            "           O       0.98      0.98      0.98      1848\n",
            "           P       0.98      0.96      0.97      2281\n",
            "           R       0.91      0.92      0.91      1219\n",
            "           S       0.82      0.50      0.62        28\n",
            "           T       0.81      0.94      0.87       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.99      0.97      0.98      3954\n",
            "           X       0.60      0.84      0.70        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.94      0.29      0.44        52\n",
            "           ^       0.85      0.91      0.88      1662\n",
            "           ~       0.94      0.91      0.92       910\n",
            "\n",
            "    accuracy                           0.95     26078\n",
            "   macro avg       0.81      0.81      0.80     26078\n",
            "weighted avg       0.95      0.95      0.95     26078\n",
            "\n",
            "TEST FOR ITERATION 13\n",
            "Validation loss per 100 evaluation steps: 0.5839427709579468\n",
            "Validation loss per 100 evaluation steps: 0.5185575130208505\n",
            "Validation loss per 100 evaluation steps: 0.5423779573159017\n",
            "Validation loss per 100 evaluation steps: 0.5475631084048463\n",
            "Validation loss per 100 evaluation steps: 0.5480244255708875\n",
            "Validation loss per 100 evaluation steps: 0.5532255580617207\n",
            "Validation loss per 100 evaluation steps: 0.5749671844595333\n",
            "Validation loss per 100 evaluation steps: 0.5679537694883277\n",
            "Validation loss per 100 evaluation steps: 0.5539232922391774\n",
            "Validation loss per 100 evaluation steps: 0.5541035187863254\n",
            "Validation loss per 100 evaluation steps: 0.5456748993983156\n",
            "Validation loss per 100 evaluation steps: 0.5471844362949152\n",
            "Validation loss per 100 evaluation steps: 0.5384943908979042\n",
            "Validation loss per 100 evaluation steps: 0.5382590880445001\n",
            "Validation loss per 100 evaluation steps: 0.5388245259797007\n",
            "Validation loss per 100 evaluation steps: 0.5354448424146111\n",
            "Validation loss per 100 evaluation steps: 0.5327142326589458\n",
            "Validation loss per 100 evaluation steps: 0.5285870565974352\n",
            "Validation Loss: 0.5397970067157963\n",
            "Validation Accuracy: 0.937892777333506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.79      0.94      0.86       665\n",
            "           #       0.69      0.99      0.81       267\n",
            "           $       0.86      0.97      0.91       380\n",
            "           &       0.96      0.99      0.97       456\n",
            "           ,       0.97      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.91      0.92      0.91      1344\n",
            "           D       0.98      0.97      0.97      1614\n",
            "           E       0.75      0.95      0.84       246\n",
            "           G       0.52      0.38      0.44       257\n",
            "           L       0.96      0.94      0.95       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.97      0.91      0.94      3582\n",
            "           O       0.98      0.97      0.97      1848\n",
            "           P       0.97      0.96      0.96      2281\n",
            "           R       0.91      0.92      0.91      1219\n",
            "           S       0.83      0.54      0.65        28\n",
            "           T       0.84      0.91      0.87       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.99      0.96      0.97      3954\n",
            "           X       0.58      0.88      0.70        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.92      0.21      0.34        52\n",
            "           ^       0.84      0.90      0.87      1662\n",
            "           ~       0.94      0.92      0.93       910\n",
            "\n",
            "    accuracy                           0.94     26078\n",
            "   macro avg       0.81      0.80      0.79     26078\n",
            "weighted avg       0.94      0.94      0.94     26078\n",
            "\n",
            "TEST FOR ITERATION 14\n",
            "Validation loss per 100 evaluation steps: 0.41615113615989685\n",
            "Validation loss per 100 evaluation steps: 0.4705071461748687\n",
            "Validation loss per 100 evaluation steps: 0.4733706639365922\n",
            "Validation loss per 100 evaluation steps: 0.49752593527992334\n",
            "Validation loss per 100 evaluation steps: 0.49372239383966393\n",
            "Validation loss per 100 evaluation steps: 0.4935228940563763\n",
            "Validation loss per 100 evaluation steps: 0.5217519286683403\n",
            "Validation loss per 100 evaluation steps: 0.5159270133018603\n",
            "Validation loss per 100 evaluation steps: 0.5069932492278395\n",
            "Validation loss per 100 evaluation steps: 0.5031382670316005\n",
            "Validation loss per 100 evaluation steps: 0.4951958998278926\n",
            "Validation loss per 100 evaluation steps: 0.49770873845549224\n",
            "Validation loss per 100 evaluation steps: 0.49270542502620146\n",
            "Validation loss per 100 evaluation steps: 0.49550389146519314\n",
            "Validation loss per 100 evaluation steps: 0.4976395836807749\n",
            "Validation loss per 100 evaluation steps: 0.4915803066131129\n",
            "Validation loss per 100 evaluation steps: 0.49240826653633946\n",
            "Validation loss per 100 evaluation steps: 0.48996097369148095\n",
            "Validation Loss: 0.5026596448597344\n",
            "Validation Accuracy: 0.9414503847397574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.83      0.94      0.88       665\n",
            "           #       0.68      1.00      0.81       267\n",
            "           $       0.88      0.97      0.92       380\n",
            "           &       0.97      0.98      0.98       456\n",
            "           ,       0.97      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.94      0.91      0.92      1344\n",
            "           D       0.98      0.97      0.98      1614\n",
            "           E       0.74      0.96      0.83       246\n",
            "           G       0.57      0.37      0.45       257\n",
            "           L       0.99      0.93      0.96       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.96      0.93      0.95      3582\n",
            "           O       0.98      0.97      0.98      1848\n",
            "           P       0.98      0.95      0.96      2281\n",
            "           R       0.91      0.92      0.91      1219\n",
            "           S       0.78      0.50      0.61        28\n",
            "           T       0.84      0.92      0.88       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.99      0.97      0.98      3954\n",
            "           X       0.65      0.80      0.71        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.92      0.23      0.37        52\n",
            "           ^       0.85      0.90      0.88      1662\n",
            "           ~       0.93      0.92      0.93       910\n",
            "\n",
            "    accuracy                           0.94     26078\n",
            "   macro avg       0.81      0.80      0.79     26078\n",
            "weighted avg       0.95      0.94      0.94     26078\n",
            "\n",
            "TEST FOR ITERATION 15\n",
            "Validation loss per 100 evaluation steps: 0.5048244595527649\n",
            "Validation loss per 100 evaluation steps: 0.4764262453854376\n",
            "Validation loss per 100 evaluation steps: 0.4796179577211905\n",
            "Validation loss per 100 evaluation steps: 0.5054888036527918\n",
            "Validation loss per 100 evaluation steps: 0.5008130542988701\n",
            "Validation loss per 100 evaluation steps: 0.4996240357399539\n",
            "Validation loss per 100 evaluation steps: 0.5321306344389353\n",
            "Validation loss per 100 evaluation steps: 0.5301172950089436\n",
            "Validation loss per 100 evaluation steps: 0.5235394055825646\n",
            "Validation loss per 100 evaluation steps: 0.5208709557184544\n",
            "Validation loss per 100 evaluation steps: 0.5122195383850481\n",
            "Validation loss per 100 evaluation steps: 0.5135391966398011\n",
            "Validation loss per 100 evaluation steps: 0.5069613664407232\n",
            "Validation loss per 100 evaluation steps: 0.5138858090371448\n",
            "Validation loss per 100 evaluation steps: 0.5197523133155901\n",
            "Validation loss per 100 evaluation steps: 0.5144853399479261\n",
            "Validation loss per 100 evaluation steps: 0.5160125596814852\n",
            "Validation loss per 100 evaluation steps: 0.5132187910558549\n",
            "Validation Loss: 0.5244686818227098\n",
            "Validation Accuracy: 0.9406256322497194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.89      0.92      0.91       665\n",
            "           #       0.69      1.00      0.82       267\n",
            "           $       0.88      0.97      0.92       380\n",
            "           &       0.97      0.98      0.98       456\n",
            "           ,       0.97      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.93      0.92      0.92      1344\n",
            "           D       0.98      0.97      0.97      1614\n",
            "           E       0.72      0.96      0.82       246\n",
            "           G       0.58      0.40      0.47       257\n",
            "           L       0.98      0.94      0.96       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.96      0.92      0.94      3582\n",
            "           O       0.98      0.97      0.98      1848\n",
            "           P       0.97      0.96      0.97      2281\n",
            "           R       0.92      0.92      0.92      1219\n",
            "           S       0.93      0.50      0.65        28\n",
            "           T       0.83      0.92      0.87       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.98      0.97      0.98      3954\n",
            "           X       0.62      0.80      0.70        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.82      0.17      0.29        52\n",
            "           ^       0.84      0.91      0.87      1662\n",
            "           ~       0.93      0.91      0.92       910\n",
            "\n",
            "    accuracy                           0.94     26078\n",
            "   macro avg       0.81      0.80      0.79     26078\n",
            "weighted avg       0.95      0.94      0.94     26078\n",
            "\n",
            "TEST FOR ITERATION 16\n",
            "Validation loss per 100 evaluation steps: 0.3202677071094513\n",
            "Validation loss per 100 evaluation steps: 0.46580436989461305\n",
            "Validation loss per 100 evaluation steps: 0.48597529460359273\n",
            "Validation loss per 100 evaluation steps: 0.5084923808749152\n",
            "Validation loss per 100 evaluation steps: 0.5023177693313226\n",
            "Validation loss per 100 evaluation steps: 0.5028025689169539\n",
            "Validation loss per 100 evaluation steps: 0.5396938362601894\n",
            "Validation loss per 100 evaluation steps: 0.5323559567330828\n",
            "Validation loss per 100 evaluation steps: 0.5232461355847462\n",
            "Validation loss per 100 evaluation steps: 0.5243604101154488\n",
            "Validation loss per 100 evaluation steps: 0.5207784033551897\n",
            "Validation loss per 100 evaluation steps: 0.5219131415962476\n",
            "Validation loss per 100 evaluation steps: 0.518890598996401\n",
            "Validation loss per 100 evaluation steps: 0.5231625739651911\n",
            "Validation loss per 100 evaluation steps: 0.5263647348189484\n",
            "Validation loss per 100 evaluation steps: 0.5171394223289311\n",
            "Validation loss per 100 evaluation steps: 0.5186057733189401\n",
            "Validation loss per 100 evaluation steps: 0.5175700795691716\n",
            "Validation Loss: 0.527530242216494\n",
            "Validation Accuracy: 0.9371856264635692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.85      0.94      0.89       665\n",
            "           #       0.68      1.00      0.81       267\n",
            "           $       0.88      0.97      0.92       380\n",
            "           &       0.98      0.98      0.98       456\n",
            "           ,       0.97      0.98      0.97      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.92      0.91      0.92      1344\n",
            "           D       0.98      0.96      0.97      1614\n",
            "           E       0.72      0.96      0.82       246\n",
            "           G       0.52      0.42      0.47       257\n",
            "           L       0.98      0.95      0.97       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.97      0.90      0.93      3582\n",
            "           O       0.98      0.97      0.97      1848\n",
            "           P       0.98      0.95      0.96      2281\n",
            "           R       0.90      0.93      0.91      1219\n",
            "           S       0.81      0.61      0.69        28\n",
            "           T       0.78      0.93      0.85       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.97      0.97      0.97      3954\n",
            "           X       0.57      0.80      0.67        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.86      0.23      0.36        52\n",
            "           ^       0.86      0.87      0.87      1662\n",
            "           ~       0.94      0.92      0.93       910\n",
            "\n",
            "    accuracy                           0.94     26078\n",
            "   macro avg       0.80      0.80      0.79     26078\n",
            "weighted avg       0.94      0.94      0.94     26078\n",
            "\n",
            "TEST FOR ITERATION 17\n",
            "Validation loss per 100 evaluation steps: 0.3209080100059509\n",
            "Validation loss per 100 evaluation steps: 0.5167474921304392\n",
            "Validation loss per 100 evaluation steps: 0.5248102071630668\n",
            "Validation loss per 100 evaluation steps: 0.5479722309084772\n",
            "Validation loss per 100 evaluation steps: 0.5478291012322304\n",
            "Validation loss per 100 evaluation steps: 0.551559690425561\n",
            "Validation loss per 100 evaluation steps: 0.586634146876422\n",
            "Validation loss per 100 evaluation steps: 0.5782239858247609\n",
            "Validation loss per 100 evaluation steps: 0.5696934222739495\n",
            "Validation loss per 100 evaluation steps: 0.5709411419571269\n",
            "Validation loss per 100 evaluation steps: 0.5643304800373317\n",
            "Validation loss per 100 evaluation steps: 0.5662652794852843\n",
            "Validation loss per 100 evaluation steps: 0.559417631070673\n",
            "Validation loss per 100 evaluation steps: 0.5641397414167206\n",
            "Validation loss per 100 evaluation steps: 0.5668258559611644\n",
            "Validation loss per 100 evaluation steps: 0.5618317163102474\n",
            "Validation loss per 100 evaluation steps: 0.5642356410537901\n",
            "Validation loss per 100 evaluation steps: 0.5598258345487053\n",
            "Validation Loss: 0.5709218553217617\n",
            "Validation Accuracy: 0.9381596278035099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.81      0.94      0.87       665\n",
            "           #       0.68      1.00      0.81       267\n",
            "           $       0.86      0.97      0.91       380\n",
            "           &       0.98      0.98      0.98       456\n",
            "           ,       0.97      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.91      0.91      0.91      1344\n",
            "           D       0.98      0.97      0.97      1614\n",
            "           E       0.77      0.92      0.84       246\n",
            "           G       0.55      0.40      0.46       257\n",
            "           L       0.98      0.95      0.96       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.96      0.92      0.94      3582\n",
            "           O       0.97      0.98      0.97      1848\n",
            "           P       0.98      0.95      0.97      2281\n",
            "           R       0.91      0.91      0.91      1219\n",
            "           S       0.77      0.36      0.49        28\n",
            "           T       0.80      0.91      0.85       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.98      0.97      0.97      3954\n",
            "           X       0.53      0.80      0.63        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.85      0.21      0.34        52\n",
            "           ^       0.87      0.87      0.87      1662\n",
            "           ~       0.94      0.91      0.93       910\n",
            "\n",
            "    accuracy                           0.94     26078\n",
            "   macro avg       0.80      0.79      0.78     26078\n",
            "weighted avg       0.94      0.94      0.94     26078\n",
            "\n",
            "TEST FOR ITERATION 18\n",
            "Validation loss per 100 evaluation steps: 0.4050542116165161\n",
            "Validation loss per 100 evaluation steps: 0.5688730188152711\n",
            "Validation loss per 100 evaluation steps: 0.5853824511502457\n",
            "Validation loss per 100 evaluation steps: 0.5987160694472774\n",
            "Validation loss per 100 evaluation steps: 0.5985248475354205\n",
            "Validation loss per 100 evaluation steps: 0.5942488033370863\n",
            "Validation loss per 100 evaluation steps: 0.6231423350169982\n",
            "Validation loss per 100 evaluation steps: 0.6090838760453948\n",
            "Validation loss per 100 evaluation steps: 0.6009851877542145\n",
            "Validation loss per 100 evaluation steps: 0.6003278457726964\n",
            "Validation loss per 100 evaluation steps: 0.5928309054791795\n",
            "Validation loss per 100 evaluation steps: 0.5955349919707146\n",
            "Validation loss per 100 evaluation steps: 0.5879189420753738\n",
            "Validation loss per 100 evaluation steps: 0.5947547475640621\n",
            "Validation loss per 100 evaluation steps: 0.5981028816347118\n",
            "Validation loss per 100 evaluation steps: 0.5906161133756086\n",
            "Validation loss per 100 evaluation steps: 0.5913501987590359\n",
            "Validation loss per 100 evaluation steps: 0.5926013757380907\n",
            "Validation Loss: 0.6038212462563052\n",
            "Validation Accuracy: 0.9337569303866027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.82      0.94      0.88       665\n",
            "           #       0.68      1.00      0.81       267\n",
            "           $       0.85      0.97      0.91       380\n",
            "           &       0.97      0.99      0.98       456\n",
            "           ,       0.97      0.98      0.98      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.92      0.91      0.91      1344\n",
            "           D       0.98      0.97      0.97      1614\n",
            "           E       0.77      0.93      0.84       246\n",
            "           G       0.54      0.41      0.46       257\n",
            "           L       0.97      0.94      0.96       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.97      0.89      0.93      3582\n",
            "           O       0.98      0.97      0.97      1848\n",
            "           P       0.97      0.95      0.96      2281\n",
            "           R       0.90      0.92      0.91      1219\n",
            "           S       0.78      0.50      0.61        28\n",
            "           T       0.78      0.91      0.84       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.99      0.96      0.97      3954\n",
            "           X       0.50      0.88      0.64        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.79      0.29      0.42        52\n",
            "           ^       0.81      0.90      0.85      1662\n",
            "           ~       0.92      0.93      0.92       910\n",
            "\n",
            "    accuracy                           0.94     26078\n",
            "   macro avg       0.79      0.80      0.79     26078\n",
            "weighted avg       0.94      0.94      0.94     26078\n",
            "\n",
            "TEST FOR ITERATION 19\n",
            "Validation loss per 100 evaluation steps: 0.4441118836402893\n",
            "Validation loss per 100 evaluation steps: 0.5912804236724138\n",
            "Validation loss per 100 evaluation steps: 0.6106982357306825\n",
            "Validation loss per 100 evaluation steps: 0.6221943503526223\n",
            "Validation loss per 100 evaluation steps: 0.6171893683604774\n",
            "Validation loss per 100 evaluation steps: 0.6181538854671706\n",
            "Validation loss per 100 evaluation steps: 0.6464704064201865\n",
            "Validation loss per 100 evaluation steps: 0.6317395054797149\n",
            "Validation loss per 100 evaluation steps: 0.6199712702173928\n",
            "Validation loss per 100 evaluation steps: 0.6177956147953269\n",
            "Validation loss per 100 evaluation steps: 0.6080445484581533\n",
            "Validation loss per 100 evaluation steps: 0.6059905864675769\n",
            "Validation loss per 100 evaluation steps: 0.5975458863873755\n",
            "Validation loss per 100 evaluation steps: 0.6039000921946247\n",
            "Validation loss per 100 evaluation steps: 0.607719086498387\n",
            "Validation loss per 100 evaluation steps: 0.6004965342787522\n",
            "Validation loss per 100 evaluation steps: 0.5984965313342997\n",
            "Validation loss per 100 evaluation steps: 0.5944042252349921\n",
            "Validation Loss: 0.605998887414667\n",
            "Validation Accuracy: 0.934017110961464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           !       0.82      0.94      0.87       665\n",
            "           #       0.67      1.00      0.80       267\n",
            "           $       0.87      0.97      0.92       380\n",
            "           &       0.99      0.98      0.98       456\n",
            "           ,       0.97      0.98      0.97      3031\n",
            "           @       0.98      0.99      0.99      1233\n",
            "           A       0.92      0.91      0.91      1344\n",
            "           D       0.97      0.97      0.97      1614\n",
            "           E       0.76      0.93      0.83       246\n",
            "           G       0.49      0.42      0.45       257\n",
            "           L       0.97      0.94      0.95       438\n",
            "           M       0.00      0.00      0.00         3\n",
            "           N       0.96      0.90      0.93      3582\n",
            "           O       0.98      0.97      0.97      1848\n",
            "           P       0.97      0.95      0.96      2281\n",
            "           R       0.93      0.90      0.91      1219\n",
            "           S       0.91      0.36      0.51        28\n",
            "           T       0.80      0.91      0.85       163\n",
            "           U       1.00      0.97      0.98       418\n",
            "           V       0.99      0.96      0.98      3954\n",
            "           X       0.50      0.84      0.63        25\n",
            "           Y       0.00      0.00      0.00         2\n",
            "           Z       0.69      0.21      0.32        52\n",
            "           ^       0.81      0.89      0.85      1662\n",
            "           ~       0.92      0.91      0.92       910\n",
            "\n",
            "    accuracy                           0.94     26078\n",
            "   macro avg       0.79      0.79      0.78     26078\n",
            "weighted avg       0.94      0.94      0.94     26078\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_acc = [0.9832061816675643, 0.9915116148886468, 0.9963239813027643, 0.996896048968625, 0.997583019148653, 0.9982412081235348, 0.9986991767146035, 0.9984213172189726, 0.9987608050583717, 0.9992337275768479, 0.999613115923455, 0.9992951771067535, 0.9993470284696347, 0.9994586071534086, 0.9995001245571722, 0.9990049958292523, 0.9994978013111345, 0.9996505143884221, 0.9994173092361026]"
      ],
      "metadata": {
        "id": "pop23SVEYhKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Results"
      ],
      "metadata": {
        "id": "_mWINBqI6UxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_title('POS Unsupervised Training Results')\n",
        "ax.plot(iteration,validation_acc, color='r')\n",
        "ax.plot(range(19),training_acc, color='b')\n",
        "ax.set_xlabel(\"Iteration\")\n",
        "ax.set_xticks(range(20))\n",
        "ax.set_ylabel(\"Accuracy\")\n",
        "\n",
        "ax.legend([\"Test Accuracy\", \"Training Accuracy\"], loc=0)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "ldI7zmF9S5kK",
        "outputId": "8f1f3e16-e696-4041-e0a2-93b429d2fba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f07d94ba6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}